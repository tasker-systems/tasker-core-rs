//! # Batch Processing Workflow Happy Path Integration Tests
//!
//! Tests the TAS-59 batch processing pattern with cursor-based resumability, dynamic
//! worker creation, and checkpoint health monitoring.
//!
//! ## Batch Processing Pattern Structure
//!
//! ```text
//!    analyze_dataset (batchable)
//!           |
//!     [Creates N Workers]
//!           |
//!    +------+------+------+
//!    |      |      |      |
//! worker_1 worker_2  worker_N (batch_worker instances)
//!    |      |      |      |
//!    +------+------+------+
//!           |
//!    aggregate_results (deferred_convergence)
//! ```
//!
//! ## Test Coverage
//!
//! - Task initialization with batchable step type
//! - Batchable step execution and BatchProcessingOutcome detection
//! - Dynamic batch worker creation with cursor configurations
//! - Batch worker execution with checkpoint support
//! - Deferred convergence step and result aggregation
//! - Small dataset scenario (NoBatches outcome)
//! - Validation of cursor-based resumability

use anyhow::Result;
use sqlx::PgPool;

use crate::common::lifecycle_test_manager::LifecycleTestManager;

/// Test small dataset returns NoBatches outcome
#[sqlx::test(migrator = "tasker_shared::database::migrator::MIGRATOR")]
async fn test_batch_processing_small_dataset_no_batches(pool: PgPool) -> Result<()> {
    tracing::info!("ğŸ” BATCH PROCESSING: Small dataset NoBatches scenario");

    let manager =
        LifecycleTestManager::with_template_path(pool, "tests/fixtures/task_templates/rust")
            .await?;

    // Create task request with small dataset (< batch_size)
    // Template has batch_size: 1000, so 500 items should return NoBatches
    let task_request = manager.create_task_request_for_template(
        "large_dataset_processor",
        "data_processing",
        serde_json::json!({
            "dataset_size": 500,
            "dataset_name": "small_users",
            "processing_mode": "parallel"
        }),
    );

    // Initialize task
    let init_result = manager.initialize_task(task_request).await?;

    // Validate initial task execution context
    // Should have 2 base steps: analyze_dataset, aggregate_results (process_batch template excluded)
    manager
        .validate_task_execution_context(
            init_result.task_uuid,
            2, // total_steps initially (BatchWorker templates excluded)
            0, // completed_steps
            1, // ready_steps (only analyze_dataset)
        )
        .await?;

    // Validate analyze_dataset step is ready
    manager
        .validate_step_readiness(
            init_result.task_uuid,
            "analyze_dataset",
            true, // ready
            0,    // total_parents
            0,    // completed_parents
        )
        .await?;

    // Validate process_batch template is not ready (no parents yet - it's a template)
    // Note: Template steps should not be in ready state as they're never executed directly
    manager
        .validate_step_readiness(
            init_result.task_uuid,
            "process_batch",
            false, // not ready
            1,     // total_parents (analyze_dataset edge exists in template)
            0,     // completed_parents
        )
        .await?;

    tracing::info!("âœ… Small dataset initialization validated");

    Ok(())
}

/// Test medium dataset creates multiple batch workers
#[sqlx::test(migrator = "tasker_shared::database::migrator::MIGRATOR")]
async fn test_batch_processing_medium_dataset_creates_workers(pool: PgPool) -> Result<()> {
    tracing::info!("ğŸ” BATCH PROCESSING: Medium dataset worker creation");

    let manager =
        LifecycleTestManager::with_template_path(pool, "tests/fixtures/task_templates/rust")
            .await?;

    // Create task request with medium dataset
    // Template has batch_size: 1000, max_workers: 10
    // 3500 items should create 4 workers (ceil(3500/1000) = 4)
    let task_request = manager.create_task_request_for_template(
        "large_dataset_processor",
        "data_processing",
        serde_json::json!({
            "dataset_size": 3500,
            "dataset_name": "medium_users",
            "processing_mode": "parallel"
        }),
    );

    // Initialize task
    let init_result = manager.initialize_task(task_request).await?;

    // Validate initial task execution context
    manager
        .validate_task_execution_context(
            init_result.task_uuid,
            3, // total_steps initially
            0, // completed_steps
            1, // ready_steps
        )
        .await?;

    // Validate analyze_dataset step is ready
    manager
        .validate_step_readiness(
            init_result.task_uuid,
            "analyze_dataset",
            true, // ready
            0,    // total_parents
            0,    // completed_parents
        )
        .await?;

    // Validate aggregate_results is not ready (depends on batch workers that don't exist yet)
    manager
        .validate_step_readiness(
            init_result.task_uuid,
            "aggregate_results",
            false, // not ready
            1,     // total_parents (process_batch template edge)
            0,     // completed_parents
        )
        .await?;

    tracing::info!("âœ… Medium dataset initialization validated");

    Ok(())
}

/// Test large dataset respects max_workers limit
#[sqlx::test(migrator = "tasker_shared::database::migrator::MIGRATOR")]
async fn test_batch_processing_large_dataset_respects_max_workers(pool: PgPool) -> Result<()> {
    tracing::info!("ğŸ” BATCH PROCESSING: Large dataset max_workers limit");

    let manager =
        LifecycleTestManager::with_template_path(pool, "tests/fixtures/task_templates/rust")
            .await?;

    // Create task request with large dataset
    // Template has batch_size: 1000, max_workers: 10
    // 25000 items would create 25 workers ideally, but should cap at 10
    let task_request = manager.create_task_request_for_template(
        "large_dataset_processor",
        "data_processing",
        serde_json::json!({
            "dataset_size": 25000,
            "dataset_name": "large_users",
            "processing_mode": "parallel"
        }),
    );

    // Initialize task
    let init_result = manager.initialize_task(task_request).await?;

    // Validate initial task execution context
    manager
        .validate_task_execution_context(
            init_result.task_uuid,
            3, // total_steps initially
            0, // completed_steps
            1, // ready_steps
        )
        .await?;

    // Validate analyze_dataset step is ready
    manager
        .validate_step_readiness(
            init_result.task_uuid,
            "analyze_dataset",
            true, // ready
            0,    // total_parents
            0,    // completed_parents
        )
        .await?;

    tracing::info!("âœ… Large dataset initialization validated with max_workers constraint");

    Ok(())
}

/// Test exact batch_size boundary conditions
#[sqlx::test(migrator = "tasker_shared::database::migrator::MIGRATOR")]
async fn test_batch_processing_exact_batch_size_boundary(pool: PgPool) -> Result<()> {
    tracing::info!("ğŸ” BATCH PROCESSING: Exact batch_size boundary");

    let manager =
        LifecycleTestManager::with_template_path(pool, "tests/fixtures/task_templates/rust")
            .await?;

    // Create task request with exactly 1000 items (= batch_size)
    // Should create 1 worker (1000/1000 = 1)
    let task_request = manager.create_task_request_for_template(
        "large_dataset_processor",
        "data_processing",
        serde_json::json!({
            "dataset_size": 1000,
            "dataset_name": "exact_batch",
            "processing_mode": "parallel"
        }),
    );

    // Initialize task
    let init_result = manager.initialize_task(task_request).await?;

    // Validate initial task execution context
    manager
        .validate_task_execution_context(
            init_result.task_uuid,
            3, // total_steps initially
            0, // completed_steps
            1, // ready_steps
        )
        .await?;

    tracing::info!("âœ… Exact batch_size boundary validated");

    Ok(())
}

/// Test minimal batching scenario (just over batch_size threshold)
#[sqlx::test(migrator = "tasker_shared::database::migrator::MIGRATOR")]
async fn test_batch_processing_minimal_batching(pool: PgPool) -> Result<()> {
    tracing::info!("ğŸ” BATCH PROCESSING: Minimal batching scenario");

    let manager =
        LifecycleTestManager::with_template_path(pool, "tests/fixtures/task_templates/rust")
            .await?;

    // Create task request with 1001 items (just over batch_size threshold)
    // Should create 2 workers (ceil(1001/1000) = 2)
    let task_request = manager.create_task_request_for_template(
        "large_dataset_processor",
        "data_processing",
        serde_json::json!({
            "dataset_size": 1001,
            "dataset_name": "minimal_batch",
            "processing_mode": "parallel"
        }),
    );

    // Initialize task
    let init_result = manager.initialize_task(task_request).await?;

    // Validate initial task execution context
    manager
        .validate_task_execution_context(
            init_result.task_uuid,
            3, // total_steps initially
            0, // completed_steps
            1, // ready_steps
        )
        .await?;

    // Validate analyze_dataset step is ready
    manager
        .validate_step_readiness(
            init_result.task_uuid,
            "analyze_dataset",
            true, // ready
            0,    // total_parents
            0,    // completed_parents
        )
        .await?;

    tracing::info!("âœ… Minimal batching scenario validated");

    Ok(())
}
