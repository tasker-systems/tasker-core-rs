# Tasker V2 Common Configuration (Base)
# Shared across all contexts (orchestration and worker)

[common.system]
version = "0.1.0"
default_dependent_system = "default"
max_recursion_depth = 50

[common.database]
url = "${DATABASE_URL:-postgresql://localhost/tasker}"
database = "tasker_development"
skip_migration_check = false

[common.database.pool]
max_connections = 25
min_connections = 5
acquire_timeout_seconds = 10
idle_timeout_seconds = 300
max_lifetime_seconds = 1800
slow_acquire_threshold_ms = 100

[common.database.variables]
statement_timeout = 30000

# PGMQ Separate Database Configuration (optional)
# When url is empty or PGMQ_DATABASE_URL not set, PGMQ uses main database
[common.pgmq_database]
url = "${PGMQ_DATABASE_URL:-}"
enabled = true
skip_migration_check = false

[common.pgmq_database.pool]
max_connections = 15
min_connections = 3
acquire_timeout_seconds = 5
idle_timeout_seconds = 300
max_lifetime_seconds = 1800
slow_acquire_threshold_ms = 100

[common.queues]
# Messaging backend selection (TAS-133)
# Valid values: pgmq (default), rabbitmq
# - pgmq: PostgreSQL Message Queue (single-dependency deployment, LISTEN/NOTIFY)
# - rabbitmq: AMQP broker (native push via basic_consume, higher throughput)
backend = "${TASKER_MESSAGING_BACKEND:-pgmq}"
orchestration_namespace = "orchestration"
worker_namespace = "worker"
default_visibility_timeout_seconds = 30
default_batch_size = 10
max_batch_size = 100
naming_pattern = "{namespace}_{name}_queue"
health_check_interval = 60

[common.queues.orchestration_queues]
task_requests = "orchestration_task_requests"
task_finalizations = "orchestration_task_finalizations"
step_results = "orchestration_step_results"

[common.queues.pgmq]
poll_interval_ms = 500
shutdown_timeout_seconds = 10
max_retries = 3

# TAS-75 Phase 3: Queue depth thresholds for backpressure monitoring
# These are SOFT limits - messages are never rejected, but API returns 503 at critical depth
[common.queues.pgmq.queue_depth_thresholds]
warning_threshold = 1000    # Log warnings above this depth
critical_threshold = 5000   # Return 503 Service Unavailable above this depth
overflow_threshold = 10000  # Emergency level requiring manual intervention

# RabbitMQ Configuration (TAS-133d - alternative to PGMQ)
# Only used when backend = "rabbitmq"
[common.queues.rabbitmq]
# Note: %2F is URL-encoded "/" for the default vhost
url = "${RABBITMQ_URL:-amqp://guest:guest@localhost:5672/%2F}"
prefetch_count = 100
heartbeat_seconds = 30
connection_timeout_seconds = 10

[common.circuit_breakers]
enabled = true

[common.circuit_breakers.global_settings]
max_circuit_breakers = 50
metrics_collection_interval_seconds = 30
min_state_transition_interval_seconds = 5.0

[common.circuit_breakers.default_config]
failure_threshold = 5
timeout_seconds = 30
success_threshold = 2

[common.circuit_breakers.component_configs.task_readiness]
failure_threshold = 10
timeout_seconds = 60
success_threshold = 3

[common.circuit_breakers.component_configs.pgmq]
failure_threshold = 5
timeout_seconds = 30
success_threshold = 2

# TAS-171: Cache circuit breaker (protects Redis/Dragonfly operations)
[common.circuit_breakers.component_configs.cache]
failure_threshold = 5
timeout_seconds = 15   # Shorter than default - cache is non-critical
success_threshold = 2

[common.mpsc_channels.event_publisher]
event_queue_buffer_size = 5000

[common.mpsc_channels.ffi]
ruby_event_buffer_size = 1000

[common.mpsc_channels.overflow_policy]
log_warning_threshold = 0.8
drop_policy = "block"

[common.mpsc_channels.overflow_policy.metrics]
enabled = true
saturation_check_interval_seconds = 30

[common.execution]
max_concurrent_tasks = 100
max_concurrent_steps = 1000
default_timeout_seconds = 3600
step_execution_timeout_seconds = 600
max_discovery_attempts = 5
step_batch_size = 50
max_retries = 3
max_workflow_steps = 10000
connection_timeout_seconds = 30
environment = "development"

[common.backoff]
default_backoff_seconds = [1, 5, 15, 30, 60]
max_backoff_seconds = 3600
backoff_multiplier = 2.0
jitter_enabled = true
jitter_max_percentage = 0.15

[common.backoff.reenqueue_delays]
initializing = 10
enqueuing_steps = 5
steps_in_process = 15
evaluating_results = 10
waiting_for_dependencies = 60
waiting_for_retry = 30
blocked_by_failures = 120

# Distributed Cache Configuration (TAS-156)
# When enabled=false or section missing, system uses direct DB queries only
[common.cache]
enabled = false
backend = "redis"
default_ttl_seconds = 3600
template_ttl_seconds = 3600
analytics_ttl_seconds = 60
key_prefix = "tasker"

[common.cache.redis]
url = "${REDIS_URL:-redis://localhost:6379}"
max_connections = 10
connection_timeout_seconds = 5
database = 0

# In-memory cache (Moka) for single-instance deployments or DoS protection layer
# Note: Templates MUST use Redis (distributed) to avoid stale data across instances
[common.cache.moka]
max_capacity = 10000

# TAS-171: Memcached configuration (optional, requires cache-memcached feature)
# Uncomment to use memcached as distributed cache instead of Redis
# [common.cache.memcached]
# url = "${MEMCACHED_URL:-tcp://localhost:11211}"
# connection_timeout_seconds = 5

[common.task_templates]
search_paths = ["config/tasks/**/*.{yml,yaml}"]

# TODO: This section is currently unused. Telemetry is configured via environment
# variables only (TELEMETRY_ENABLED, OTEL_SERVICE_NAME, etc.) because logging must
# be initialized before the TOML config loader runs. See tasker-shared/src/logging.rs.
# Consider removing this section or implementing a re-init pattern if needed.
[common.telemetry]
enabled = true
service_name = "tasker-core"
sample_rate = 0.1
