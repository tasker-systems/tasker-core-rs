# Current environment (overridden by environment-specific config)
environment = "development"

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

[database]
url = "${DATABASE_URL:-postgresql://tasker:tasker@localhost:5432/tasker_development}"
database = "tasker_development"


[database.pool]
max_connections = 30
min_connections = 8
acquire_timeout_seconds = 30
idle_timeout_seconds = 300
max_lifetime_seconds = 3600

# Documentation for database.pool parameters
[database.pool._docs.max_connections]
description = "Maximum number of concurrent database connections in the pool"
type = "u32"
valid_range = "1-1000"
default = "30"
system_impact = "Controls database connection concurrency. Too few = query queuing and slow task processing. Too many = database resource exhaustion and connection overhead."
related = ["database.pool.min_connections", "database.pool.acquire_timeout_seconds"]
example = """
# Production example for high-load orchestration
[database.pool]
max_connections = 50    # Scale based on concurrent task volume
min_connections = 10    # Keep connections warm
"""

[database.pool._docs.max_connections.recommendations]
test = { value = "5", rationale = "Minimal connections for test isolation and fast cleanup" }
development = { value = "10", rationale = "Small pool for local development with low concurrency" }
production = { value = "30-50", rationale = "Scale based on expected concurrent task and step processing load" }

[database.pool._docs.min_connections]
description = "Minimum number of idle connections to maintain in the pool"
type = "u32"
valid_range = "1-100"
default = "8"
system_impact = "Keeps connections warm to avoid cold start latency when processing tasks. Too few = connection setup overhead. Too many = wasted database resources."
related = ["database.pool.max_connections"]

[database.pool._docs.min_connections.recommendations]
test = { value = "2", rationale = "Minimal idle connections for test environments" }
development = { value = "4", rationale = "Small idle pool for local development" }
production = { value = "8-10", rationale = "Maintain warm connections for consistent performance" }


[database.variables]
statement_timeout = 5000

# ============================================================================
# QUEUE CONFIGURATION
# ============================================================================


[queues]
# Backend selection (aligns with UnifiedMessageClient)
backend = "pgmq"                            # Future: "rabbitmq", "redis", etc.
orchestration_namespace = "orchestration"
worker_namespace = "worker"
naming_pattern = "{namespace}_{name}_queue"

# Universal queue configuration (backend-agnostic)
health_check_interval = 60
default_batch_size = 10
max_batch_size = 100
default_visibility_timeout_seconds = 30

# Queue type definitions for orchestration system

[queues.orchestration_queues]
task_requests = "orchestration_task_requests_queue"
task_finalizations = "orchestration_task_finalizations_queue"
step_results = "orchestration_step_results_queue"

# Backend-specific configuration for PGMQ

[queues.pgmq]
poll_interval_ms = 250
shutdown_timeout_seconds = 5
max_retries = 3

# Documentation for queues.pgmq parameters
[queues.pgmq._docs.poll_interval_ms]
description = "Interval in milliseconds between PGMQ queue polling checks"
type = "u64"
valid_range = "50-5000"
default = "250"
system_impact = "Controls polling frequency for queue message checks. Lower values = more responsive but higher database load. Higher values = lower database load but increased latency."
related = ["queues.pgmq.shutdown_timeout_seconds", "queues.default_batch_size"]

[queues.pgmq._docs.poll_interval_ms.recommendations]
test = { value = "100", rationale = "Fast polling for responsive test execution" }
development = { value = "250", rationale = "Balanced polling for local development" }
production = { value = "500-1000", rationale = "Reduce database load while maintaining acceptable latency" }

[queues.pgmq._docs.shutdown_timeout_seconds]
description = "Graceful shutdown timeout for PGMQ operations in seconds"
type = "u64"
valid_range = "1-300"
default = "5"
system_impact = "Time allowed for in-flight PGMQ messages to complete during shutdown. Too short = message loss during restart. Too long = slow deployments."
related = ["queues.pgmq.poll_interval_ms"]

[queues.pgmq._docs.shutdown_timeout_seconds.recommendations]
test = { value = "5", rationale = "Quick shutdown for fast test cycles" }
development = { value = "10", rationale = "Allow message completion in local development" }
production = { value = "30", rationale = "Ensure in-flight messages complete during rolling deployments" }

# Future RabbitMQ configuration (prepared for TAS-40+)

[queues.rabbitmq]
connection_timeout_seconds = 10

# ============================================================================
# SHARED MPSC CHANNELS
# ============================================================================


[shared_channels.event_publisher]
# Internal event publishing queue
# Previously unbounded! Now bounded to prevent event storm OOM
# Handles: System-wide event publishing (step completions, state transitions)
event_queue_buffer_size = 5000


[shared_channels.ffi]
# Ruby FFI event communication channel
# Previously unbounded! Now bounded to provide backpressure
# Handles: Rust â†’ Ruby event communication across FFI boundary
ruby_event_buffer_size = 1000


[mpsc_channels.overflow_policy]
# Overflow policy for channel backpressure (TAS-51)
# Log warning when channel usage exceeds this threshold (0.0-1.0)
log_warning_threshold = 0.8
# Default drop policy: block (provides natural backpressure)
drop_policy = "block"


[mpsc_channels.overflow_policy.metrics]
# Overflow metrics collection
enabled = true
saturation_check_interval_seconds = 60

# ============================================================================
# CIRCUIT BREAKERS
# ============================================================================


[circuit_breakers]
# Global circuit breaker enablement
# Used for database, queue, and API resilience patterns
enabled = true


[circuit_breakers.global_settings]
# Maximum number of circuit breakers system can create
max_circuit_breakers = 50
# Metrics collection interval for monitoring
metrics_collection_interval_seconds = 30
# Enable automatic circuit breaker creation for components
# Minimum time between state transitions to prevent oscillation
min_state_transition_interval_seconds = 1


[circuit_breakers.default_config]
# Default configuration for new circuit breakers
failure_threshold = 5
timeout_seconds = 30
success_threshold = 2


[circuit_breakers.component_configs.pgmq]
# PGMQ queue operations circuit breaker
failure_threshold = 3
timeout_seconds = 15
success_threshold = 2


[circuit_breakers.component_configs.task_readiness]
# Task readiness event processing circuit breaker
failure_threshold = 5
timeout_seconds = 30
success_threshold = 2

# ============================================================================
# TELEMETRY CONFIGURATION
# ============================================================================
#
# OpenTelemetry configuration for metrics and tracing
# Note: Full OpenTelemetry configuration (endpoints, exporters) is managed
# via OTEL_* environment variables following OpenTelemetry spec
#
# ============================================================================

[telemetry]
# Enable telemetry collection (metrics and tracing)
enabled = false
# Service name for telemetry identification
service_name = "tasker-core"
# Sampling rate for traces (0.0-1.0, where 1.0 = 100% sampling)
sample_rate = 1.0

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================
#
# Core system-level settings including dependent system defaults,
# versioning, and recursion limits.
#
# ============================================================================

[system]
# Default dependent system identifier (used when no specific system specified)
default_dependent_system = "default"
# Tasker core version (semantic versioning)
version = "0.1.0"
# Maximum recursion depth for nested workflow operations (prevents stack overflow)
max_recursion_depth = 50

# ============================================================================
# EXECUTION CONFIGURATION
# ============================================================================
#
# Workflow execution parameters including concurrency limits, timeouts,
# and operational constraints.
#
# ============================================================================

[execution]
# CRITICAL: Environment detection (test/development/production)
environment = "${TASKER_ENV:-development}"

# Maximum number of tasks that can execute concurrently
max_concurrent_tasks = 100
# Maximum number of workflow steps that can execute concurrently
max_concurrent_steps = 1000

# Default timeout for task operations (seconds)
default_timeout_seconds = 3600
# Timeout for individual step execution (seconds)
step_execution_timeout_seconds = 300

# Maximum attempts to discover ready steps before failure
max_discovery_attempts = 3
# Batch size for step discovery operations
step_batch_size = 10

# Maximum retry attempts for failed steps
max_retries = 3
# Maximum number of steps allowed in a single workflow (prevents runaway expansion)
max_workflow_steps = 1000
# Connection timeout for API and database operations (seconds)
connection_timeout_seconds = 10

# ============================================================================
# TASK TEMPLATES CONFIGURATION
# ============================================================================
#
# Template discovery paths for task definitions (YAML/JSON workflow files).
# Supports glob patterns for flexible template organization.
#
# ============================================================================

[task_templates]
# Glob patterns for discovering task template files
# Example patterns:
#   - "config/task_templates/*.{yml,yaml}"   - Top-level templates only
#   - "config/task_templates/**/*.{yml,yaml}" - Recursive search
#   - Multiple paths for different template categories
search_paths = [
    "config/task_templates/*.{yml,yaml}",
    "config/task_templates/**/*.{yml,yaml}"
]

# ============================================================================
# DECISION POINT CONFIGURATION (TAS-53)
# ============================================================================
#
# Configuration for dynamic workflow decision points that enable runtime
# conditional branching and step creation based on handler outcomes.
#
# Decision points allow workflows to:
# - Make runtime decisions based on step results
# - Dynamically create workflow branches
# - Support nested decision points
# - Maintain DAG integrity with cycle detection
#
# ============================================================================

[decision_points]
# Enable decision point processing
enabled = true

# Maximum number of steps that can be created from a single decision
# Prevents runaway step creation from misconfigured handlers
max_steps_per_decision = 50

# Maximum depth of nested decision points
# Prevents infinite recursion in decision point chains
max_decision_depth = 10

# Warn when decision point creates more than this many steps
# Helps identify potentially misconfigured decision handlers
warn_threshold_steps = 20

# Warn when decision depth exceeds this threshold
# Helps identify overly complex decision chains
warn_threshold_depth = 5

# Enable detailed decision point logging
enable_detailed_logging = false

# Enable decision point metrics collection
enable_metrics = true
