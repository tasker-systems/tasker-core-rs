# Tasker V2 Worker Configuration (Base)
# Worker-specific settings

[worker]
worker_id = "worker-default-001"
worker_type = "general"

[worker._docs.worker_id]
description = "Unique identifier for this worker instance"
type = "String"
valid_range = "non-empty string"
system_impact = "Used in logging, metrics, and step claim attribution; must be unique across all worker instances in a cluster"

[worker._docs.worker_type]
description = "Worker type classification for routing and reporting"
type = "String"
valid_range = "non-empty string"
system_impact = "Used to match worker capabilities with step handler requirements; 'general' handles all step types"

# TAS-75: Worker circuit breakers configuration
[worker.circuit_breakers.ffi_completion_send]
failure_threshold = 5
recovery_timeout_seconds = 5
success_threshold = 2
slow_send_threshold_ms = 100

[worker.circuit_breakers.ffi_completion_send._docs.slow_send_threshold_ms]
description = "Threshold in milliseconds above which FFI completion channel sends are logged as slow"
type = "u32"
valid_range = "10-10000"
system_impact = "Observability: identifies when the FFI completion channel is under pressure from slow consumers"

[worker.event_systems.worker]
system_id = "worker-event-system"
deployment_mode = "Hybrid"

[worker.event_systems.worker._docs.system_id]
description = "Unique identifier for the worker event system instance"
type = "String"
valid_range = "non-empty string"
system_impact = "Used in logging and metrics to distinguish this event system from others"

[worker.event_systems.worker._docs.deployment_mode]
description = "Event delivery mode: 'Hybrid' (LISTEN/NOTIFY + polling fallback), 'EventDrivenOnly', or 'PollingOnly'"
type = "DeploymentMode"
valid_range = "Hybrid | EventDrivenOnly | PollingOnly"
system_impact = "Hybrid is recommended; EventDrivenOnly has lowest latency but no fallback; PollingOnly has highest latency but no LISTEN/NOTIFY dependency"

[worker.event_systems.worker.timing]
health_check_interval_seconds = 30
fallback_polling_interval_seconds = 2
visibility_timeout_seconds = 30
processing_timeout_seconds = 60
claim_timeout_seconds = 300

[worker.event_systems.worker.processing]
max_concurrent_operations = 100
batch_size = 20
max_retries = 3

[worker.event_systems.worker.processing.backoff]
initial_delay_ms = 100
max_delay_ms = 10000
multiplier = 2.0
jitter_percent = 0.1

[worker.event_systems.worker.health]
enabled = true
performance_monitoring_enabled = true
max_consecutive_errors = 10
error_rate_threshold_per_minute = 20

[worker.event_systems.worker.metadata.in_process_events]
ffi_integration_enabled = true
deduplication_cache_size = 10000

[worker.event_systems.worker.metadata.listener]
retry_interval_seconds = 5
max_retry_attempts = 5
event_timeout_seconds = 60
batch_processing = true
connection_timeout_seconds = 30

[worker.event_systems.worker.metadata.fallback_poller]
enabled = true
polling_interval_ms = 1000
batch_size = 20
age_threshold_seconds = 5
max_age_hours = 24
visibility_timeout_seconds = 30
supported_namespaces = []

[worker.event_systems.worker.metadata.resource_limits]
max_memory_mb = 4096
max_cpu_percent = 80.0
max_database_connections = 100
max_queue_connections = 50

[worker.step_processing]
claim_timeout_seconds = 300
max_retries = 3
max_concurrent_steps = 50

[worker.step_processing._docs.claim_timeout_seconds]
description = "Maximum time in seconds a step claim remains valid before expiring"
type = "u32"
valid_range = "1-3600"
system_impact = "If a worker fails to complete a step within this window, the claim expires and the step becomes available for retry"

[worker.step_processing._docs.max_retries]
description = "Maximum number of retry attempts for a failed step at the worker level"
type = "u32"
valid_range = "0-100"
system_impact = "Worker-level retry cap; interacts with the orchestration-level execution.max_retries"

[worker.step_processing._docs.max_concurrent_steps]
description = "Maximum number of steps this worker processes simultaneously"
type = "u32"
valid_range = "1-100000"
system_impact = "Primary worker concurrency control; bounded by semaphore in HandlerDispatchService"

[worker.health_monitoring]
health_check_interval_seconds = 30
performance_monitoring_enabled = true
error_rate_threshold = 0.05

[worker.health_monitoring._docs.health_check_interval_seconds]
description = "Interval in seconds between worker health self-checks"
type = "u32"
valid_range = "1-3600"
system_impact = "Controls how frequently the worker evaluates its own health status for readiness probes"

[worker.health_monitoring._docs.error_rate_threshold]
description = "Error rate threshold (0.0-1.0) above which the worker reports as unhealthy"
type = "f64"
valid_range = "0.0-1.0"
system_impact = "A value of 0.05 means the worker becomes unhealthy if more than 5% of recent step executions fail"

[worker.mpsc_channels.command_processor]
command_buffer_size = 2000

[worker.mpsc_channels.event_systems]
event_channel_buffer_size = 2000

[worker.mpsc_channels.event_subscribers]
completion_buffer_size = 1000
result_buffer_size = 1000

# TAS-65: In-process event bus configuration for fast domain event delivery
[worker.mpsc_channels.in_process_events]
broadcast_buffer_size = 2000
log_subscriber_errors = true
dispatch_timeout_ms = 5000

[worker.mpsc_channels.event_listeners]
pgmq_event_buffer_size = 10000

# TAS-65/TAS-69: Domain Event System MPSC Configuration
[worker.mpsc_channels.domain_events]
command_buffer_size = 1000
shutdown_drain_timeout_ms = 5000
log_dropped_events = true

# TAS-67: Handler Dispatch System MPSC Configuration
[worker.mpsc_channels.handler_dispatch]
dispatch_buffer_size = 1000
completion_buffer_size = 1000
max_concurrent_handlers = 10
handler_timeout_ms = 30000

# TAS-75 Phase 4: Worker Load Shedding Configuration
[worker.mpsc_channels.handler_dispatch.load_shedding]
enabled = true
# Refuse step claims when handler capacity exceeds this threshold (0-100%)
capacity_threshold_percent = 80.0
# Log warning when approaching threshold (0-100%)
warning_threshold_percent = 70.0

# TAS-67: FFI Dispatch Channel Configuration
[worker.mpsc_channels.ffi_dispatch]
dispatch_buffer_size = 1000
completion_timeout_ms = 30000
# TAS-67 Risk Mitigation Phase 1: Fire-and-forget callback timeout
# Prevents indefinite blocking of FFI threads during domain event publishing
callback_timeout_ms = 5000
# TAS-67 Risk Mitigation Phase 2: Starvation warning threshold
# Logs warnings when pending events exceed this age (milliseconds)
# Enables proactive detection before timeout occurs
starvation_warning_threshold_ms = 10000
# TAS-67 Risk Mitigation Phase 2: Completion send timeout
# Max time to retry sending completion results when channel is full
# Uses try_send with retry loop instead of blocking send
completion_send_timeout_ms = 10000

# Orchestration Client Configuration
# How the worker connects to the orchestration API
[worker.orchestration_client]
base_url = "http://localhost:8080"
timeout_ms = 30000
max_retries = 3

[worker.orchestration_client._docs.base_url]
description = "Base URL of the orchestration REST API that this worker reports to"
type = "String"
valid_range = "valid HTTP(S) URL"
system_impact = "Workers send step completion results and health reports to this endpoint"
related = ["orchestration.web.bind_address"]

[worker.orchestration_client._docs.base_url.recommendations]
test = { value = "http://localhost:8080", rationale = "Local orchestration for testing" }
production = { value = "http://orchestration:8080", rationale = "Container-internal DNS in Kubernetes/Docker" }

[worker.orchestration_client._docs.timeout_ms]
description = "HTTP request timeout in milliseconds for orchestration API calls"
type = "u32"
valid_range = "100-300000"
system_impact = "Worker-to-orchestration calls exceeding this timeout fail and may be retried"

[worker.orchestration_client._docs.max_retries]
description = "Maximum retry attempts for failed orchestration API calls"
type = "u32"
valid_range = "0-10"
system_impact = "Retries use backoff; higher values improve resilience to transient network issues"

[worker.web]
enabled = true
bind_address = "${TASKER_WEB_BIND_ADDRESS:-0.0.0.0:8081}"
request_timeout_ms = 30000

[worker.web._docs.enabled]
description = "Enable the REST API server for the worker service"
type = "bool"
valid_range = "true/false"
system_impact = "When false, no HTTP endpoints are available; the worker operates via messaging only"

[worker.web._docs.bind_address]
description = "Socket address for the worker REST API server"
type = "String"
valid_range = "host:port"
system_impact = "Must not conflict with orchestration.web.bind_address when co-located; default 8081"

[worker.web._docs.bind_address.recommendations]
test = { value = "0.0.0.0:8081", rationale = "Default port offset from orchestration (8080)" }
production = { value = "0.0.0.0:8081", rationale = "Standard worker port; use TASKER_WEB_BIND_ADDRESS env var to override" }

[worker.web.database_pools]
web_api_pool_size = 10
web_api_max_connections = 15
web_api_connection_timeout_seconds = 30
web_api_idle_timeout_seconds = 600
max_total_connections_hint = 25

# TAS-61: Removed [worker.web.cors] - CORS uses hardcoded tower_http::cors::Any in middleware

[worker.web.auth]
enabled = false
jwt_issuer = "tasker-worker"
jwt_audience = "worker-api"
jwt_token_expiry_hours = 24
jwt_private_key = ""
jwt_public_key = "${TASKER_JWT_PUBLIC_KEY:-}"
jwt_public_key_path = "${TASKER_JWT_PUBLIC_KEY_PATH:-}"
api_key = ""
api_key_header = "X-API-Key"

[worker.web.auth._docs.enabled]
description = "Enable authentication for the worker REST API"
type = "bool"
valid_range = "true/false"
system_impact = "When false, all worker API endpoints are unauthenticated"

# TAS-169: Template routes moved to /v1/templates, cache operations removed
# Old routes removed: DELETE /templates/cache, POST /templates/cache/maintain,
# POST /templates/{namespace}/{name}/{version}/refresh

# TAS-61: Removed [worker.web.rate_limiting] - no rate limiting middleware implemented

[worker.web.resilience]
circuit_breaker_enabled = true
# TAS-61: Removed request_timeout_seconds - timeout hardcoded in middleware (30s)
# TAS-61: Removed max_concurrent_requests - no concurrency limiting implemented

# TAS-177: gRPC API Configuration
[worker.grpc]
enabled = true
# Note: Default changed from 9100 to 9191 to avoid potential conflicts
bind_address = "${TASKER_WORKER_GRPC_BIND_ADDRESS:-0.0.0.0:9191}"
tls_enabled = false
keepalive_interval_seconds = 30
keepalive_timeout_seconds = 20
max_concurrent_streams = 1000
max_frame_size = 16384
enable_reflection = true
enable_health_service = true

[worker.grpc._docs.enabled]
description = "Enable the gRPC API server for the worker service"
type = "bool"
valid_range = "true/false"
system_impact = "When false, no gRPC endpoints are available; clients must use REST"

[worker.grpc._docs.bind_address]
description = "Socket address for the worker gRPC server"
type = "String"
valid_range = "host:port"
system_impact = "Must not conflict with the REST API or orchestration gRPC ports; default 9191"

[worker.grpc._docs.max_concurrent_streams]
description = "Maximum number of concurrent gRPC streams per connection"
type = "u32"
valid_range = "1-10000"
system_impact = "Workers typically handle more concurrent streams than orchestration; default 1000 reflects this"
