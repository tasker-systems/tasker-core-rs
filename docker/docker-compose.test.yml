# =============================================================================
# Test Compose Configuration - Local Integration Testing (OPTIMIZED)
# =============================================================================
# Optimized for local development and testing:
# - Fast debug builds with optimized Dockerfile.test-local
# - BuildKit cache configuration for faster rebuilds
# - Always builds from local source (no cached images)
# - Grafana LGTM stack for OpenTelemetry observability (TAS-65 Phase 1)
# - Minimal resource usage
# - Quick startup times
#
# Usage:
# export DOCKER_BUILDKIT=1
# export COMPOSE_DOCKER_CLI_BUILD=1
# docker-compose -f docker/docker-compose.test.yml up --build
#
# Access:
# - Grafana UI: http://localhost:3000 (admin/admin)
# - PostgreSQL: localhost:5432
# - RabbitMQ AMQP: localhost:5672 (tasker/tasker)
# - RabbitMQ Management: http://localhost:15672 (tasker/tasker)
# - OTLP gRPC: localhost:4317
# - Prometheus: http://localhost:9090

services:
  # ==========================================================================
  # Grafana LGTM Stack - All-in-One Observability (TAS-65 Phase 1)
  # ==========================================================================
  # Includes: Loki (logs), Grafana (UI), Tempo (traces), Mimir (metrics)
  # OpenTelemetry integration for distributed tracing and metrics
  observability:
    image: grafana/otel-lgtm:latest
    container_name: tasker-test-observability
    ports:
      # Grafana UI
      - "3000:3000"
      # OTLP receivers
      - "4317:4317" # OTLP gRPC
      - "4318:4318" # OTLP HTTP
      # Prometheus metrics
      - "9090:9090"
      # Tempo query
      - "3200:3200"
    environment:
      # Enable all features
      ENABLE_LOGS: "true"
      ENABLE_METRICS: "true"
      ENABLE_TRACES: "true"
    volumes:
      # Persist Grafana dashboards and data sources
      - grafana_data:/var/lib/grafana
      # Persist Tempo traces
      - tempo_data:/var/tempo
      # Persist Loki logs
      - loki_data:/loki
    # TAS-94: Increase memory to prevent OOM under telemetry load
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "test -f /tmp/ready || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 10s
    networks:
      - tasker-test

  # ==========================================================================
  # RabbitMQ Message Broker (TAS-133: Alternative Messaging Provider)
  # ==========================================================================
  # Provides AMQP 0.9.1 message broker for testing RabbitMQ messaging provider.
  # Used alongside PGMQ to validate messaging abstraction works with both backends.
  rabbitmq:
    image: rabbitmq:4-management-alpine
    container_name: tasker-test-rabbitmq
    ports:
      - "5672:5672" # AMQP protocol
      - "15672:15672" # Management UI
    environment:
      RABBITMQ_DEFAULT_USER: tasker
      RABBITMQ_DEFAULT_PASS: tasker
      RABBITMQ_DEFAULT_VHOST: /
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - tasker-test

  # ==========================================================================
  # PostgreSQL Database with PGMQ Extension
  # ==========================================================================
  # TAS-73: Optimized for multi-instance cluster testing
  # - max_connections=500: Support multiple orchestration + worker instances
  # - shared_buffers=256MB: Cache for frequently accessed data
  # - effective_cache_size=1GB: Planner hint for available memory
  # - work_mem=16MB: Memory for complex queries (sorts, hashes)
  # - maintenance_work_mem=128MB: Memory for maintenance ops
  # - wal_buffers=16MB: Write-ahead log buffer
  # - random_page_cost=1.1: Assume SSD storage
  # - checkpoint_completion_target=0.9: Spread checkpoint I/O
  # - default_statistics_target=100: Better query planning
  postgres:
    build:
      context: ..
      dockerfile: docker/db/Dockerfile
    command:
      - "postgres"
      # Connection handling
      - "-c"
      - "max_connections=500"
      # Memory settings
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "effective_cache_size=1GB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=128MB"
      # WAL and checkpoint settings
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      # Query planning
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "default_statistics_target=100"
      # Logging (useful for debugging connection issues)
      - "-c"
      - "log_connections=on"
      - "-c"
      - "log_disconnections=on"
    environment:
      POSTGRES_DB: tasker_rust_test
      POSTGRES_USER: tasker
      POSTGRES_PASSWORD: tasker
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5432:5432"
    volumes:
      # Note: pg18+ uses /var/lib/postgresql with version-specific subdirectories
      # See: https://github.com/docker-library/postgres/pull/1259
      - postgres_data:/var/lib/postgresql
    # TAS-73: Increase memory for cluster workloads
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U tasker -d tasker_rust_test"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - tasker-test

  # ==========================================================================
  # Dragonfly Cache (TAS-171: Redis + Memcached compatible)
  # ==========================================================================
  # Dragonfly is a Redis-compatible in-memory data store with better
  # multi-threaded performance. Supports both Redis (6379) and Memcached (11211)
  # protocols, allowing us to test both cache backends with a single service.
  dragonfly:
    image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
    container_name: tasker-test-dragonfly
    command: ["--memcached_port", "11211"]
    ports:
      - "6379:6379"   # Redis protocol
      - "11211:11211" # Memcached protocol
    volumes:
      - dragonfly_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6379", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - tasker-test

volumes:
  postgres_data:
  grafana_data:
  tempo_data:
  loki_data:
  rabbitmq_data:
  dragonfly_data:

networks:
  tasker-test:
    driver: bridge
