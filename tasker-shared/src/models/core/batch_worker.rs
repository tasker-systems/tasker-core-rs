//! # Batch Worker Models (TAS-59)
//!
//! Data structures for batch processing worker initialization and execution.
//!
//! ## Architecture Overview
//!
//! Batch processing involves several distinct data structures at different lifecycle stages:
//!
//! 1. **Template Configuration** (`BatchConfiguration` in `task_template.rs`)
//!    - Static configuration defined in YAML templates
//!    - Defines batch size, parallelism, cursor field, etc.
//!    - Lives in the template, not in runtime data
//!
//! 2. **Runtime Cursor Data** (`CursorConfig` in `messaging/execution_types.rs`)
//!    - Dynamic data created by batchable handler at runtime
//!    - Generated by analyzing dataset size against template configuration
//!    - Transmitted via messaging system in `BatchProcessingOutcome`
//!    - Ephemeral - exists only during task execution
//!
//! 3. **Worker Initialization** (`BatchWorkerInputs` in this file)
//!    - Persistent data stored in `workflow_steps.initialization` (JSONB)
//!    - Composes runtime cursor data with template metadata
//!    - Provides workers with everything needed for execution
//!    - Type-safe structure preventing manual JSON construction errors
//!
//! ## Data Flow
//!
//! ```text
//! Template YAML
//!     ↓
//! BatchConfiguration (static)
//!     ↓
//! Batchable Handler analyzes dataset
//!     ↓
//! CursorConfig instances (runtime, ephemeral)
//!     ↓
//! BatchProcessingOutcome (transmitted via messaging)
//!     ↓
//! BatchWorkerInputs (persisted in DB for worker execution)
//!     ↓
//! Worker instances execute with cursor + metadata
//! ```

use serde::{Deserialize, Serialize};

use crate::messaging::CursorConfig;
use crate::models::core::task_template::{BatchConfiguration, FailureStrategy};

/// Initialization inputs for batch worker instances
///
/// This structure is serialized to JSONB and stored in `workflow_steps.initialization`
/// for dynamically created batch worker steps. It composes:
/// - Runtime cursor configuration (where to process)
/// - Template metadata (how to process)
///
/// ## Storage
///
/// Merged into the worker's handler initialization JSONB:
/// ```sql
/// UPDATE workflow_steps
/// SET initialization = initialization || $batch_worker_inputs_json
/// WHERE workflow_step_uuid = $worker_uuid
/// ```
///
/// ## Ruby Worker Access
///
/// Ruby workers receive this data via the initialization hash:
/// ```ruby
/// def execute(inputs)
///   cursor = inputs[:cursor]
///   batch_id = cursor[:batch_id]
///   start_cursor = cursor[:start_cursor]
///   checkpoint_interval = inputs[:batch_metadata][:checkpoint_interval]
///   # ... process batch with checkpointing
/// end
/// ```
///
/// ## Example JSON Structure
///
/// ```json
/// {
///   "cursor": {
///     "batch_id": "batch_001",
///     "start_cursor": 0,
///     "end_cursor": 1000,
///     "batch_size": 1000
///   },
///   "batch_metadata": {
///     "checkpoint_interval": 100,
///     "cursor_field": "id",
///     "failure_strategy": "ContinueOnFailure"
///   },
///   "is_no_op": false
/// }
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct BatchWorkerInputs {
    /// Cursor configuration defining this worker's processing range
    ///
    /// Created by the batchable handler after analyzing dataset size.
    /// Tells the worker:
    /// - Which batch it's responsible for (`batch_id`)
    /// - Start and end positions in the dataset
    /// - How many items to process
    pub cursor: CursorConfig,

    /// Batch processing metadata from template configuration
    ///
    /// Derived from the template's `BatchConfiguration` and provides:
    /// - Checkpointing frequency
    /// - Cursor field name for queries
    /// - Failure handling strategy
    pub batch_metadata: BatchMetadata,

    /// Explicit flag indicating if this is a no-op/placeholder worker
    ///
    /// Set by orchestration outcome handlers based on BatchProcessingOutcome type:
    /// - `true` for NoBatches outcome (placeholder worker with cursor 0-0)
    /// - `false` for WithBatches outcome (real worker that should process data)
    ///
    /// Workers should check this flag FIRST before any processing logic.
    /// If `true`, worker should immediately return success without processing.
    ///
    /// This eliminates the need for workers to infer no-op status from cursor
    /// positions or batch_id patterns - orchestration explicitly declares intent.
    pub is_no_op: bool,
}

/// Batch processing metadata passed to worker instances
///
/// This structure extracts relevant template configuration that workers
/// need during execution. It's deliberately separate from `BatchConfiguration`
/// because workers don't need parallelism settings, batch size calculation logic, etc.
///
/// Workers need to know:
/// - **How often** to checkpoint (`checkpoint_interval`)
/// - **What field** to use for cursor queries (`cursor_field`)
/// - **How to handle** failures in their batch (`failure_strategy`)
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct BatchMetadata {
    /// Number of items between progress checkpoints
    ///
    /// Workers should update `workflow_steps.results.batch_cursor.last_checkpoint`
    /// after processing this many items. This enables:
    /// - Resumability after failures (TAS-49 ResetForRetry preserves cursors)
    /// - Staleness detection (TAS-59 Phase 3 checkpoint health monitoring)
    /// - Progress observability
    ///
    /// Example: With `checkpoint_interval = 100`, update cursor after items:
    /// 100, 200, 300, ..., batch_size
    pub checkpoint_interval: u32,

    /// Database field name used for cursor-based pagination
    ///
    /// Workers use this to construct queries like:
    /// ```sql
    /// SELECT * FROM table
    /// WHERE cursor_field > $start_cursor
    ///   AND cursor_field <= $end_cursor
    /// ORDER BY cursor_field
    /// LIMIT $batch_size
    /// ```
    ///
    /// Common values: `"id"`, `"created_at"`, `"sequence_number"`
    pub cursor_field: String,

    /// How this worker should handle failures during batch processing
    ///
    /// Determines worker behavior when processing individual items fails:
    /// - `ContinueOnFailure`: Log errors, continue processing remaining items
    /// - `FailFast`: Stop immediately on first error, transition to Error state
    /// - `Isolate`: Mark batch for manual investigation, continue other batches
    pub failure_strategy: FailureStrategy,
}

impl BatchWorkerInputs {
    /// Create new batch worker inputs from runtime cursor and template configuration
    ///
    /// This is the primary constructor used by `BatchProcessingService` when
    /// creating worker instances. It composes:
    /// - Runtime data: cursor positions from batchable handler analysis
    /// - Template data: configuration from the task template
    /// - Orchestration context: explicit no-op status from outcome handler
    ///
    /// # Arguments
    ///
    /// * `cursor_config` - Runtime cursor configuration with batch boundaries
    /// * `batch_config` - Template configuration with processing rules
    /// * `is_no_op` - Explicit flag set by outcome handler (true for NoBatches, false for WithBatches)
    ///
    /// # Example
    ///
    /// ```rust
    /// use tasker_shared::models::core::batch_worker::BatchWorkerInputs;
    /// use tasker_shared::messaging::CursorConfig;
    /// use tasker_shared::models::core::task_template::{BatchConfiguration, FailureStrategy};
    /// use serde_json::json;
    ///
    /// let cursor = CursorConfig {
    ///     batch_id: "batch_001".to_string(),
    ///     start_cursor: json!(0),
    ///     end_cursor: json!(1000),
    ///     batch_size: 1000,
    /// };
    ///
    /// let batch_config = BatchConfiguration {
    ///     batch_size: 1000,
    ///     parallelism: 5,
    ///     cursor_field: "id".to_string(),
    ///     checkpoint_interval: 100,
    ///     worker_template: "batch_worker".to_string(),
    ///     failure_strategy: FailureStrategy::ContinueOnFailure,
    /// };
    ///
    /// // Real worker with data to process
    /// let inputs = BatchWorkerInputs::new(cursor, &batch_config, false);
    ///
    /// assert_eq!(inputs.cursor.batch_id, "batch_001");
    /// assert_eq!(inputs.batch_metadata.checkpoint_interval, 100);
    /// assert_eq!(inputs.is_no_op, false);
    /// ```
    #[must_use]
    pub fn new(cursor_config: CursorConfig, batch_config: &BatchConfiguration, is_no_op: bool) -> Self {
        Self {
            cursor: cursor_config,
            batch_metadata: BatchMetadata {
                checkpoint_interval: batch_config.checkpoint_interval,
                cursor_field: batch_config.cursor_field.clone(),
                failure_strategy: batch_config.failure_strategy.clone(),
            },
            is_no_op,
        }
    }

    /// Convert to JSON value for merging into handler initialization
    ///
    /// This produces the JSONB structure that gets merged with the worker's
    /// handler initialization data before being stored in the database.
    ///
    /// # Example
    ///
    /// ```rust
    /// # use tasker_shared::models::core::batch_worker::BatchWorkerInputs;
    /// # use tasker_shared::messaging::CursorConfig;
    /// # use tasker_shared::models::core::task_template::{BatchConfiguration, FailureStrategy};
    /// # use serde_json::json;
    /// # let cursor = CursorConfig {
    /// #     batch_id: "batch_001".to_string(),
    /// #     start_cursor: json!(0),
    /// #     end_cursor: json!(1000),
    /// #     batch_size: 1000,
    /// # };
    /// # let batch_config = BatchConfiguration {
    /// #     batch_size: 1000,
    /// #     parallelism: 5,
    /// #     cursor_field: "id".to_string(),
    /// #     checkpoint_interval: 100,
    /// #     worker_template: "batch_worker".to_string(),
    /// #     failure_strategy: FailureStrategy::ContinueOnFailure,
    /// # };
    /// let inputs = BatchWorkerInputs::new(cursor, &batch_config);
    /// let json_value = inputs.to_value();
    ///
    /// // Can be merged with handler initialization
    /// let mut init = serde_json::json!({
    ///     "some_handler_param": "value"
    /// });
    /// if let Some(obj) = init.as_object_mut() {
    ///     for (key, value) in json_value.as_object().unwrap() {
    ///         obj.insert(key.clone(), value.clone());
    ///     }
    /// }
    /// ```
    #[must_use]
    pub fn to_value(&self) -> serde_json::Value {
        serde_json::to_value(self).expect("BatchWorkerInputs serialization should never fail")
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    fn sample_cursor_config() -> CursorConfig {
        CursorConfig {
            batch_id: "batch_001".to_string(),
            start_cursor: json!(0),
            end_cursor: json!(1000),
            batch_size: 1000,
        }
    }

    fn sample_batch_configuration() -> BatchConfiguration {
        BatchConfiguration {
            batch_size: 1000,
            parallelism: 5,
            cursor_field: "id".to_string(),
            checkpoint_interval: 100,
            worker_template: "batch_worker_template".to_string(),
            failure_strategy: FailureStrategy::ContinueOnFailure,
        }
    }

    #[test]
    fn test_batch_worker_inputs_creation() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();

        let inputs = BatchWorkerInputs::new(cursor.clone(), &batch_config, false);

        assert_eq!(inputs.cursor.batch_id, "batch_001");
        assert_eq!(inputs.cursor.batch_size, 1000);
        assert_eq!(inputs.batch_metadata.checkpoint_interval, 100);
        assert_eq!(inputs.batch_metadata.cursor_field, "id");
        assert_eq!(
            inputs.batch_metadata.failure_strategy,
            FailureStrategy::ContinueOnFailure
        );
        assert_eq!(inputs.is_no_op, false);
    }

    #[test]
    fn test_batch_worker_inputs_serialization() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();
        let inputs = BatchWorkerInputs::new(cursor, &batch_config, false);

        let json_value = inputs.to_value();

        // Verify structure
        assert!(json_value.get("cursor").is_some());
        assert!(json_value.get("batch_metadata").is_some());
        assert!(json_value.get("is_no_op").is_some());

        // Verify cursor data
        let cursor_obj = json_value.get("cursor").unwrap();
        assert_eq!(cursor_obj.get("batch_id").unwrap(), "batch_001");
        assert_eq!(cursor_obj.get("batch_size").unwrap(), 1000);

        // Verify metadata
        let metadata_obj = json_value.get("batch_metadata").unwrap();
        assert_eq!(metadata_obj.get("checkpoint_interval").unwrap(), 100);
        assert_eq!(metadata_obj.get("cursor_field").unwrap(), "id");

        // Verify is_no_op flag
        assert_eq!(json_value.get("is_no_op").unwrap(), false);
    }

    #[test]
    fn test_batch_worker_inputs_deserialization() {
        let json_str = r#"{
            "cursor": {
                "batch_id": "batch_002",
                "start_cursor": 1000,
                "end_cursor": 2000,
                "batch_size": 1000
            },
            "batch_metadata": {
                "checkpoint_interval": 50,
                "cursor_field": "created_at",
                "failure_strategy": "fail_fast"
            },
            "is_no_op": true
        }"#;

        let inputs: BatchWorkerInputs = serde_json::from_str(json_str).unwrap();

        assert_eq!(inputs.cursor.batch_id, "batch_002");
        assert_eq!(inputs.batch_metadata.checkpoint_interval, 50);
        assert_eq!(inputs.batch_metadata.cursor_field, "created_at");
        assert_eq!(
            inputs.batch_metadata.failure_strategy,
            FailureStrategy::FailFast
        );
        assert_eq!(inputs.is_no_op, true);
    }

    #[test]
    fn test_batch_worker_inputs_round_trip() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();
        let original = BatchWorkerInputs::new(cursor, &batch_config, true);

        // Serialize to JSON
        let json_value = original.to_value();

        // Deserialize back
        let deserialized: BatchWorkerInputs = serde_json::from_value(json_value).unwrap();

        assert_eq!(original, deserialized);
    }

    #[test]
    fn test_different_failure_strategies() {
        let cursor = sample_cursor_config();

        let strategies = vec![
            FailureStrategy::ContinueOnFailure,
            FailureStrategy::FailFast,
            FailureStrategy::Isolate,
        ];

        for strategy in strategies {
            let mut batch_config = sample_batch_configuration();
            batch_config.failure_strategy = strategy.clone();

            let inputs = BatchWorkerInputs::new(cursor.clone(), &batch_config, false);
            assert_eq!(inputs.batch_metadata.failure_strategy, strategy);
        }
    }

    #[test]
    fn test_no_op_flag_serialization() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();

        // Test with is_no_op = true
        let no_op_inputs = BatchWorkerInputs::new(cursor.clone(), &batch_config, true);
        let no_op_json = no_op_inputs.to_value();
        assert_eq!(no_op_json.get("is_no_op").unwrap(), true);

        // Test with is_no_op = false
        let real_inputs = BatchWorkerInputs::new(cursor, &batch_config, false);
        let real_json = real_inputs.to_value();
        assert_eq!(real_json.get("is_no_op").unwrap(), false);
    }
}
