//! # Batch Worker Models (TAS-59)
//!
//! Data structures for batch processing worker initialization and execution.
//!
//! ## Architecture Overview
//!
//! Batch processing involves several distinct data structures at different lifecycle stages:
//!
//! 1. **Template Configuration** (`BatchConfiguration` in `task_template.rs`)
//!    - Static configuration defined in YAML templates
//!    - Defines batch size, parallelism, cursor field, etc.
//!    - Lives in the template, not in runtime data
//!
//! 2. **Runtime Cursor Data** (`CursorConfig` in `messaging/execution_types.rs`)
//!    - Dynamic data created by batchable handler at runtime
//!    - Generated by analyzing dataset size against template configuration
//!    - Transmitted via messaging system in `BatchProcessingOutcome`
//!    - Ephemeral - exists only during task execution
//!
//! 3. **Worker Initialization** (`BatchWorkerInputs` in this file)
//!    - Persistent data stored in `workflow_steps.initialization` (JSONB)
//!    - Composes runtime cursor data with template metadata
//!    - Provides workers with everything needed for execution
//!    - Type-safe structure preventing manual JSON construction errors
//!
//! ## Data Flow
//!
//! ```text
//! Template YAML
//!     ↓
//! BatchConfiguration (static)
//!     ↓
//! Batchable Handler analyzes dataset
//!     ↓
//! CursorConfig instances (runtime, ephemeral)
//!     ↓
//! BatchProcessingOutcome (transmitted via messaging)
//!     ↓
//! BatchWorkerInputs (persisted in DB for worker execution)
//!     ↓
//! Worker instances execute with cursor + metadata
//! ```

use serde::{Deserialize, Serialize};

use crate::messaging::CursorConfig;
use crate::models::core::task_template::{BatchConfiguration, FailureStrategy};

/// Initialization inputs for batch worker instances
///
/// This structure is serialized to JSONB and stored in `workflow_steps.initialization`
/// for dynamically created batch worker steps. It composes:
/// - Runtime cursor configuration (where to process)
/// - Template metadata (how to process)
///
/// ## Storage
///
/// Merged into the worker's handler initialization JSONB:
/// ```sql
/// UPDATE workflow_steps
/// SET initialization = initialization || $batch_worker_inputs_json
/// WHERE workflow_step_uuid = $worker_uuid
/// ```
///
/// ## Ruby Worker Access
///
/// Ruby workers receive this data via the initialization hash:
/// ```ruby
/// def execute(inputs)
///   cursor = inputs[:cursor]
///   batch_id = cursor[:batch_id]
///   start_cursor = cursor[:start_cursor]
///   checkpoint_interval = inputs[:batch_metadata][:checkpoint_interval]
///   # ... process batch with checkpointing
/// end
/// ```
///
/// ## Example JSON Structure
///
/// ```json
/// {
///   "cursor": {
///     "batch_id": "batch_001",
///     "start_cursor": 0,
///     "end_cursor": 1000,
///     "batch_size": 1000
///   },
///   "batch_metadata": {
///     "checkpoint_interval": 100,
///     "cursor_field": "id",
///     "failure_strategy": "ContinueOnFailure"
///   },
///   "is_no_op": false
/// }
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct BatchWorkerInputs {
    /// Cursor configuration defining this worker's processing range
    ///
    /// Created by the batchable handler after analyzing dataset size.
    /// Tells the worker:
    /// - Which batch it's responsible for (`batch_id`)
    /// - Start and end positions in the dataset
    /// - How many items to process
    pub cursor: CursorConfig,

    /// Batch processing metadata from template configuration
    ///
    /// Derived from the template's `BatchConfiguration` and provides:
    /// - Checkpointing frequency
    /// - Cursor field name for queries
    /// - Failure handling strategy
    pub batch_metadata: BatchMetadata,

    /// Explicit flag indicating if this is a no-op/placeholder worker
    ///
    /// Set by orchestration outcome handlers based on BatchProcessingOutcome type:
    /// - `true` for NoBatches outcome (placeholder worker with cursor 0-0)
    /// - `false` for WithBatches outcome (real worker that should process data)
    ///
    /// Workers should check this flag FIRST before any processing logic.
    /// If `true`, worker should immediately return success without processing.
    ///
    /// This eliminates the need for workers to infer no-op status from cursor
    /// positions or batch_id patterns - orchestration explicitly declares intent.
    pub is_no_op: bool,
}

/// Batch processing metadata passed to worker instances
///
/// This structure extracts relevant template configuration that workers
/// need during execution. It's deliberately separate from `BatchConfiguration`
/// because workers don't need parallelism settings, batch size calculation logic, etc.
///
/// Workers need to know:
/// - **How often** to checkpoint (`checkpoint_interval`)
/// - **What field** to use for cursor queries (`cursor_field`)
/// - **How to handle** failures in their batch (`failure_strategy`)
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct BatchMetadata {
    /// Number of items between progress checkpoints
    ///
    /// Workers should update `workflow_steps.results.batch_cursor.last_checkpoint`
    /// after processing this many items. This enables:
    /// - Resumability after failures (TAS-49 ResetForRetry preserves cursors)
    /// - Staleness detection (TAS-59 Phase 3 checkpoint health monitoring)
    /// - Progress observability
    ///
    /// Example: With `checkpoint_interval = 100`, update cursor after items:
    /// 100, 200, 300, ..., batch_size
    pub checkpoint_interval: u32,

    /// Database field name used for cursor-based pagination
    ///
    /// Workers use this to construct queries like:
    /// ```sql
    /// SELECT * FROM table
    /// WHERE cursor_field > $start_cursor
    ///   AND cursor_field <= $end_cursor
    /// ORDER BY cursor_field
    /// LIMIT $batch_size
    /// ```
    ///
    /// Common values: `"id"`, `"created_at"`, `"sequence_number"`
    pub cursor_field: String,

    /// How this worker should handle failures during batch processing
    ///
    /// Determines worker behavior when processing individual items fails:
    /// - `ContinueOnFailure`: Log errors, continue processing remaining items
    /// - `FailFast`: Stop immediately on first error, transition to Error state
    /// - `Isolate`: Mark batch for manual investigation, continue other batches
    pub failure_strategy: FailureStrategy,
}

impl BatchWorkerInputs {
    /// Create new batch worker inputs from runtime cursor and template configuration
    ///
    /// This is the primary constructor used by `BatchProcessingService` when
    /// creating worker instances. It composes:
    /// - Runtime data: cursor positions from batchable handler analysis
    /// - Template data: configuration from the task template
    /// - Orchestration context: explicit no-op status from outcome handler
    ///
    /// # Arguments
    ///
    /// * `cursor_config` - Runtime cursor configuration with batch boundaries
    /// * `batch_config` - Template configuration with processing rules
    /// * `is_no_op` - Explicit flag set by outcome handler (true for NoBatches, false for WithBatches)
    ///
    /// # Example
    ///
    /// ```rust
    /// use tasker_shared::models::core::batch_worker::BatchWorkerInputs;
    /// use tasker_shared::messaging::CursorConfig;
    /// use tasker_shared::models::core::task_template::{BatchConfiguration, FailureStrategy};
    /// use serde_json::json;
    ///
    /// let cursor = CursorConfig {
    ///     batch_id: "batch_001".to_string(),
    ///     start_cursor: json!(0),
    ///     end_cursor: json!(1000),
    ///     batch_size: 1000,
    /// };
    ///
    /// let batch_config = BatchConfiguration {
    ///     batch_size: 1000,
    ///     parallelism: 5,
    ///     cursor_field: "id".to_string(),
    ///     checkpoint_interval: 100,
    ///     worker_template: "batch_worker".to_string(),
    ///     failure_strategy: FailureStrategy::ContinueOnFailure,
    /// };
    ///
    /// // Real worker with data to process
    /// let inputs = BatchWorkerInputs::new(cursor, &batch_config, false);
    ///
    /// assert_eq!(inputs.cursor.batch_id, "batch_001");
    /// assert_eq!(inputs.batch_metadata.checkpoint_interval, 100);
    /// assert_eq!(inputs.is_no_op, false);
    /// ```
    #[must_use]
    pub fn new(
        cursor_config: CursorConfig,
        batch_config: &BatchConfiguration,
        is_no_op: bool,
    ) -> Self {
        Self {
            cursor: cursor_config,
            batch_metadata: BatchMetadata {
                checkpoint_interval: batch_config.checkpoint_interval,
                cursor_field: batch_config.cursor_field.clone(),
                failure_strategy: batch_config.failure_strategy.clone(),
            },
            is_no_op,
        }
    }

    /// Convert to JSON value for merging into handler initialization
    ///
    /// This produces the JSONB structure that gets merged with the worker's
    /// handler initialization data before being stored in the database.
    ///
    /// # Example
    ///
    /// ```rust
    /// # use tasker_shared::models::core::batch_worker::BatchWorkerInputs;
    /// # use tasker_shared::messaging::CursorConfig;
    /// # use tasker_shared::models::core::task_template::{BatchConfiguration, FailureStrategy};
    /// # use serde_json::json;
    /// # let cursor = CursorConfig {
    /// #     batch_id: "batch_001".to_string(),
    /// #     start_cursor: json!(0),
    /// #     end_cursor: json!(1000),
    /// #     batch_size: 1000,
    /// # };
    /// # let batch_config = BatchConfiguration {
    /// #     batch_size: 1000,
    /// #     parallelism: 5,
    /// #     cursor_field: "id".to_string(),
    /// #     checkpoint_interval: 100,
    /// #     worker_template: "batch_worker".to_string(),
    /// #     failure_strategy: FailureStrategy::ContinueOnFailure,
    /// # };
    /// let inputs = BatchWorkerInputs::new(cursor, &batch_config, false);
    /// let json_value = inputs.to_value();
    ///
    /// // Can be merged with handler initialization
    /// let mut init = serde_json::json!({
    ///     "some_handler_param": "value"
    /// });
    /// if let Some(obj) = init.as_object_mut() {
    ///     for (key, value) in json_value.as_object().unwrap() {
    ///         obj.insert(key.clone(), value.clone());
    ///     }
    /// }
    /// ```
    #[must_use]
    pub fn to_value(&self) -> serde_json::Value {
        serde_json::to_value(self).expect("BatchWorkerInputs serialization should never fail")
    }
}

/// Checkpoint progress data for batch worker resumability (TAS-64)
///
/// This structure represents checkpoint progress stored in `StepExecutionResult.metadata.context`
/// or `workflow_steps.results.metadata.context`. It enables cursor-based resumption after failures,
/// allowing workers to continue from their last checkpoint rather than reprocessing from the beginning.
///
/// ## Storage Location
///
/// Checkpoint data is stored in the `context` HashMap of `StepExecutionMetadata`:
/// ```rust,no_run
/// use std::collections::HashMap;
/// use serde_json::json;
/// # use uuid::Uuid;
/// # use tasker_shared::messaging::StepExecutionResult;
/// # let step_uuid = Uuid::new_v4();
/// # let error_message = "Simulated failure".to_string();
/// # let error_code = Some("BATCH_FAILURE".to_string());
/// # let error_type = Some("RetryableError".to_string());
/// # let retryable = true;
/// # let execution_time_ms = 100;
/// let _result = StepExecutionResult::failure(
///     step_uuid,
///     error_message,
///     error_code,
///     error_type,
///     retryable,
///     execution_time_ms,
///     Some(HashMap::from([
///         ("checkpoint_progress".to_string(), json!(50)),
///         ("processed_before_failure".to_string(), json!(50)),
///         ("resumed_from".to_string(), json!(0)),
///     ])),
/// );
/// ```
///
/// ## Resumption Flow
///
/// 1. **First attempt**: Worker processes items 0-50, checkpoints at 25 and 50, fails at 50
/// 2. **Failure**: Creates `CheckpointProgress` with `checkpoint_progress: 50`
/// 3. **Retry**: Worker extracts checkpoint, resumes from position 50 instead of 0
/// 4. **Completion**: Worker completes remaining items without duplicate processing
///
/// ## Type-Safe Extraction
///
/// Instead of raw HashMap navigation:
/// ```rust,no_run
/// # use tasker_shared::types::TaskSequenceStep;
/// # async fn example(step_data: &TaskSequenceStep) -> anyhow::Result<()> {
/// // BEFORE (error-prone, no type safety)
/// let checkpoint = step_data.workflow_step.results.as_ref()
///     .and_then(|r| r.get("metadata"))
///     .and_then(|m| m.get("context"))
///     .and_then(|c| c.get("checkpoint_progress"))
///     .and_then(|v| v.as_u64())
///     .unwrap_or(0);
/// # Ok(())
/// # }
/// ```
///
/// Use type-safe extraction with proper error handling:
/// ```rust,no_run
/// # use tasker_shared::models::core::batch_worker::CheckpointProgress;
/// # use tasker_shared::types::TaskSequenceStep;
/// # async fn example(step_data: &TaskSequenceStep) -> anyhow::Result<()> {
/// // AFTER (type-safe, Result-based error handling)
/// let checkpoint = CheckpointProgress::from_workflow_step_with_name(&step_data.workflow_step)?
///     .map(|cp| cp.checkpoint_progress)
///     .unwrap_or(0);
/// # Ok(())
/// # }
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct CheckpointProgress {
    /// Current position in the dataset where checkpoint was saved
    ///
    /// Workers should resume processing from this position on retry.
    /// Typically represents the last successfully processed item's cursor value.
    pub checkpoint_progress: u64,

    /// Number of items successfully processed before failure occurred
    ///
    /// Used for observability and verification that resumption works correctly.
    /// Should equal `checkpoint_progress - resumed_from` for the attempt.
    pub processed_before_failure: u64,

    /// Position from which this attempt resumed processing
    ///
    /// - First attempt: 0 (starting from beginning)
    /// - Subsequent attempts: Previous checkpoint_progress value
    ///
    /// Proves no duplicate processing when `resumed_from > 0` on retry.
    pub resumed_from: u64,
}

impl CheckpointProgress {
    /// Create new checkpoint progress for storing in error results
    ///
    /// This is the constructor workers should use when creating error results
    /// with checkpoint data for resumability.
    ///
    /// # Example
    ///
    /// ```rust
    /// use tasker_shared::models::core::batch_worker::CheckpointProgress;
    /// use std::collections::HashMap;
    /// use serde_json::json;
    ///
    /// // Worker processed 50 items starting from position 0 before failure
    /// let checkpoint = CheckpointProgress::new(50, 50, 0);
    ///
    /// // Convert to HashMap for StepExecutionResult::failure context parameter
    /// let context = checkpoint.to_context_map();
    ///
    /// assert_eq!(context.get("checkpoint_progress").unwrap(), &json!(50));
    /// ```
    #[must_use]
    pub fn new(checkpoint_progress: u64, processed_before_failure: u64, resumed_from: u64) -> Self {
        Self {
            checkpoint_progress,
            processed_before_failure,
            resumed_from,
        }
    }

    /// Convert to HashMap for storing in StepExecutionResult.metadata.context
    ///
    /// This produces the HashMap format expected by `StepExecutionResult::failure()`
    /// context parameter.
    ///
    /// # Example
    ///
    /// ```rust
    /// use tasker_shared::models::core::batch_worker::CheckpointProgress;
    /// use tasker_shared::messaging::StepExecutionResult;
    /// use uuid::Uuid;
    ///
    /// let checkpoint = CheckpointProgress::new(50, 50, 0);
    /// let context = Some(checkpoint.to_context_map());
    ///
    /// let result = StepExecutionResult::failure(
    ///     Uuid::new_v4(),
    ///     "Simulated failure".to_string(),
    ///     Some("BATCH_FAILURE".to_string()),
    ///     Some("RetryableError".to_string()),
    ///     true,  // retryable
    ///     100,   // execution_time_ms
    ///     context,
    /// );
    /// ```
    #[must_use]
    pub fn to_context_map(&self) -> std::collections::HashMap<String, serde_json::Value> {
        use std::collections::HashMap;
        HashMap::from([
            (
                "checkpoint_progress".to_string(),
                serde_json::json!(self.checkpoint_progress),
            ),
            (
                "processed_before_failure".to_string(),
                serde_json::json!(self.processed_before_failure),
            ),
            (
                "resumed_from".to_string(),
                serde_json::json!(self.resumed_from),
            ),
        ])
    }

    /// Extract checkpoint progress from `StepExecutionResult.metadata.context`
    ///
    /// This is used when handlers need to access checkpoint data from a result
    /// object directly (e.g., during result processing or analysis).
    ///
    /// # Errors
    ///
    /// Returns `Err` if:
    /// - Checkpoint data exists but has invalid structure (wrong types, missing fields)
    /// - JSON deserialization fails
    ///
    /// Returns `Ok(None)` if:
    /// - No checkpoint data exists in context (expected for non-batch steps)
    /// - Context HashMap is empty
    ///
    /// # Example
    ///
    /// ```rust
    /// use tasker_shared::models::core::batch_worker::CheckpointProgress;
    /// use tasker_shared::messaging::StepExecutionResult;
    /// # use uuid::Uuid;
    /// # use std::collections::HashMap;
    /// # use serde_json::json;
    ///
    /// # let checkpoint = CheckpointProgress::new(50, 50, 0);
    /// # let result = StepExecutionResult::failure(
    /// #     Uuid::new_v4(), "".to_string(), None, None, true, 100,
    /// #     Some(checkpoint.to_context_map()),
    /// # );
    /// // Extract from result
    /// let checkpoint = CheckpointProgress::from_step_result(&result)?;
    /// assert!(checkpoint.is_some());
    /// assert_eq!(checkpoint.unwrap().checkpoint_progress, 50);
    /// # Ok::<(), anyhow::Error>(())
    /// ```
    pub fn from_step_result(
        result: &crate::messaging::StepExecutionResult,
    ) -> anyhow::Result<Option<Self>> {
        // Check if context has checkpoint_progress key
        if let Some(checkpoint_value) = result.metadata.context.get("checkpoint_progress") {
            // Build complete checkpoint structure from context
            let checkpoint_json = serde_json::json!({
                "checkpoint_progress": checkpoint_value,
                "processed_before_failure": result.metadata.context.get("processed_before_failure").unwrap_or(&serde_json::json!(0)),
                "resumed_from": result.metadata.context.get("resumed_from").unwrap_or(&serde_json::json!(0)),
            });

            let checkpoint: CheckpointProgress =
                serde_json::from_value(checkpoint_json).map_err(|e| {
                    anyhow::anyhow!(
                        "Failed to deserialize checkpoint progress from StepExecutionResult: {}",
                        e
                    )
                })?;

            Ok(Some(checkpoint))
        } else {
            // No checkpoint data present (not an error - just not a batch step or first attempt)
            Ok(None)
        }
    }

    /// Extract checkpoint progress from `workflow_steps.results.metadata.context`
    ///
    /// This is the primary method for batch workers to check for existing checkpoint
    /// data on retry attempts. Workers should call this early in execution to determine
    /// their resume position.
    ///
    /// ## Structure After TAS-64 Fix
    ///
    /// After fixing worker serialization to use `to_persistence_format()`, results have:
    /// ```json
    /// {
    ///   "success": false,
    ///   "result": {},
    ///   "metadata": {
    ///     "context": {
    ///       "checkpoint_progress": 50
    ///     }
    ///   }
    /// }
    /// ```
    ///
    /// Access path: `results.metadata.context.checkpoint_progress` (correct single nesting)
    ///
    /// ## Backwards Compatibility
    ///
    /// This method handles BOTH old double-nested structure (pre-TAS-64) and new
    /// correct structure (post-TAS-64) for graceful migration of existing workflows.
    ///
    /// # Errors
    ///
    /// Returns `Err` if:
    /// - Results field exists and has checkpoint data, but structure is invalid
    /// - JSON deserialization fails for checkpoint fields
    ///
    /// Returns `Ok(None)` if:
    /// - No results field exists (first attempt, no previous execution)
    /// - Results exist but no checkpoint data present
    /// - Results exist but don't have expected metadata.context structure
    ///
    /// # Example
    ///
    /// ```rust,no_run
    /// use tasker_shared::models::core::batch_worker::CheckpointProgress;
    /// use tasker_shared::models::WorkflowStep;
    ///
    /// # async fn example(step: &WorkflowStep) -> anyhow::Result<()> {
    /// // Check for checkpoint from previous attempt
    /// let checkpoint = CheckpointProgress::from_workflow_step(step)?;
    ///
    /// let resume_position = if let Some(cp) = checkpoint {
    ///     // Resume from last checkpoint
    ///     cp.checkpoint_progress
    /// } else {
    ///     // First attempt, start from beginning
    ///     0
    /// };
    ///
    /// // Process from resume_position...
    /// # Ok(())
    /// # }
    /// ```
    pub fn from_workflow_step(step: &crate::models::WorkflowStep) -> anyhow::Result<Option<Self>> {
        let results = match step.results.as_ref() {
            Some(r) => r,
            None => return Ok(None), // No results yet (first attempt)
        };

        let metadata = match results.get("metadata") {
            Some(m) => m,
            None => return Ok(None), // No metadata (shouldn't happen but handle gracefully)
        };

        let context = match metadata.get("context") {
            Some(c) => c,
            None => return Ok(None), // No context (first attempt or non-batch step)
        };

        // TAS-64: Try correct structure first (post-fix: metadata.context.checkpoint_progress)
        if let Some(checkpoint_value) = context.get("checkpoint_progress") {
            let checkpoint_json = serde_json::json!({
                "checkpoint_progress": checkpoint_value,
                "processed_before_failure": context.get("processed_before_failure").unwrap_or(&serde_json::json!(0)),
                "resumed_from": context.get("resumed_from").unwrap_or(&serde_json::json!(0)),
            });

            let checkpoint: CheckpointProgress = serde_json::from_value(checkpoint_json)
                .map_err(|e| {
                    anyhow::anyhow!(
                        "Failed to deserialize checkpoint progress from WorkflowStep (correct structure): {}",
                        e
                    )
                })?;

            return Ok(Some(checkpoint));
        }

        // BACKWARDS COMPATIBILITY: Try old double-nested structure (pre-fix: metadata.context.context.checkpoint_progress)
        // This handles existing workflows that were created before TAS-64 fix
        if let Some(nested_context) = context.get("context") {
            if let Some(checkpoint_value) = nested_context.get("checkpoint_progress") {
                let checkpoint_json = serde_json::json!({
                    "checkpoint_progress": checkpoint_value,
                    "processed_before_failure": nested_context.get("processed_before_failure").unwrap_or(&serde_json::json!(0)),
                    "resumed_from": nested_context.get("resumed_from").unwrap_or(&serde_json::json!(0)),
                });

                let checkpoint: CheckpointProgress = serde_json::from_value(checkpoint_json)
                    .map_err(|e| {
                        anyhow::anyhow!(
                            "Failed to deserialize checkpoint progress from WorkflowStep (legacy double-nested structure): {}",
                            e
                        )
                    })?;

                tracing::warn!(
                    step_uuid = %step.workflow_step_uuid,
                    "Found checkpoint in legacy double-nested structure - consider resetting workflow to use new format"
                );

                return Ok(Some(checkpoint));
            }
        }

        // No checkpoint data found
        Ok(None)
    }

    /// Extract checkpoint progress from WorkflowStepWithName (used in workers)
    ///
    /// This is a convenience method that delegates to `from_workflow_step()` since both
    /// `WorkflowStep` and `WorkflowStepWithName` have the same `results` field structure.
    ///
    /// # Arguments
    ///
    /// * `step` - The workflow step with name to extract checkpoint from
    ///
    /// # Returns
    ///
    /// * `Ok(Some(checkpoint))` if checkpoint data found and valid
    /// * `Ok(None)` if no checkpoint data (first attempt or non-batch step)
    /// * `Err` if checkpoint data exists but cannot be deserialized
    ///
    /// # Examples
    ///
    /// ```ignore
    /// use tasker_shared::models::core::batch_worker::CheckpointProgress;
    ///
    /// async fn process_step(step_data: &TaskSequenceStep) -> Result<()> {
    ///     let checkpoint = CheckpointProgress::from_workflow_step_with_name(&step_data.workflow_step)?;
    ///     let resume_from = checkpoint
    ///         .as_ref()
    ///         .map(|cp| cp.checkpoint_progress)
    ///         .unwrap_or(0);
    ///
    ///     // Process from resume_position...
    ///     Ok(())
    /// }
    /// ```
    pub fn from_workflow_step_with_name(
        step: &crate::models::workflow_step::WorkflowStepWithName,
    ) -> anyhow::Result<Option<Self>> {
        let results = match step.results.as_ref() {
            Some(r) => r,
            None => return Ok(None), // No results yet (first attempt)
        };

        let metadata = match results.get("metadata") {
            Some(m) => m,
            None => return Ok(None), // No metadata (shouldn't happen but handle gracefully)
        };

        let context = match metadata.get("context") {
            Some(c) => c,
            None => return Ok(None), // No context (first attempt or non-batch step)
        };

        // TAS-64: Try correct structure first (post-fix: metadata.context.checkpoint_progress)
        if let Some(checkpoint_value) = context.get("checkpoint_progress") {
            let checkpoint_json = serde_json::json!({
                "checkpoint_progress": checkpoint_value,
                "processed_before_failure": context.get("processed_before_failure").unwrap_or(&serde_json::json!(0)),
                "resumed_from": context.get("resumed_from").unwrap_or(&serde_json::json!(0)),
            });

            let checkpoint: CheckpointProgress = serde_json::from_value(checkpoint_json)
                .map_err(|e| {
                    anyhow::anyhow!(
                        "Failed to deserialize checkpoint progress from WorkflowStepWithName (correct structure): {}",
                        e
                    )
                })?;

            return Ok(Some(checkpoint));
        }

        // BACKWARDS COMPATIBILITY: Try old double-nested structure (pre-fix: metadata.context.context.checkpoint_progress)
        // This handles existing workflows that were created before TAS-64 fix
        if let Some(nested_context) = context.get("context") {
            if let Some(checkpoint_value) = nested_context.get("checkpoint_progress") {
                let checkpoint_json = serde_json::json!({
                    "checkpoint_progress": checkpoint_value,
                    "processed_before_failure": nested_context.get("processed_before_failure").unwrap_or(&serde_json::json!(0)),
                    "resumed_from": nested_context.get("resumed_from").unwrap_or(&serde_json::json!(0)),
                });

                let checkpoint: CheckpointProgress = serde_json::from_value(checkpoint_json)
                    .map_err(|e| {
                        anyhow::anyhow!(
                            "Failed to deserialize checkpoint progress from WorkflowStepWithName (legacy double-nested structure): {}",
                            e
                        )
                    })?;

                tracing::warn!(
                    step_uuid = %step.workflow_step_uuid,
                    "Found checkpoint in legacy double-nested structure - consider resetting workflow to use new format"
                );

                return Ok(Some(checkpoint));
            }
        }

        // No checkpoint data found
        Ok(None)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    fn sample_cursor_config() -> CursorConfig {
        CursorConfig {
            batch_id: "batch_001".to_string(),
            start_cursor: json!(0),
            end_cursor: json!(1000),
            batch_size: 1000,
        }
    }

    fn sample_batch_configuration() -> BatchConfiguration {
        BatchConfiguration {
            batch_size: 1000,
            parallelism: 5,
            cursor_field: "id".to_string(),
            checkpoint_interval: 100,
            worker_template: "batch_worker_template".to_string(),
            failure_strategy: FailureStrategy::ContinueOnFailure,
        }
    }

    #[test]
    fn test_batch_worker_inputs_creation() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();

        let inputs = BatchWorkerInputs::new(cursor.clone(), &batch_config, false);

        assert_eq!(inputs.cursor.batch_id, "batch_001");
        assert_eq!(inputs.cursor.batch_size, 1000);
        assert_eq!(inputs.batch_metadata.checkpoint_interval, 100);
        assert_eq!(inputs.batch_metadata.cursor_field, "id");
        assert_eq!(
            inputs.batch_metadata.failure_strategy,
            FailureStrategy::ContinueOnFailure
        );
        assert!(!inputs.is_no_op);
    }

    #[test]
    fn test_batch_worker_inputs_serialization() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();
        let inputs = BatchWorkerInputs::new(cursor, &batch_config, false);

        let json_value = inputs.to_value();

        // Verify structure
        assert!(json_value.get("cursor").is_some());
        assert!(json_value.get("batch_metadata").is_some());
        assert!(json_value.get("is_no_op").is_some());

        // Verify cursor data
        let cursor_obj = json_value.get("cursor").unwrap();
        assert_eq!(cursor_obj.get("batch_id").unwrap(), "batch_001");
        assert_eq!(cursor_obj.get("batch_size").unwrap(), 1000);

        // Verify metadata
        let metadata_obj = json_value.get("batch_metadata").unwrap();
        assert_eq!(metadata_obj.get("checkpoint_interval").unwrap(), 100);
        assert_eq!(metadata_obj.get("cursor_field").unwrap(), "id");

        // Verify is_no_op flag
        assert_eq!(json_value.get("is_no_op").unwrap(), false);
    }

    #[test]
    fn test_batch_worker_inputs_deserialization() {
        let json_str = r#"{
            "cursor": {
                "batch_id": "batch_002",
                "start_cursor": 1000,
                "end_cursor": 2000,
                "batch_size": 1000
            },
            "batch_metadata": {
                "checkpoint_interval": 50,
                "cursor_field": "created_at",
                "failure_strategy": "fail_fast"
            },
            "is_no_op": true
        }"#;

        let inputs: BatchWorkerInputs = serde_json::from_str(json_str).unwrap();

        assert_eq!(inputs.cursor.batch_id, "batch_002");
        assert_eq!(inputs.batch_metadata.checkpoint_interval, 50);
        assert_eq!(inputs.batch_metadata.cursor_field, "created_at");
        assert_eq!(
            inputs.batch_metadata.failure_strategy,
            FailureStrategy::FailFast
        );
        assert!(inputs.is_no_op);
    }

    #[test]
    fn test_batch_worker_inputs_round_trip() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();
        let original = BatchWorkerInputs::new(cursor, &batch_config, true);

        // Serialize to JSON
        let json_value = original.to_value();

        // Deserialize back
        let deserialized: BatchWorkerInputs = serde_json::from_value(json_value).unwrap();

        assert_eq!(original, deserialized);
    }

    #[test]
    fn test_different_failure_strategies() {
        let cursor = sample_cursor_config();

        let strategies = vec![
            FailureStrategy::ContinueOnFailure,
            FailureStrategy::FailFast,
            FailureStrategy::Isolate,
        ];

        for strategy in strategies {
            let mut batch_config = sample_batch_configuration();
            batch_config.failure_strategy = strategy.clone();

            let inputs = BatchWorkerInputs::new(cursor.clone(), &batch_config, false);
            assert_eq!(inputs.batch_metadata.failure_strategy, strategy);
        }
    }

    #[test]
    fn test_no_op_flag_serialization() {
        let cursor = sample_cursor_config();
        let batch_config = sample_batch_configuration();

        // Test with is_no_op = true
        let no_op_inputs = BatchWorkerInputs::new(cursor.clone(), &batch_config, true);
        let no_op_json = no_op_inputs.to_value();
        assert_eq!(no_op_json.get("is_no_op").unwrap(), true);

        // Test with is_no_op = false
        let real_inputs = BatchWorkerInputs::new(cursor, &batch_config, false);
        let real_json = real_inputs.to_value();
        assert_eq!(real_json.get("is_no_op").unwrap(), false);
    }

    // CheckpointProgress tests
    #[test]
    fn test_checkpoint_progress_creation() {
        let checkpoint = CheckpointProgress::new(50, 50, 0);

        assert_eq!(checkpoint.checkpoint_progress, 50);
        assert_eq!(checkpoint.processed_before_failure, 50);
        assert_eq!(checkpoint.resumed_from, 0);
    }

    #[test]
    fn test_checkpoint_progress_to_context_map() {
        let checkpoint = CheckpointProgress::new(50, 50, 0);
        let context = checkpoint.to_context_map();

        assert_eq!(context.get("checkpoint_progress").unwrap(), &json!(50));
        assert_eq!(context.get("processed_before_failure").unwrap(), &json!(50));
        assert_eq!(context.get("resumed_from").unwrap(), &json!(0));
    }

    #[test]
    fn test_checkpoint_progress_serialization() {
        let checkpoint = CheckpointProgress::new(100, 75, 25);

        let json_value = serde_json::to_value(&checkpoint).unwrap();

        assert_eq!(json_value.get("checkpoint_progress").unwrap(), 100);
        assert_eq!(json_value.get("processed_before_failure").unwrap(), 75);
        assert_eq!(json_value.get("resumed_from").unwrap(), 25);
    }

    #[test]
    fn test_checkpoint_progress_deserialization() {
        let json_str = r#"{
            "checkpoint_progress": 150,
            "processed_before_failure": 150,
            "resumed_from": 0
        }"#;

        let checkpoint: CheckpointProgress = serde_json::from_str(json_str).unwrap();

        assert_eq!(checkpoint.checkpoint_progress, 150);
        assert_eq!(checkpoint.processed_before_failure, 150);
        assert_eq!(checkpoint.resumed_from, 0);
    }

    #[test]
    fn test_checkpoint_progress_round_trip() {
        let original = CheckpointProgress::new(200, 100, 100);

        let json_value = serde_json::to_value(&original).unwrap();
        let deserialized: CheckpointProgress = serde_json::from_value(json_value).unwrap();

        assert_eq!(original, deserialized);
    }
}
