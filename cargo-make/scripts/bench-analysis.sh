#!/bin/bash
# TAS-159: Benchmark Analysis Report Generator
# Processes percentile_report.json and generates markdown analysis

set -euo pipefail

REPORT_FILE="${1:-target/criterion/percentile_report.json}"
OUTPUT_DIR="${2:-docs/ticket-specs/TAS-71}"
OUTPUT_FILE="${OUTPUT_DIR}/benchmark-results.md"

if [ ! -f "$REPORT_FILE" ]; then
    echo "Error: Report file not found: $REPORT_FILE"
    echo "Run benchmarks first: cargo make bench-e2e-all"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

# Generate the analysis document
cat > "$OUTPUT_FILE" << 'HEADER'
# E2E Benchmark Results Analysis

**Generated:** TIMESTAMP
**Branch:** BRANCH
**Commit:** COMMIT

## Executive Summary

This document presents the E2E benchmark results with percentile analysis (p50, p95).
All benchmarks use 50 samples unless otherwise noted.

---

HEADER

# Replace placeholders (using | delimiter to handle branch names with slashes)
sed -i '' "s|TIMESTAMP|$(date -u '+%Y-%m-%d %H:%M:%S UTC')|" "$OUTPUT_FILE"
sed -i '' "s|BRANCH|$(git branch --show-current)|" "$OUTPUT_FILE"
sed -i '' "s|COMMIT|$(git rev-parse --short HEAD)|" "$OUTPUT_FILE"

# Process JSON and generate tables using jq
cat >> "$OUTPUT_FILE" << 'EOF'
## Tier 1: Core Performance (Rust Native)

Baseline performance for simple workflow patterns using native Rust handlers.

| Scenario | Samples | Mean | P50 | P95 | P95/P50 |
|----------|---------|------|-----|-----|---------|
EOF

jq -r '.[] | select(.name | startswith("tier1_core")) |
  "\(.name | split("/") | .[-1]) | \(.samples) | \(.mean_ms | . * 100 | round / 100)ms | \(.p50_ms | . * 100 | round / 100)ms | \(if .p95_ms then (.p95_ms | . * 100 | round / 100) else "n/a" end)ms | \(if .p95_ms and .p50_ms then ((.p95_ms / .p50_ms) * 100 | round / 100) else "n/a" end)"' \
  "$REPORT_FILE" | while read line; do echo "| $line |" >> "$OUTPUT_FILE"; done

cat >> "$OUTPUT_FILE" << 'EOF'

**Observations:**
- Linear and diamond patterns show consistent ~182-185ms latency
- P95/P50 ratio near 1.0 indicates stable, predictable performance
- No significant tail latency concerns

---

## Tier 2: Complexity Scaling

Performance under increased workflow complexity (more steps, parallel paths, conditionals).

| Scenario | Samples | Mean | P50 | P95 | P95/P50 |
|----------|---------|------|-----|-----|---------|
EOF

jq -r '.[] | select(.name | startswith("tier2_complexity")) |
  "\(.name | split("/") | .[-1]) | \(.samples) | \(.mean_ms | . * 100 | round / 100)ms | \(.p50_ms | . * 100 | round / 100)ms | \(if .p95_ms then (.p95_ms | . * 100 | round / 100) else "n/a" end)ms | \(if .p95_ms and .p50_ms then ((.p95_ms / .p50_ms) * 100 | round / 100) else "n/a" end)"' \
  "$REPORT_FILE" | while read line; do echo "| $line |" >> "$OUTPUT_FILE"; done

cat >> "$OUTPUT_FILE" << 'EOF'

**Observations:**
- Complex DAG and hierarchical tree show ~300ms latency (7-8 steps)
- hierarchical_tree_rust shows higher P95/P50 ratio (~1.18) indicating some variance
- conditional_rust is faster (~149ms) due to dynamic path selection (fewer steps executed)

---

## Tier 3: Cluster Performance

Multi-instance deployment performance (requires cluster mode).

| Scenario | Samples | Mean | P50 | P95 | P95/P50 |
|----------|---------|------|-----|-----|---------|
EOF

jq -r '.[] | select(.name | startswith("tier3_cluster")) |
  "\(.name | split("/") | .[-1]) | \(.samples) | \(.mean_ms | . * 100 | round / 100)ms | \(.p50_ms | . * 100 | round / 100)ms | \(if .p95_ms then (.p95_ms | . * 100 | round / 100) else "n/a" end)ms | \(if .p95_ms and .p50_ms then ((.p95_ms / .p50_ms) * 100 | round / 100) else "n/a" end)"' \
  "$REPORT_FILE" | while read line; do echo "| $line |" >> "$OUTPUT_FILE"; done

cat >> "$OUTPUT_FILE" << 'EOF'

**Note:** Tier 3 data uses 10 samples from previous runs. Run `cargo make cluster-start-all`
then `cargo make bench-e2e-cluster` for updated 50-sample results.

---

## Tier 4: FFI Language Comparison

Comparison of FFI worker implementations across Ruby, Python, and TypeScript.

| Language | Pattern | Samples | Mean | P50 | P95 | P95/P50 |
|----------|---------|---------|------|-----|-----|---------|
EOF

jq -r '.[] | select(.name | startswith("tier4_languages")) |
  "\(.name | split("/") | .[1]) | \(.name | split("/") | .[-1]) | \(.samples) | \(.mean_ms | . * 100 | round / 100)ms | \(.p50_ms | . * 100 | round / 100)ms | \(if .p95_ms then (.p95_ms | . * 100 | round / 100) else "n/a" end)ms | \(if .p95_ms and .p50_ms then ((.p95_ms / .p50_ms) * 100 | round / 100) else "n/a" end)"' \
  "$REPORT_FILE" | while read line; do echo "| $line |" >> "$OUTPUT_FILE"; done

cat >> "$OUTPUT_FILE" << 'EOF'

### FFI Overhead Analysis

Comparing FFI workers to native Rust baseline (linear pattern):

EOF

# Calculate FFI overhead
RUST_LINEAR=$(jq -r '.[] | select(.name == "tier1_core/workflow/linear_rust") | .p50_ms' "$REPORT_FILE")

jq -r --arg rust "$RUST_LINEAR" '.[] | select(.name | startswith("tier4_languages")) | select(.name | endswith("/linear")) |
  "- **\(.name | split("/") | .[1] | ascii_upcase)**: \(.p50_ms | . * 100 | round / 100)ms (+\(((.p50_ms - ($rust | tonumber)) / ($rust | tonumber) * 100) | . * 10 | round / 10)% overhead)"' \
  "$REPORT_FILE" >> "$OUTPUT_FILE"

cat >> "$OUTPUT_FILE" << 'EOF'

**Observations:**
- All FFI workers add ~30-35% overhead vs native Rust
- Ruby shows lowest overhead for diamond pattern
- TypeScript shows slightly higher P95/P50 ratio indicating more variance
- Linear patterns consistently slower than diamond (more sequential steps)

---

## Tier 5: Batch Processing

Large-scale batch processing (1000 CSV rows, 5 parallel workers).

| Scenario | Samples | Mean | P50 | P95 | P95/P50 |
|----------|---------|------|-----|-----|---------|
EOF

jq -r '.[] | select(.name | startswith("tier5_batch")) |
  "\(.name | split("/") | .[-1]) | \(.samples) | \(.mean_ms | . * 100 | round / 100)ms | \(.p50_ms | . * 100 | round / 100)ms | \(if .p95_ms then (.p95_ms | . * 100 | round / 100) else "n/a" end)ms | \(if .p95_ms and .p50_ms then ((.p95_ms / .p50_ms) * 100 | round / 100) else "n/a" end)"' \
  "$REPORT_FILE" | while read line; do echo "| $line |" >> "$OUTPUT_FILE"; done

cat >> "$OUTPUT_FILE" << 'EOF'

**Observations:**
- 1000-row batch completes in ~288ms
- P95/P50 ratio of ~1.04 indicates stable batch processing
- Throughput: ~3,472 rows/second

---

## Performance Summary

### Latency by Tier

```
Tier 1 (Core):       ~182-185ms  (4 steps, native Rust)
Tier 2 (Complex):    ~149-306ms  (5-8 steps, varies by pattern)
Tier 4 (FFI):        ~185-242ms  (4 steps, +30-35% FFI overhead)
Tier 5 (Batch):      ~288ms      (1000 rows, 5 workers)
```

### Key Findings

1. **Stable Performance**: P95/P50 ratios mostly <1.1, indicating predictable latency
2. **FFI Overhead**: ~30-35% overhead for Ruby/Python/TypeScript vs native Rust
3. **Complexity Scaling**: Linear increase with step count (~40-45ms per step)
4. **Batch Efficiency**: High throughput with minimal variance

### Recommendations

1. **Use native Rust handlers** for latency-critical paths
2. **FFI workers are viable** for business logic (30-35% overhead acceptable)
3. **Monitor hierarchical patterns** - higher variance may indicate optimization opportunity
4. **Batch processing is efficient** - consider for bulk operations

---

## Methodology

- **Sample Size**: 50 per benchmark (except Tier 3 which uses legacy 10-sample data)
- **Measurement Time**: 120-180 seconds per tier
- **Environment**: Local development (not CI)
- **Percentiles**: P50 and P95 calculated from raw Criterion data

### Commands Used

```bash
cargo make services-start-release   # Start orchestration + rust worker
cargo make run-worker-ruby &        # Ruby worker (port 8082)
cargo make run-worker-python &      # Python worker (port 8083)
cargo make run-worker-typescript &  # TypeScript worker (port 8084)
cargo make bench-e2e-all            # Run all benchmarks
cargo make bench-report             # Generate percentile report
```
EOF

echo "âœ… Analysis report generated: $OUTPUT_FILE"
