# Evolving from ZeroMQ to Proactive Distributed Orchestration

## Executive Summary

This document outlines the architectural evolution from the successful ZeroMQ TCP implementation to a proactive distributed orchestration system with database-backed worker coordination and SQL-driven task discovery. Building on the proven ZeroMQ foundation, this evolution transforms Tasker from a reactive service into an intelligent, proactive workflow processing system.

**Previous Achievement**: ‚úÖ **ZeroMQ TCP Success** - Production-ready fire-and-forget batch execution with dual result pattern
**Current Challenge**: üîß **Namespace Mismatch** - Worker registration conflicts preventing intelligent task routing
**Evolution Goal**: üöÄ **Database-Backed Intelligence** - Explicit worker-task associations with intelligent selection
**Future Vision**: üéØ **Proactive Orchestration** - Rust cores autonomously discover and process ready tasks using SQL-driven coordination

## Current State Analysis

### ‚úÖ ZeroMQ TCP Architecture Success (Foundation Complete)

**ACHIEVEMENT (July 2025)**: Successfully implemented production-ready ZeroMQ TCP architecture providing the foundation for distributed orchestration:

1. **‚úÖ TCP Localhost Communication**: Resolved all socket timing and binding issues with `tcp://127.0.0.1:5555/5556`
2. **‚úÖ Fire-and-Forget Batch Execution**: High-throughput concurrent step processing with dual result pattern
3. **‚úÖ Cross-Language Integration**: Seamless Rust ‚Üî Ruby communication with structured JSON messaging
4. **‚úÖ Production Reliability**: Comprehensive error handling, graceful degradation, audit trails
5. **‚úÖ Configuration-Driven**: YAML-based environment detection with singleton pattern management
6. **‚úÖ Performance Excellence**: Sub-millisecond TCP communication supporting 10-1000+ concurrent steps
7. **‚úÖ Database Integration**: Complete HABTM batch tracking with audit trails and state machine coordination

### üîß Critical Discovery: Namespace Mismatch Problem

**IDENTIFIED ISSUE**: Fundamental architectural conflict preventing intelligent work distribution:

```
TaskHandler Registration:
- Namespace: "fulfillment" 
- Uses YAML configuration files
- Stored in in-memory TaskHandlerRegistry

Worker Registration:
- Namespace: "default" (generic)
- TCP-based worker capabilities
- Stored in in-memory WorkerManagementHandler

Result: "No workers available" errors due to namespace mismatch
```

**ROOT CAUSE**: In-memory registration systems designed for single-process deployment conflicting with distributed worker architecture requirements.

**IMPACT**: Prevents intelligent worker selection, forces fallback to basic routing, eliminates benefits of database-backed worker capabilities.

## Critical Workflows Analysis

### System Entry Points and Current State

Based on analysis of the system architecture, there are **4 primary entry points** that define the system's capabilities:

#### 1. ‚úÖ `initialize_task(task_request)` - Task Creation (Evolving to Commands)
**Current Flow**: `Ruby Application ‚Üí FFI ‚Üí Rust TaskInitializer ‚Üí Database`
- **Status**: ‚úÖ **WORKING** - Handle-based FFI with hash return values
- **Performance**: 1-6ms database operations, zero connection pool timeouts
- **Architecture**: SharedOrchestrationHandle with persistent Arc references
- **Evolution**: **MIGRATE TO COMMANDS** - Becomes `InitializeTask` command over transport

#### 2. ‚úÖ `handle(task_id)` - Workflow Execution (Evolving to Commands)
**Current Flow**: `Ruby ‚Üí handle(task_id) ‚Üí WorkflowCoordinator ‚Üí ZeroMQ ‚Üí Ruby workers`
- **Status**: ‚úÖ **WORKING** - Fire-and-forget workflow execution operational
- **Performance**: Sub-millisecond TCP communication, 10-1000+ concurrent steps
- **Architecture**: ZeroMQ TCP pub-sub with dual result pattern
- **Evolution**: **MIGRATE TO COMMANDS** - Becomes `ExecuteWorkflow` command over transport

#### 3. ‚úÖ Performance/Analytics Queries (Evolving to Commands)
**Current Flow**: `Ruby ‚Üí FFI ‚Üí Rust analytics ‚Üí Database queries ‚Üí Results`
- **Status**: ‚úÖ **WORKING** - Real-time performance monitoring without additional connections
- **Features**: Sophisticated SQL aggregations, clean Ruby hash interface
- **Evolution**: **MIGRATE TO COMMANDS** - Becomes `QueryAnalytics` command over transport

#### 4. üîß ZeroMQ Batch Execution (Needs Evolution)
**Current Flow**: `WorkflowCoordinator ‚Üí ZeroMQ pub ‚Üí Ruby workers ‚Üí ZeroMQ results`
- **Status**: üîß **NAMESPACE MISMATCH** - Worker selection failing due to registration conflicts
- **Problem**: TaskHandler registry ("fulfillment") vs Worker registry ("default")
- **Recommendation**: **DATABASE-BACKED EVOLUTION** - Replace in-memory with explicit associations

### Evolution Strategy: Pure Command Architecture

**PRESERVE** (Proven Foundations):
- ZeroMQ TCP communication patterns and performance
- State machine orchestration logic and database operations
- Existing workflow coordination and dependency resolution
- Database performance optimizations and HABTM audit trails

**MIGRATE TO COMMANDS** (All Entry Points):
- Entry Point 1: `initialize_task` ‚Üí `InitializeTask` command
- Entry Point 2: `handle(task_id)` ‚Üí `ExecuteWorkflow` command  
- Entry Point 3: Analytics queries ‚Üí `QueryAnalytics` command
- Entry Point 4: Worker batch execution ‚Üí `ExecuteBatch` command

**EVOLVE** (Add Intelligence):
- Replace namespace guessing with database-backed worker selection
- Add proactive discovery alongside reactive execution
- Add distributed coordination for multiple Rust cores
- Enhance with Unix sockets for local, TCP for distributed communication

## Database-Backed Worker Registration Solution

### üéØ Solution: Explicit Worker-Task Associations

**REPLACE** in-memory assumptions with **database-backed explicit associations**:

```sql
-- Workers declare exactly which tasks they support
CREATE TABLE tasker_workers (
    worker_id SERIAL PRIMARY KEY,
    worker_name VARCHAR(255) UNIQUE NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Many-to-many: workers ‚Üî tasks with configuration
CREATE TABLE tasker_worker_named_tasks (
    id SERIAL PRIMARY KEY,
    worker_id INTEGER REFERENCES tasker_workers(worker_id),
    named_task_id INTEGER REFERENCES tasker_named_tasks(named_task_id),
    configuration JSONB NOT NULL DEFAULT '{}',
    priority INTEGER DEFAULT 100,
    UNIQUE(worker_id, named_task_id)
);

-- Connection lifecycle and health tracking
CREATE TABLE tasker_worker_registrations (
    id SERIAL PRIMARY KEY,
    worker_id INTEGER REFERENCES tasker_workers(worker_id),
    status VARCHAR(50) CHECK (status IN ('registered', 'healthy', 'unhealthy', 'disconnected')),
    connection_type VARCHAR(50) NOT NULL,
    connection_details JSONB NOT NULL DEFAULT '{}',
    last_heartbeat_at TIMESTAMP,
    registered_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

### üöÄ Intelligent Worker Selection

**Database-driven routing** replaces namespace guessing:

```rust
// BEFORE: Namespace guessing (fails)
let workers = worker_pool.find_workers_for_namespace("default"); // ‚ùå Mismatch

// AFTER: Explicit database lookup (succeeds)
let workers = worker_selection_service.select_workers_for_task(
    "fulfillment",     // namespace
    "process_order",   // name  
    "1.0.0"            // version
).await?;
```

**SQL Query for Intelligent Selection**:
```sql
SELECT 
    w.worker_name,
    w.metadata,
    wr.connection_details,
    wnt.configuration as task_configuration,
    wnt.priority
FROM tasker_workers w
INNER JOIN tasker_worker_registrations wr ON w.worker_id = wr.worker_id
INNER JOIN tasker_worker_named_tasks wnt ON w.worker_id = wnt.worker_id
INNER JOIN tasker_named_tasks nt ON wnt.named_task_id = nt.named_task_id
WHERE nt.namespace = $1 AND nt.name = $2 AND nt.version = $3
  AND wr.status = 'healthy'
  AND wr.last_heartbeat_at > NOW() - INTERVAL '2 minutes'
ORDER BY wnt.priority DESC, wr.last_heartbeat_at DESC
LIMIT 1;
```

### ‚úÖ Benefits Achieved

1. **Zero "No Workers Available" Errors**: Explicit associations eliminate namespace guessing
2. **Persistent Worker State**: Registrations survive restarts and failures
3. **Intelligent Load Balancing**: Priority-based selection with health tracking
4. **Configuration Per Association**: Different workers can have different configs for same task
5. **Complete Observability**: Full audit trail of worker capabilities and performance

## Proactive Orchestration Vision

### üéÜ Revolutionary Architecture: From Reactive to Proactive

**CURRENT (Reactive)**: Frameworks initiate workflow processing
```
Rails Application ‚Üí handle(task_id) ‚Üí Rust discovers viable steps ‚Üí processes work
```

**FUTURE (Proactive)**: Rust cores autonomously discover and process ready work
```
Rust WorkflowCoordinator Loop:
1. Query SQL functions for ready tasks (using existing TaskExecutionContext patterns)
2. Acquire distributed coordination lock for batch processing
3. FOR EACH ready task:
   a. Discover viable steps using existing ViableStepDiscovery
   b. Select workers using database-backed WorkerSelectionService
   c. Send commands to workers via TCP/Unix sockets
   d. Process results and update task state
4. Release coordination lock, wait interval, repeat
```

### üìä SQL-Driven Ready Task Discovery

**Leverage Existing SQL Intelligence**: Build on proven `get_task_execution_context` patterns

```rust
impl WorkflowCoordinator {
    pub async fn proactive_orchestration_loop(&self) -> Result<(), OrchestrationError> {
        loop {
            // 1. Acquire coordination lock for ready task discovery
            let coordination_token = self.acquire_ready_task_coordination().await?;
            
            // 2. Use existing SQL functions to discover ready tasks
            let ready_tasks = self.discover_ready_tasks_for_processing(10).await?;
            
            // 3. Process each ready task using existing orchestration logic
            for task_id in ready_tasks {
                self.process_ready_task_proactively(task_id).await?;
            }
            
            // 4. Release coordination lock
            self.release_ready_task_coordination(coordination_token).await?;
            
            // 5. Wait before next discovery cycle (configurable)
            tokio::time::sleep(Duration::from_secs(5)).await;
        }
    }

    async fn discover_ready_tasks_for_processing(&self, limit: i32) -> Result<Vec<i64>, OrchestrationError> {
        // Leverage existing SQL functions for task readiness
        let query = r#"
            SELECT task_id 
            FROM get_ready_tasks_for_processing($1)
            WHERE processing_coordination_id IS NULL
            ORDER BY created_at ASC
        "#;
        
        let task_ids = sqlx::query_scalar::<_, i64>(query)
            .bind(limit)
            .fetch_all(&self.db_pool)
            .await?;
            
        Ok(task_ids)
    }

    async fn process_ready_task_proactively(&self, task_id: i64) -> Result<(), OrchestrationError> {
        // Use existing WorkflowCoordinator logic
        let viable_steps = self.discover_viable_steps(task_id).await?;
        
        if !viable_steps.is_empty() {
            // Use database-backed worker selection (new)
            let selected_workers = self.worker_selection_service
                .select_workers_for_steps(&viable_steps).await?;
            
            // Send work to selected workers (enhanced)
            self.distribute_work_to_workers(task_id, viable_steps, selected_workers).await?;
        }
        
        Ok(())
    }
}
```

## Distributed Coordination Design

### üîó Database-Backed Distributed Mutex

**Challenge**: Multiple Rust cores must coordinate ready task discovery without race conditions

**Solution**: Transient coordination table for distributed locking (separate from tasker_tasks)

```sql
-- Lightweight coordination table for ready task discovery
CREATE TABLE tasker_ready_task_coordination (
    id SERIAL PRIMARY KEY,
    coordination_token VARCHAR(255) UNIQUE NOT NULL,
    coordinating_core_id VARCHAR(255) NOT NULL,
    coordination_type VARCHAR(50) NOT NULL DEFAULT 'ready_task_discovery',
    claimed_task_ids BIGINT[] DEFAULT '{}',
    expires_at TIMESTAMP NOT NULL,
    acquired_at TIMESTAMP NOT NULL DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'
);

-- Index for performance and cleanup
CREATE INDEX idx_ready_task_coordination_active 
    ON tasker_ready_task_coordination(coordination_type, expires_at)
    WHERE expires_at > NOW();
```

### ‚ö° Transient Coordination Pattern

**Key Insight**: Mutex is **very transient** - only held long enough to identify ready tasks and create batches

```rust
impl DistributedCoordination {
    pub async fn acquire_ready_task_coordination(&self) -> Result<CoordinationToken, CoordinationError> {
        let token = format!("coord_{}_{}", self.core_id, Uuid::new_v4());
        let expires_at = Utc::now() + Duration::minutes(2); // Very short-lived
        
        // Atomic coordination acquisition
        let query = r#"
            INSERT INTO tasker_ready_task_coordination 
                (coordination_token, coordinating_core_id, expires_at)
            VALUES ($1, $2, $3)
            ON CONFLICT (coordination_type) DO UPDATE
            SET coordination_token = $1, coordinating_core_id = $2, expires_at = $3
            WHERE tasker_ready_task_coordination.expires_at < NOW()
            RETURNING coordination_token
        "#;
        
        let result = sqlx::query_scalar::<_, String>(query)
            .bind(&token)
            .bind(&self.core_id) 
            .bind(expires_at)
            .fetch_optional(&self.db_pool)
            .await?;
            
        match result {
            Some(_) => Ok(CoordinationToken { token, expires_at }),
            None => Err(CoordinationError::AnotherCoreProcessing)
        }
    }

    pub async fn release_ready_task_coordination(&self, token: CoordinationToken) -> Result<(), CoordinationError> {
        // Clean up coordination immediately after processing
        let query = "DELETE FROM tasker_ready_task_coordination WHERE coordination_token = $1";
        sqlx::query(query)
            .bind(&token.token)
            .execute(&self.db_pool)
            .await?;
            
        Ok(())
    }
}
```

### üîÑ Integration with Existing State Machines

**Preserve Existing State Logic**: Coordination layer doesn't interfere with task/step state management

```rust
// Coordination is separate from task state
struct ReadyTaskProcessor {
    coordination: DistributedCoordination,  // üÜï NEW: Distributed coordination
    state_manager: StateManager,            // ‚úÖ EXISTING: Task/step state management
    workflow_coordinator: WorkflowCoordinator, // ‚úÖ EXISTING: Orchestration logic
}

impl ReadyTaskProcessor {
    async fn process_ready_tasks(&self) -> Result<(), ProcessingError> {
        // 1. Acquire coordination (transient)
        let token = self.coordination.acquire_ready_task_coordination().await?;
        
        // 2. Discover ready tasks (existing SQL functions)
        let ready_task_ids = self.discover_ready_tasks().await?;
        
        // 3. Process using existing orchestration (unchanged)
        for task_id in ready_task_ids {
            self.workflow_coordinator.execute_task_workflow(task_id).await?;
        }
        
        // 4. Release coordination immediately (transient)
        self.coordination.release_ready_task_coordination(token).await?;
        
        Ok(())
    }
}
```

### üéØ Benefits of Distributed Architecture

1. **Race Condition Prevention**: Multiple Rust cores coordinate without conflicts
2. **Transient Coordination**: Very short-lived locks (minutes, not hours)
3. **State Machine Preservation**: Existing task/step state logic unchanged
4. **Horizontal Scaling**: Unlimited distributed Rust cores supported
5. **Failure Recovery**: Expired coordination locks automatically cleaned up
6. **Fair Distribution**: PostgreSQL coordination ensures fair work distribution

## Command Pattern Evolution

### üéÜ From ZeroMQ Topics to Structured Commands

**EVOLUTION STRATEGY**: Build on ZeroMQ success while adding structured communication

**Current ZeroMQ Flow** (‚úÖ Working Foundation):
```
"steps {json_batch_request}"           # Rust ‚Üí Ruby (fire-and-forget)
"partial_result {json_result}"         # Ruby ‚Üí Rust (real-time updates)
"batch_completion {json_completion}"   # Ruby ‚Üí Rust (reconciliation)
```

**Enhanced Command Flow** (üéØ Target):
```
InitializeTask { task_request }         # Framework ‚Üí Rust (or Rust internal)
ExecuteBatch { batch_data }             # Rust ‚Üí Worker (replaces "steps")
ReportPartialResult { step_result }     # Worker ‚Üí Rust (replaces "partial_result")
ReportBatchCompletion { batch_summary } # Worker ‚Üí Rust (replaces "batch_completion")
RegisterWorker { worker_capabilities }  # Worker ‚Üí Rust (database-backed)
DiscoverReadyTasks { coordination }     # Rust ‚Üî Rust (distributed coordination)
```

#### Phase 1: Database-Backed Worker Registration (üõ†Ô∏è In Progress)

**Problem Solved**: Namespace mismatch between TaskHandler registration (`fulfillment`) and Worker registration (`default`)

**Database Schema** (implemented in migration `20250728000001_create_workers_tables.sql`):
```sql
-- Explicit worker identity and metadata
CREATE TABLE tasker_workers (
    worker_id SERIAL PRIMARY KEY,
    worker_name VARCHAR(255) UNIQUE NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Many-to-many worker ‚Üî task associations with configuration
CREATE TABLE tasker_worker_named_tasks (
    id SERIAL PRIMARY KEY,
    worker_id INTEGER REFERENCES tasker_workers(worker_id),
    named_task_id INTEGER REFERENCES tasker_named_tasks(named_task_id),
    configuration JSONB NOT NULL DEFAULT '{}',
    priority INTEGER DEFAULT 100,
    UNIQUE(worker_id, named_task_id)
);

-- Connection lifecycle and health tracking
CREATE TABLE tasker_worker_registrations (
    id SERIAL PRIMARY KEY,
    worker_id INTEGER REFERENCES tasker_workers(worker_id),
    status VARCHAR(50) CHECK (status IN ('registered', 'healthy', 'unhealthy', 'disconnected')),
    connection_type VARCHAR(50) NOT NULL,
    connection_details JSONB NOT NULL DEFAULT '{}',
    last_heartbeat_at TIMESTAMP,
    registered_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Command audit trail for observability
CREATE TABLE tasker_worker_command_audit (
    id BIGSERIAL PRIMARY KEY,
    worker_id INTEGER REFERENCES tasker_workers(worker_id),
    command_type VARCHAR(100) NOT NULL,
    command_direction VARCHAR(20) CHECK (command_direction IN ('sent', 'received')),
    correlation_id VARCHAR(255),
    batch_id BIGINT REFERENCES tasker_step_execution_batches(batch_id),
    execution_time_ms BIGINT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

## Ruby Codebase Evolution Strategy

### üìù Philosophy: Intelligent Simplification

**Core Insight**: Much of the Ruby code will become obsolete as we move to Rust-based command infrastructure - **this is good** as it dramatically simplifies the architecture.

**PRESERVE** (Essential Test Coverage):
- **Execution specs**: Core workflow execution test patterns
- **Handler integration tests**: Step handler testing frameworks  
- **Order fulfillment integration**: Real-world workflow validation
- **Performance test harnesses**: Benchmarking and validation tools

**EVOLVE TO THIN WRAPPERS** (Ruby becomes interface layer):
- **TaskerCore module**: Becomes thin FFI wrapper over Rust command infrastructure
- **Worker registration**: Simple Ruby clients for Rust-based WorkerManager
- **Command communication**: Lightweight TCP/Unix socket clients
- **Configuration management**: YAML processing delegates to Rust configuration system

### üöÄ Rust-Based Command Infrastructure Vision

**NEW ARCHITECTURE**: Rust becomes the primary infrastructure, Ruby becomes a client

```ruby
# BEFORE: Complex Ruby orchestration infrastructure
class BatchStepExecutionOrchestrator
  # 500+ lines of complex worker pool management
  # ZeroMQ socket lifecycle
  # Concurrent-ruby coordination
  # Type validation and error handling
end

# AFTER: Thin Ruby client for Rust infrastructure
class WorkerClient
  def initialize(rust_command_server_endpoint)
    @client = TaskerCore::CommandClient.new(endpoint)
  end
  
  def register_capabilities(supported_tasks)
    @client.send_command(RegisterWorker.new(supported_tasks))
  end
  
  def start_processing
    @client.listen_for_commands do |command|
      case command
      when ExecuteBatch
        process_batch(command.batch_data)
      end
    end
  end
end
```

### üîß Rust-Based WorkerManager and CommandClient

**Rust Infrastructure** (Primary Implementation):
```rust
// src/execution/worker_manager.rs
pub struct WorkerManager {
    command_server: TcpCommandServer,
    worker_registry: DatabaseWorkerRegistry,
    coordination: DistributedCoordination,
    health_monitor: WorkerHealthMonitor,
}

// src/execution/command_client.rs  
pub struct CommandClient {
    connection: TcpConnection,
    message_router: CommandRouter,
    health_reporter: HealthReporter,
}
```

**Ruby FFI Wrappers** (Thin Interface):
```ruby
# bindings/ruby/lib/tasker_core/worker_manager.rb
class WorkerManager
  def initialize
    @rust_handle = TaskerCore::FFI.create_worker_manager
  end
  
  def register_worker(capabilities)
    TaskerCore::FFI.register_worker(@rust_handle, capabilities.to_json)
  end
end

# bindings/ruby/lib/tasker_core/command_client.rb
class CommandClient
  def initialize(endpoint)
    @rust_handle = TaskerCore::FFI.create_command_client(endpoint)
  end
  
  def send_command(command)
    TaskerCore::FFI.send_command(@rust_handle, command.to_json)
  end
end
```

### üìä Benefits of Ruby Simplification

1. **Dramatic Code Reduction**: 80%+ reduction in Ruby orchestration complexity
2. **Enhanced Reliability**: Rust infrastructure handles complex coordination
3. **Improved Performance**: Native Rust performance for critical operations
4. **Easier Maintenance**: Single source of truth for orchestration logic
5. **Language Flexibility**: Same Rust infrastructure supports Python, Node.js, etc.

## Implementation Roadmap

#### Phase 2: Command Pattern Integration (üûØ Next Priority)

**Rust Side: Command-Based TCP Architecture**

```rust
// Evolution: From ZeroMQ topics to structured commands
pub struct CommandBasedTcpExecutor {
    server: TcpListener,
    command_router: CommandRouter,
    worker_pool: DatabaseWorkerPool,  // üÜï Database-backed instead of in-memory
    connection_manager: ConnectionManager,
    worker_selection_service: WorkerSelectionService,  // üÜï Intelligent routing
}

pub struct CommandRouter {
    // Route commands to appropriate handlers
    handlers: HashMap<CommandType, Box<dyn CommandHandler>>,

    // Command lifecycle management with database audit
    pending_commands: HashMap<String, PendingCommand>,
    command_audit: WorkerCommandAuditService,  // üÜï Database-backed audit trail
}

// üÜï Database-backed worker pool replacing in-memory state
pub struct DatabaseWorkerPool {
    db_pool: PgPool,
    worker_service: WorkerService,
    registration_service: WorkerRegistrationService,
    selection_service: WorkerSelectionService,
}

// üÜï Database-backed worker state with persistent health tracking
pub struct DatabaseWorkerState {
    worker_id: i32,
    worker_name: String,
    connection_details: serde_json::Value,
    supported_tasks: Vec<WorkerNamedTask>,  // üÜï Explicit task associations
    current_registrations: Vec<WorkerRegistration>,
    last_command_audit: Option<WorkerCommandAudit>,
}
```

**Key Command Handlers:**
- **TaskInitializationHandler**: Handles `InitializeTask` commands for task creation
- **BatchExecutionHandler**: Manages `ExecuteBatch` commands with database-backed worker selection
- **ResultAggregationHandler**: Processes `ReportPartialResult` and `ReportBatchCompletion` commands
- **WorkerManagementHandler**: üÜï Database-backed `RegisterWorker`, `WorkerHeartbeat` commands
- **ReadyTaskDiscoveryHandler**: üÜï **NEW** - Proactive task discovery using SQL functions
- **DistributedMutexHandler**: üÜï **NEW** - Coordinates multiple Rust cores for ready task processing
- **HealthCheckHandler**: System monitoring with worker capability reporting

#### Phase 3: Proactive Orchestration Vision (üîÆ Future State)

**Revolutionary Architecture Change**: From reactive (framework-initiated) to proactive (Rust-initiated) workflow processing

**Current Reactive Flow**:
```
Rails Application ‚Üí handle(task_id) ‚Üí Rust WorkflowCoordinator ‚Üí discovers viable steps ‚Üí publishes to workers
```

**Future Proactive Flow**:
```
Rust WorkflowCoordinator Loop:
1. Acquire distributed mutex for ready task discovery
2. Query SQL functions (TaskExecutionContext) for ready tasks
3. FOR EACH ready task:
   a. Send InitializeTask command (if needed)
   b. Send ExecuteBatch command to selected workers via database-backed selection
   c. Receive results via command pattern
   d. Update task state and continue workflow
4. Release mutex, wait interval, repeat
```

**Database-Backed Distributed Mutex Design**:
```sql
-- Atomic task claiming with PostgreSQL FOR UPDATE SKIP LOCKED
UPDATE tasker_tasks
SET processing_core_id = $core_id, claimed_at = NOW()
WHERE task_id IN (
  SELECT task_id FROM get_ready_tasks_for_processing(10) -- batch size
  FOR UPDATE SKIP LOCKED  -- Avoid blocking between cores
)
AND processing_core_id IS NULL
RETURNING task_id;

-- Distributed lock with expiry for ready task discovery coordination
INSERT INTO tasker_distributed_processing_locks (lock_name, owner_core_id, expires_at)
VALUES ('ready_task_discovery', $core_id, NOW() + INTERVAL '30 seconds')
ON CONFLICT (lock_name) DO UPDATE
SET owner_core_id = $core_id, expires_at = NOW() + INTERVAL '30 seconds'
WHERE tasker_distributed_processing_locks.expires_at < NOW()
RETURNING owner_core_id;
```

#### Ruby Side: Command-Based Worker Architecture

```ruby
# Evolution: From ZeroMQ BatchStepExecutionOrchestrator to Command-based worker
class CommandBasedWorker
  # Maintains persistent TCP connection to Rust server
  # Database-backed worker registration with explicit task support
  # Command-based communication replacing ZeroMQ pub-sub

  def initialize(worker_name:, supported_tasks:)
    @worker_name = worker_name  # üÜï Persistent identity for database
    @supported_tasks = supported_tasks  # üÜï Explicit task declarations
    @connection_type = 'tcp'  # or 'unix' for local workers
    @metadata = build_worker_metadata
  end

  def start
    connect_to_rust_command_server
    register_worker_with_database_backend
    start_command_listener
    start_heartbeat_loop
  end

  private

  # üÜï Database-backed registration with explicit task support
  def register_worker_with_database_backend
    command = {
      command_type: "RegisterWorker",
      command_id: SecureRandom.uuid,
      payload: {
        worker_name: @worker_name,
        metadata: @metadata,
        supported_tasks: @supported_tasks.map do |task|
          {
            namespace: task[:namespace],
            name: task[:name],
            version: task[:version],
            configuration: load_task_configuration(task)
          }
        end,
        connection_info: {
          connection_type: @connection_type,
          listener_port: discover_listener_port,
          host: discover_host_ip,
          protocol_version: '2.0'
        }
      }
    }
    send_command(command)
  end

  # üÜï Discover supported tasks from filesystem (replaces namespace guessing)
  def discover_supported_tasks_from_config
    Dir.glob("config/tasker/**/*.yaml").map do |config_path|
      config = YAML.load_file(config_path)
      {
        namespace: config['namespace_name'],
        name: config['name'],
        version: config['version'] || '1.0.0'
      }
    end
  end
end
```

### Unified Command Protocol Evolution

**Evolution from ZeroMQ Topics to Structured Commands**:

**BEFORE (ZeroMQ - ‚úÖ Working)**:
```
"steps {json_batch_request}"           # Rust ‚Üí Ruby
"partial_result {json_result}"         # Ruby ‚Üí Rust
"batch_completion {json_completion}"   # Ruby ‚Üí Rust
```

**AFTER (Commands - üûØ Target)**:
```
InitializeTask { task_request }         # Framework ‚Üí Rust (or Rust internal)
ExecuteBatch { batch_data }             # Rust ‚Üí Worker (replaces "steps")
ReportPartialResult { step_result }     # Worker ‚Üí Rust (replaces "partial_result")
ReportBatchCompletion { batch_summary } # Worker ‚Üí Rust (replaces "batch_completion")
RegisterWorker { worker_capabilities }  # Worker ‚Üí Rust (database-backed)
QueryReadyTasks { mutex_coordination }  # Rust ‚Üî Rust (distributed coordination)
```

**Enhanced Command Structure** with database integration and audit trails:

```rust
#[derive(Serialize, Deserialize)]
pub struct Command {
    // Command identification with audit trail support
    pub command_type: CommandType,
    pub command_id: String,
    pub correlation_id: Option<String>,

    // Routing and execution context
    pub metadata: CommandMetadata,

    // Database integration for worker selection and audit
    pub worker_routing: Option<WorkerRouting>,  // üÜï Database-backed worker selection
    pub audit_context: Option<AuditContext>,   // üÜï Command audit trail context

    // The actual payload
    pub payload: CommandPayload,
}

#[derive(Serialize, Deserialize)]
pub struct WorkerRouting {
    pub target_worker_id: Option<i32>,
    pub namespace_requirements: Vec<String>,
    pub selection_strategy: WorkerSelectionStrategy,  // round_robin, least_loaded, priority
    pub connection_preference: ConnectionType,        // tcp, unix
}

#[derive(Serialize, Deserialize)]
pub struct AuditContext {
    pub batch_id: Option<String>,
    pub task_id: Option<i64>,
    pub initiated_by: String,
    pub processing_core_id: Option<String>,
}

#[derive(Serialize, Deserialize)]
pub struct CommandMetadata {
    pub timestamp: DateTime<Utc>,
    pub source: CommandSource,
    pub target: Option<CommandTarget>,
    pub timeout_ms: Option<u64>,
    pub retry_policy: Option<RetryPolicy>,
    pub namespace: Option<String>,
    pub priority: Option<CommandPriority>,
}

#[derive(Serialize, Deserialize)]
#[serde(tag = "type", content = "data")]
pub enum CommandPayload {
    // Task management operations (Framework ‚Üí Rust or Rust internal)
    InitializeTask {
        task_request: TaskRequest,      // üÜï Structured request from framework
        workflow_definition: Option<WorkflowDefinition>,
    },

    // Step execution operations (Rust ‚Üí Worker)
    ExecuteBatch {
        batch_id: String,
        task_template: TaskTemplate,    // üÜï Includes namespace/name/version for worker selection
        steps: Vec<StepExecutionRequest>,
        worker_context: WorkerExecutionContext,
    },

    // Result reporting (Worker ‚Üí Rust)
    ReportPartialResult {
        batch_id: String,
        step_id: i64,
        result: StepResult,
        execution_time_ms: u64,
        worker_metadata: WorkerMetadata,
    },

    ReportBatchCompletion {
        batch_id: String,
        step_summaries: Vec<StepSummary>,
        total_execution_time_ms: u64,
        worker_performance_stats: WorkerPerformanceStats,
    },

    // Database-backed worker lifecycle (Worker ‚Üî Rust)
    RegisterWorker {
        worker_name: String,            // üÜï Persistent identity
        supported_tasks: Vec<SupportedTask>,  // üÜï Explicit task declarations
        connection_info: ConnectionInfo,
        metadata: WorkerMetadata,
    },

    WorkerHeartbeat {
        worker_name: String,
        current_load: usize,
        system_stats: Option<SystemStats>,
        last_command_processed: Option<String>,
    },

    // üÜï NEW: Proactive orchestration coordination (Rust ‚Üî Rust)
    QueryReadyTasks {
        requesting_core_id: String,
        max_tasks: usize,
        mutex_token: Option<String>,
    },

    ClaimReadyTasks {
        core_id: String,
        claimed_task_ids: Vec<i64>,
        mutex_token: String,
    },

    ReleaseMutex {
        core_id: String,
        mutex_token: String,
        completed_task_ids: Vec<i64>,
    },

    // System operations
    HealthCheck {
        diagnostic_level: HealthCheckLevel,
        include_worker_status: bool,
        include_ready_task_count: bool,
    },

    // üÜï System Management: Orchestration system lifecycle
    SystemManagement {
        action: SystemAction,  // startup, shutdown, health_check
        config: serde_json::Value,
    },
}

// üÜï Enhanced worker capabilities with database integration
#[derive(Serialize, Deserialize)]
pub struct SupportedTask {
    pub namespace: String,
    pub name: String,
    pub version: String,
    pub configuration: serde_json::Value,  // Task-specific config per worker
    pub priority: i32,                     // Worker preference for this task
}

#[derive(Serialize, Deserialize)]
pub struct WorkerMetadata {
    pub language_runtime: String,          // "ruby", "python", "node"
    pub runtime_version: String,
    pub max_concurrent_steps: usize,
    pub step_timeout_ms: u64,
    pub supports_retries: bool,
    pub hostname: String,
    pub pid: Option<u32>,
    pub started_at: String,               // ISO8601 timestamp
    pub custom_capabilities: serde_json::Value,
}

#[derive(Serialize, Deserialize)]
pub struct ConnectionInfo {
    pub connection_type: String,           // "tcp", "unix"
    pub host: Option<String>,             // For TCP connections
    pub listener_port: Option<u16>,       // Worker's listening port
    pub unix_socket_path: Option<String>, // For Unix domain sockets
    pub protocol_version: String,
    pub supported_commands: Vec<String>,
    pub reachable_from_cores: Vec<String>, // üÜï Which core instances can reach this worker
}

// üÜï Enhanced database schema for distributed transport awareness
#[derive(Serialize, Deserialize)]
pub struct WorkerTransportAvailability {
    pub worker_id: i32,
    pub core_instance_id: String,         // Which core instance this applies to
    pub transport_type: String,           // "tcp", "unix"
    pub connection_details: serde_json::Value,
    pub is_reachable: bool,               // Can this core actually reach this worker?
    pub last_verified_at: DateTime<Utc>,  // When reachability was last confirmed
}
```

## Critical Workflows: Command Pattern Integration

### Workflow 1: `initialize_task(task_request)` - Pure Command-Based Task Creation

**Evolution**: From FFI to pure command-based task initialization with network transparency

**Legacy Flow (FFI - üö´ Being Eliminated)**:
```
Rails Application ‚Üí TaskerCore.initialize_task(config) ‚Üí FFI ‚Üí Rust TaskInitializer ‚Üí Database
```

**New Flow (Pure Command - üéØ Target Architecture)**:
```
Client Application ‚Üí InitializeTask command ‚Üí TCP/Unix ‚Üí Rust TaskInitializer ‚Üí Database
```

**Command Implementation**:
```rust
// All task initialization through commands - no operational FFI
pub struct InitializeTaskCommand {
    pub command_type: "InitializeTask",
    pub payload: {
        pub task_request: TaskRequest {
            pub namespace: String,              // e.g., "fulfillment"
            pub name: String,                   // e.g., "process_order"
            pub version: String,                // e.g., "1.0.0"
            pub context: serde_json::Value,     // Task-specific data
            pub priority: Option<i32>,
            pub scheduled_for: Option<DateTime<Utc>>,
        },
        pub workflow_definition: Option<WorkflowDefinition> {
            pub steps: Vec<StepDefinition>,
            pub dependencies: Vec<StepDependency>,
            pub configuration: serde_json::Value,
        }
    }
}
```

**Response with Database Integration**:
```rust
pub struct InitializeTaskResponse {
    pub command_type: "TaskInitialized",
    pub payload: {
        pub task_id: i64,
        pub step_count: usize,
        pub estimated_duration_seconds: Option<i32>,
        pub assigned_workers: Vec<WorkerAssignment>,  // üÜï Database-backed worker pre-selection
        pub workflow_graph: WorkflowGraph,
    }
}

// üÜï Database-backed worker pre-selection during task creation
pub struct WorkerAssignment {
    pub step_name: String,
    pub worker_candidates: Vec<WorkerCandidate> {
        pub worker_id: i32,
        pub worker_name: String,
        pub priority: i32,
        pub current_load: usize,
        pub estimated_capacity: usize,
    }
}
```

### Workflow 2: `handle(task_id)` - Workflow Execution Through Commands

**Evolution**: From reactive (framework-initiated) to hybrid reactive/proactive execution

**Current Reactive Flow (ZeroMQ - ‚úÖ Working)**:
```
Rails ‚Üí handle(task_id) ‚Üí WorkflowCoordinator ‚Üí ZeroMQ pub ‚Üí Ruby workers ‚Üí ZeroMQ results
```

**Future Command Flow (Hybrid - üéØ Target)**:
```
Option A (Reactive): Client ‚Üí ExecuteWorkflow command ‚Üí Rust WorkflowCoordinator
Option B (Proactive): Rust WorkflowCoordinator loop ‚Üí discovers ready tasks ‚Üí processes internally
```

**Command Implementation for Reactive Execution**:
```rust
pub struct ExecuteWorkflowCommand {
    pub command_type: "ExecuteWorkflow",
    pub payload: {
        pub task_id: i64,
        pub execution_mode: ExecutionMode,     // immediate, scheduled, background
        pub worker_preferences: Option<WorkerPreferences> {
            pub preferred_connection_type: Option<String>, // "tcp", "unix"
            pub max_workers: Option<usize>,
            pub timeout_seconds: Option<i32>,
        }
    }
}
```

**Internal Proactive Discovery (Rust-initiated)**:
```rust
// üÜï Proactive workflow processing - Rust discovers ready work
impl WorkflowCoordinator {
    pub async fn proactive_orchestration_loop(&self) -> Result<(), OrchestrationError> {
        loop {
            // 1. Acquire distributed mutex for ready task discovery
            let mutex_token = self.acquire_ready_task_mutex().await?;

            // 2. Query SQL functions for ready tasks (like existing TaskExecutionContext)
            let ready_tasks = self.discover_ready_tasks_for_processing(10).await?;

            // 3. Process each ready task using database-backed worker selection
            for task_id in ready_tasks {
                self.process_ready_task_with_commands(task_id).await?;
            }

            // 4. Release mutex and wait before next discovery cycle
            self.release_ready_task_mutex(mutex_token).await?;
            tokio::time::sleep(Duration::from_secs(5)).await; // Configurable interval
        }
    }

    async fn process_ready_task_with_commands(&self, task_id: i64) -> Result<(), OrchestrationError> {
        // Discover viable steps using existing SQL functions
        let viable_steps = self.discover_viable_steps(task_id).await?;

        if !viable_steps.is_empty() {
            // Use database-backed worker selection instead of in-memory registry
            let selected_workers = self.worker_selection_service
                .select_workers_for_steps(&viable_steps).await?;

            // Send ExecuteBatch commands to selected workers
            for (worker, steps) in selected_workers.group_by_worker() {
                let batch_command = ExecuteBatchCommand {
                    batch_id: generate_batch_id(),
                    task_template: self.get_task_template(task_id).await?,
                    steps: steps,
                    worker_context: WorkerExecutionContext::from_database(&worker),
                };

                self.send_command_to_worker(&worker, batch_command).await?;
            }
        }

        Ok(())
    }
}
```

### Workflow 3: Performance and Analytics Through Commands

**Evolution**: From FFI to pure command-based analytics with worker integration

**Legacy Flow (FFI - üö´ Being Eliminated)**:
```
Ruby ‚Üí performance FFI methods ‚Üí Rust analytics ‚Üí Database queries ‚Üí Results
```

**New Flow (Pure Command with Worker Integration)**:
```
Client ‚Üí AnalyticsQuery command ‚Üí Rust analytics + worker performance data ‚Üí Aggregated results
```

**Enhanced Analytics Commands**:
```rust
pub struct AnalyticsQueryCommand {
    pub command_type: "QueryAnalytics",
    pub payload: {
        pub query_type: AnalyticsQueryType,
        pub time_range: TimeRange,
        pub include_worker_performance: bool,  // üÜï Worker-specific analytics
        pub include_command_audit_trail: bool, // üÜï Command performance metrics
    }
}

pub enum AnalyticsQueryType {
    TaskPerformance { task_namespace: Option<String> },
    WorkerEfficiency { worker_names: Option<Vec<String>> },
    CommandLatency { command_types: Option<Vec<String>> },
    SystemHealth { include_ready_task_backlog: bool },
    // üÜï Database-backed worker analytics
    WorkerTaskCompatibility,
    WorkerLoadDistribution,
    CommandAuditSummary,
}
```

### Example Command Flows

### Database-Backed Worker Registration Flow

**üÜï Enhanced Registration with Explicit Task Support**:
```json
// Ruby Worker ‚Üí Rust Command Server (Database-backed registration)
{
  "command_type": "RegisterWorker",
  "command_id": "cmd_12345",
  "metadata": {
    "timestamp": "2025-07-28T10:30:00Z",
    "source": { "type": "ruby_worker", "id": "fulfillment_worker_01" },
    "timeout_ms": 5000
  },
  "worker_routing": {
    "connection_preference": "tcp",
    "selection_strategy": "least_loaded"
  },
  "payload": {
    "type": "RegisterWorker",
    "data": {
      "worker_name": "fulfillment_worker_01",
      "supported_tasks": [
        {
          "namespace": "fulfillment",
          "name": "process_order",
          "version": "1.0.0",
          "configuration": {
            "max_order_value": 10000,
            "supported_payment_methods": ["credit_card", "paypal"]
          },
          "priority": 100
        },
        {
          "namespace": "fulfillment",
          "name": "ship_order",
          "version": "1.0.0",
          "configuration": {
            "shipping_carriers": ["ups", "fedex"],
            "max_weight_lbs": 50
          },
          "priority": 90
        }
      ],
      "connection_info": {
        "connection_type": "tcp",
        "host": "0.0.0.0",
        "listener_port": 8080,
        "protocol_version": "2.0",
        "supported_commands": ["ExecuteBatch", "HealthCheck", "Shutdown"]
      },
      "metadata": {
        "language_runtime": "ruby",
        "runtime_version": "3.1.0",
        "max_concurrent_steps": 10,
        "step_timeout_ms": 30000,
        "supports_retries": true,
        "hostname": "worker-01.fulfillment.local",
        "pid": 67890,
        "started_at": "2025-07-28T10:29:45Z"
      }
    }
  }
}

// Rust Command Server ‚Üí Ruby Worker (Database-backed acknowledgment)
{
  "command_type": "WorkerRegistered",
  "command_id": "resp_12345",
  "correlation_id": "cmd_12345",
  "metadata": {
    "timestamp": "2025-07-28T10:30:00.150Z",
    "source": { "type": "rust_command_server", "id": "core_01" }
  },
  "audit_context": {
    "initiated_by": "fulfillment_worker_01",
    "processing_core_id": "core_01"
  },
  "payload": {
    "type": "Success",
    "data": {
      "worker_database_id": 42,
      "worker_name": "fulfillment_worker_01",
      "registered_task_count": 2,
      "assigned_task_coverage": [
        {
          "namespace": "fulfillment",
          "name": "process_order",
          "version": "1.0.0",
          "competing_workers": 3,
          "your_priority_rank": 1
        },
        {
          "namespace": "fulfillment",
          "name": "ship_order",
          "version": "1.0.0",
          "competing_workers": 2,
          "your_priority_rank": 2
        }
      ],
      "heartbeat_interval_seconds": 30,
      "next_heartbeat_expected_at": "2025-07-28T10:30:30Z"
    }
  }
}
```

### Database-Backed Batch Execution Flow

**üÜï Enhanced Execution with Worker Selection and Command Audit**:
```json
// Rust WorkflowCoordinator ‚Üí Selected Worker (Database-backed routing)
{
  "command_type": "ExecuteBatch",
  "command_id": "batch_cmd_456",
  "metadata": {
    "timestamp": "2025-07-28T10:35:00Z",
    "source": { "type": "rust_orchestrator", "id": "core_01" },
    "target": { "type": "ruby_worker", "id": "fulfillment_worker_01" },
    "namespace": "fulfillment",
    "priority": "normal"
  },
  "worker_routing": {
    "target_worker_id": 42,
    "selection_strategy": "least_loaded",
    "connection_preference": "tcp"
  },
  "audit_context": {
    "batch_id": "batch_789",
    "task_id": 1001,
    "initiated_by": "proactive_orchestration_loop",
    "processing_core_id": "core_01"
  },
  "payload": {
    "type": "ExecuteBatch",
    "data": {
      "batch_id": "batch_789",
      "task_template": {
        "namespace": "fulfillment",
        "name": "process_order",
        "version": "1.0.0",
        "configuration": {
          "max_order_value": 10000,
          "timeout_seconds": 300
        }
      },
      "steps": [
        {
          "step_id": 42,
          "step_name": "validate_order",
          "step_context": {
            "order_id": 12345,
            "customer_id": 67890,
            "order_total": 299.99
          },
          "dependencies": [
            {
              "step_name": "create_order",
              "result": { "order_created": true, "order_id": 12345 }
            }
          ]
        }
      ],
      "worker_context": {
        "expected_execution_time_ms": 5000,
        "retry_policy": {
          "max_retries": 3,
          "backoff_strategy": "exponential"
        },
        "worker_configuration": {
          "max_order_value": 10000,
          "supported_payment_methods": ["credit_card", "paypal"]
        }
      }
    }
  }
}```
```

## Database-Backed Distributed Mutex Design

### Challenge: Multiple Rust Cores Processing Ready Tasks

In a distributed system with multiple Rust cores, we need coordination to prevent race conditions when discovering and processing ready tasks.

**Problem**: Multiple Rust cores simultaneously query for ready tasks and attempt to process the same work
**Solution**: Database-backed distributed mutex with atomic task claiming

### SQL-Based Mutex Implementation

#### Option 1: Advisory Locks with Atomic Task Claims
```sql
-- Rust core atomically claims a batch of ready tasks
UPDATE tasker_tasks
SET processing_core_id = $core_id, claimed_at = NOW(), state = 'claimed'
WHERE task_id IN (
  SELECT task_id
  FROM get_ready_tasks_for_processing($core_id, 10) -- Batch size limit
  FOR UPDATE SKIP LOCKED                           -- PostgreSQL: avoid blocking
)
AND processing_core_id IS NULL
AND state = 'ready'
RETURNING task_id, namespace, name, version;
```

#### Option 2: Distributed Lock with Expiry
```sql
-- Core acquires exclusive lock for ready task discovery period
INSERT INTO tasker_distributed_processing_locks (lock_name, owner_core_id, expires_at, metadata)
VALUES ('ready_task_discovery', $core_id, NOW() + INTERVAL '30 seconds', $metadata)
ON CONFLICT (lock_name) DO UPDATE
SET owner_core_id = $core_id, expires_at = NOW() + INTERVAL '30 seconds', metadata = $metadata
WHERE tasker_distributed_processing_locks.expires_at < NOW()
RETURNING owner_core_id;

-- Lock table schema
CREATE TABLE tasker_distributed_processing_locks (
    lock_name VARCHAR(255) PRIMARY KEY,
    owner_core_id VARCHAR(255) NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    acquired_at TIMESTAMP DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'
);
```

#### Option 3: Hybrid Approach (Recommended)
```sql
-- Combine lock acquisition with atomic task claiming
WITH mutex_acquired AS (
  INSERT INTO tasker_distributed_processing_locks (lock_name, owner_core_id, expires_at)
  VALUES ('ready_task_batch_' || $batch_uuid, $core_id, NOW() + INTERVAL '5 minutes')
  ON CONFLICT (lock_name) DO NOTHING
  RETURNING owner_core_id
),
claimed_tasks AS (
  UPDATE tasker_tasks
  SET processing_core_id = $core_id, claimed_at = NOW(), state = 'claimed'
  WHERE task_id IN (
    SELECT task_id FROM get_ready_tasks_for_processing(10)
    FOR UPDATE SKIP LOCKED
  )
  AND processing_core_id IS NULL
  AND EXISTS (SELECT 1 FROM mutex_acquired WHERE owner_core_id = $core_id)
  RETURNING task_id, namespace, name, version
)
SELECT
  ct.task_id,
  ct.namespace,
  ct.name,
  ct.version,
  ma.owner_core_id IS NOT NULL as mutex_acquired
FROM claimed_tasks ct
FULL OUTER JOIN mutex_acquired ma ON true;
```

### Rust Implementation

```rust
pub struct DistributedOrchestrationCoordinator {
    db_pool: PgPool,
    core_id: String,
    mutex_service: DistributedMutexService,
    worker_selection_service: WorkerSelectionService,
}

impl DistributedOrchestrationCoordinator {
    pub async fn proactive_orchestration_loop(&self) -> Result<(), OrchestrationError> {
        loop {
            // Attempt to acquire mutex and claim ready tasks in single transaction
            let claimed_tasks = self.acquire_and_claim_ready_tasks().await?;

            if !claimed_tasks.is_empty() {
                // Process claimed tasks using database-backed worker selection
                for task in claimed_tasks {
                    self.process_claimed_task_with_workers(task).await?;
                }
            }

            // Wait before next discovery cycle (configurable interval)
            tokio::time::sleep(Duration::from_secs(5)).await;
        }
    }

    async fn acquire_and_claim_ready_tasks(&self) -> Result<Vec<ClaimedTask>, OrchestrationError> {
        let batch_uuid = Uuid::new_v4().to_string();

        let claimed_tasks = sqlx::query_as::<_, ClaimedTask>(
            include_str!("../sql/acquire_and_claim_ready_tasks.sql")
        )
        .bind(&self.core_id)
        .bind(&batch_uuid)
        .fetch_all(&self.db_pool)
        .await?;

        // Log coordination metrics
        tracing::info!(
            core_id = %self.core_id,
            claimed_count = claimed_tasks.len(),
            batch_uuid = %batch_uuid,
            "Claimed ready tasks for processing"
        );

        Ok(claimed_tasks)
    }

    async fn process_claimed_task_with_workers(&self, task: ClaimedTask) -> Result<(), OrchestrationError> {
        // Use database-backed worker selection based on task namespace/name/version
        let selected_workers = self.worker_selection_service
            .select_workers_for_task(&task.namespace, &task.name, &task.version)
            .await?;

        if selected_workers.is_empty() {
            tracing::warn!(
                task_id = task.task_id,
                namespace = %task.namespace,
                name = %task.name,
                "No available workers for claimed task - releasing back to ready state"
            );

            // Release task back to ready state
            self.release_claimed_task(task.task_id).await?;
            return Ok(());
        }

        // Discover viable steps for the task
        let viable_steps = self.discover_viable_steps(task.task_id).await?;

        if viable_steps.is_empty() {
            // Task may not have viable steps yet - release back to ready
            self.release_claimed_task(task.task_id).await?;
            return Ok(());
        }

        // Send ExecuteBatch commands to selected workers
        let batch_id = format!("batch_{}_{}", task.task_id, Uuid::new_v4());

        for worker in selected_workers {
            let batch_command = ExecuteBatchCommand {
                command_type: "ExecuteBatch".to_string(),
                command_id: Uuid::new_v4().to_string(),
                metadata: CommandMetadata {
                    timestamp: Utc::now(),
                    source: CommandSource::RustCore(self.core_id.clone()),
                    target: Some(CommandTarget::Worker(worker.worker_name.clone())),
                    namespace: Some(task.namespace.clone()),
                    priority: Some(CommandPriority::Normal),
                },
                worker_routing: Some(WorkerRouting {
                    target_worker_id: Some(worker.worker_id),
                    selection_strategy: WorkerSelectionStrategy::DatabaseSelected,
                    connection_preference: worker.preferred_connection_type(),
                }),
                audit_context: Some(AuditContext {
                    batch_id: Some(batch_id.clone()),
                    task_id: Some(task.task_id),
                    initiated_by: "proactive_orchestration".to_string(),
                    processing_core_id: Some(self.core_id.clone()),
                }),
                payload: CommandPayload::ExecuteBatch {
                    batch_id: batch_id.clone(),
                    task_template: TaskTemplate {
                        namespace: task.namespace.clone(),
                        name: task.name.clone(),
                        version: task.version.clone(),
                        configuration: worker.get_task_configuration(&task.namespace, &task.name)?,
                    },
                    steps: viable_steps.clone(),
                    worker_context: WorkerExecutionContext::from_database_worker(&worker),
                },
            };

            // üö® CRITICAL: Only send to workers this core instance can actually reach
            if self.can_reach_worker(&worker).await? {
                self.send_command_to_worker(&worker, batch_command).await?;
            } else {
                tracing::warn!(
                    worker_id = worker.worker_id,
                    core_instance = self.core_id,
                    "Skipping worker - not reachable from this core instance"
                );
            }
        }

        Ok(())
    }
}
```

### Benefits of Database-Backed Mutex

1. **Race Condition Prevention**: Multiple Rust cores can't claim the same ready tasks
2. **Fair Work Distribution**: PostgreSQL FOR UPDATE SKIP LOCKED ensures fair queuing
3. **Automatic Cleanup**: Expired locks are automatically cleaned up by subsequent cores
4. **Audit Trail**: Complete coordination history with core IDs and timing
5. **Horizontal Scaling**: Supports unlimited number of distributed Rust cores
6. **Failure Recovery**: If a core crashes, its claimed tasks can be reclaimed after timeout

## Command Pattern: Pure Command Architecture

### Pure Command Architecture: Eliminating Operational FFI

The evolution eliminates all operational FFI in favor of consistent command-based communication:

**FFI Only for Infrastructure (‚úÖ Limited scope)**:
```ruby
# System lifecycle only
TaskerCore.start_embedded_server  # ‚Üí FFI (orchestration system startup)

# Infrastructure object creation only
client = TaskerCore.create_command_client(endpoint)     # ‚Üí FFI (creates Rust-backed client)
manager = TaskerCore.create_worker_manager(config)      # ‚Üí FFI (creates Rust-backed manager)
listener = TaskerCore.create_command_listener(config)   # ‚Üí FFI (creates Rust-backed listener)
```

**Commands for ALL Operations (üéØ Consistent architecture)**:
```ruby
# Task operations - no FFI
client.initialize_task(context)        # ‚Üí InitializeTask command (TCP/Unix)
client.execute_workflow(task_id)       # ‚Üí ExecuteWorkflow command (TCP/Unix)
client.query_analytics(params)         # ‚Üí AnalyticsQuery command (TCP/Unix)

# Worker operations - no FFI
manager.register_worker(capabilities)  # ‚Üí RegisterWorker command (TCP/Unix)
manager.send_heartbeat                 # ‚Üí Heartbeat command (TCP/Unix)

# All orchestration through commands
coordinator.discover_ready_tasks()     # ‚Üí QueryReadyTasks command (TCP/Unix)
coordinator.execute_batch(batch)       # ‚Üí ExecuteBatch command (TCP/Unix)
```

**Configuration-Driven Transport Selection**:
```ruby
# Transport selection based on configuration and worker reachability
class TaskerCore::CommandClient
  def initialize_task(context)
    # Always command-based - transport determined by config and availability
    available_transports = @transport_manager.get_available_transports
    selected_transport = @transport_manager.select_transport_for_operation(
      operation: :initialize_task,
      available: available_transports,
      config: @deployment_config
    )
    
    send_command(command, transport: selected_transport)
  end
  
  # Transport manager handles reachability and configuration
  def select_worker_for_task(task_namespace, task_name)
    @worker_selection_service.find_reachable_worker(
      namespace: task_namespace,
      name: task_name,
      core_instance_id: @instance_id,  # Only workers this instance can reach
      available_transports: @transport_manager.get_available_transports
    )
  end
end
```

### Distributed Transport Availability Challenge

**The Problem**: In hybrid deployments with multiple core instances, some workers may be reachable via Unix sockets from certain cores but not others. A core instance must never attempt to send work to a "local" Unix socket worker that it cannot actually reach.

**Example Scenario**: 
- 3 Docker containers running tasker-core instances (core-1, core-2, core-3)
- core-1 has local Ruby workers on Unix sockets `/tmp/worker-a.sock`
- core-2 has different local Ruby workers on Unix sockets `/tmp/worker-b.sock`  
- core-3 has no local workers, only TCP connections to remote workers
- All workers register capabilities in shared database, but with different transport availability

**Database Solution**:
```sql
-- Enhanced worker_registrations table
CREATE TABLE worker_transport_availability (
    id SERIAL PRIMARY KEY,
    worker_id INTEGER REFERENCES workers(id),
    core_instance_id VARCHAR(255) NOT NULL,  -- Which core can reach this worker
    transport_type VARCHAR(50) NOT NULL,     -- "tcp", "unix"
    connection_details JSONB NOT NULL,       -- Host/port or socket path
    is_reachable BOOLEAN NOT NULL DEFAULT false,
    last_verified_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(worker_id, core_instance_id, transport_type)
);

-- Worker selection must consider reachability
SELECT w.*, wta.transport_type, wta.connection_details
FROM workers w
JOIN worker_named_tasks wnt ON w.id = wnt.worker_id  
JOIN worker_transport_availability wta ON w.id = wta.worker_id
WHERE wnt.named_task_id = $1              -- Task we need done
  AND wta.core_instance_id = $2           -- THIS core instance
  AND wta.is_reachable = true             -- Actually reachable
  AND wta.last_verified_at > NOW() - INTERVAL '5 minutes'
ORDER BY wnt.priority DESC;
```

**Core Instance Awareness**:
```rust
impl DistributedOrchestrationCoordinator {
    async fn can_reach_worker(&self, worker: &Worker) -> Result<bool> {
        let reachability = self.worker_transport_repo.get_transport_availability(
            worker.id,
            &self.core_instance_id
        ).await?;
        
        match reachability {
            Some(transport) => Ok(transport.is_reachable && 
                                 transport.last_verified_at > Utc::now() - Duration::minutes(5)),
            None => {
                // Unknown reachability - test it and cache result
                self.verify_and_cache_worker_reachability(worker).await
            }
        }
    }
    
    async fn select_reachable_workers(&self, task: &NamedTask) -> Result<Vec<Worker>> {
        self.worker_selection_service.find_workers_for_task(
            &task.namespace,
            &task.name, 
            &task.version,
            Some(&self.core_instance_id)  // Only workers THIS core can reach
        ).await
    }
}
```

### Benefits of Pure Command + Database Architecture

1. **Intelligent Work Distribution**: Database-backed worker selection replaces namespace guessing
2. **Persistent Worker State**: Worker registrations and capabilities survive restarts
3. **Proactive Orchestration**: Rust cores discover and process ready tasks automatically
4. **Distributed Coordination**: Multiple Rust cores coordinate through database mutex
5. **Command Audit Trail**: Complete observability of all distributed operations
6. **Flexible Transport**: TCP for distributed, Unix for local, FFI for specific use cases
7. **Horizontal Scaling**: Database coordination enables unlimited distributed Rust cores
8. **Language Agnostic**: Any language can implement command protocol for worker registration
9. **Circuit Breakers**: Network-level failure handling with database-backed health tracking
10. **Replay/Debugging**: Commands stored in database with full correlation and timing data

### Database-Backed Worker Registration Lifecycle

**Enhanced Ruby Worker Lifecycle:**
```
1. Ruby process starts ‚Üí connects to Rust command server (TCP/Unix)
2. Discovers supported tasks from filesystem YAML configs
3. Sends RegisterWorker command with explicit task declarations:
   - worker_name: "fulfillment_worker_01" (persistent identity)
   - supported_tasks: [{ namespace: "fulfillment", name: "process_order", version: "1.0.0", config: {...} }]
   - connection_info: { type: "tcp", host: "worker-01.local", port: 8080 }
   - metadata: { runtime: "ruby", version: "3.1.0", max_concurrent: 10 }
4. Rust persists to database:
   - INSERT INTO tasker_workers (worker_name, metadata)
   - INSERT INTO tasker_worker_named_tasks (worker_id, named_task_id, configuration)
   - INSERT INTO tasker_worker_registrations (worker_id, status, connection_details)
5. Rust routes batches using database queries:
   - SELECT workers WHERE task_namespace/name/version matches
   - ORDER BY priority, current_load, last_heartbeat
   - Consider connection_type preference (TCP vs Unix)
6. Worker sends heartbeats ‚Üí UPDATE last_heartbeat_at
7. On disconnect ‚Üí UPDATE status = 'disconnected', unregistered_at = NOW()
8. Database state enables worker recovery and persistent task associations
```

**Database-Backed Intelligent Work Distribution:**
```rust
impl WorkerSelectionService {
    async fn select_workers_for_task(
        &self,
        namespace: &str,
        name: &str,
        version: &str
    ) -> Result<Vec<SelectedWorker>, SelectionError> {
        let query = r#"
            SELECT
                w.worker_id,
                w.worker_name,
                w.metadata,
                wr.connection_details,
                wr.connection_type,
                wnt.configuration as task_configuration,
                wnt.priority,
                wr.last_heartbeat_at,
                EXTRACT(EPOCH FROM (NOW() - wr.last_heartbeat_at)) as seconds_since_heartbeat
            FROM tasker_workers w
            INNER JOIN tasker_worker_registrations wr ON w.worker_id = wr.worker_id
            INNER JOIN tasker_worker_named_tasks wnt ON w.worker_id = wnt.worker_id
            INNER JOIN tasker_named_tasks nt ON wnt.named_task_id = nt.named_task_id
            WHERE nt.namespace = $1
              AND nt.name = $2
              AND nt.version = $3
              AND wr.status IN ('registered', 'healthy')
              AND wr.unregistered_at IS NULL
              AND wr.last_heartbeat_at > NOW() - INTERVAL '2 minutes'
            ORDER BY
                wnt.priority DESC,                    -- Higher priority workers first
                wr.last_heartbeat_at DESC,           -- Recently active workers first
                RANDOM()                             -- Load balancing tie-breaker
            LIMIT $4
        "#;

        let selected_workers = sqlx::query_as::<_, SelectedWorker>(query)
            .bind(namespace)
            .bind(name)
            .bind(version)
            .bind(self.max_workers_per_batch)
            .fetch_all(&self.db_pool)
            .await?;

        // Additional logic for connection preference and load balancing
        let balanced_workers = self.apply_load_balancing_strategy(selected_workers).await?;

        Ok(balanced_workers)
    }

    async fn apply_load_balancing_strategy(
        &self,
        workers: Vec<SelectedWorker>
    ) -> Result<Vec<SelectedWorker>, SelectionError> {
        // Query current load from worker_command_audit table
        let workers_with_load = futures::stream::iter(workers)
            .then(|worker| async move {
                let current_load = self.get_worker_current_load(&worker.worker_name).await?;
                Ok::<_, SelectionError>(SelectedWorker {
                    current_load: Some(current_load),
                    ..worker
                })
            })
            .try_collect::<Vec<_>>().await?;

        // Filter by capacity and sort by load
        let available_workers = workers_with_load
            .into_iter()
            .filter(|w| w.current_load.unwrap_or(0) < w.max_capacity())
            .collect::<Vec<_>>();

        // Apply selection strategy
        match self.selection_strategy {
            SelectionStrategy::LeastLoaded => {
                available_workers.into_iter()
                    .min_by_key(|w| w.current_load.unwrap_or(0))
                    .map(|w| vec![w])
                    .unwrap_or_default()
            },
            SelectionStrategy::RoundRobin => {
                // Implement round-robin logic with database state
                self.round_robin_selection(available_workers).await?
            },
            SelectionStrategy::Priority => {
                // Already sorted by priority in SQL query
                available_workers.into_iter().take(1).collect()
            }
        }
    }
}
```

This database-backed approach replaces in-memory worker pools with persistent, queryable worker state that enables sophisticated selection strategies, health tracking, and audit trails.

This evolution from ZeroMQ topic broadcasting ‚Üí database-backed intelligent routing represents a fundamental improvement in work distribution efficiency and observability.

## Benefits of Evolution

### Operational Benefits
1. **Zero External Dependencies**: Eliminates ZeroMQ deployment complexity
2. **Predictable Connection State**: TCP provides clear connection status
3. **Built-in Reliability**: TCP handles message delivery, ordering, and flow control
4. **Simplified Configuration**: Single server port, multiple client connections
5. **Enhanced Debugging**: Connection logs, message tracing, clear error reporting

### Performance Benefits
1. **Native Tokio Integration**: Optimal resource usage within existing async runtime
2. **Reduced Memory Footprint**: No ZeroMQ internal buffering layers
3. **Custom Protocol Optimization**: Message format tuned for specific use case
4. **Connection Pooling**: Multiple Ruby workers per Rust server instance
5. **Load Balancing**: Intelligent batch distribution across workers

### Reliability Benefits
1. **Guaranteed Message Delivery**: TCP reliability vs ZeroMQ fire-and-forget
2. **Automatic Reconnection**: Ruby clients reconnect with exponential backoff
3. **Graceful Degradation**: Circuit breaker patterns for worker failures
4. **Connection Health Monitoring**: Heartbeat mechanism for failure detection
5. **Graceful Shutdown**: Connection draining ensures in-flight batches complete

## Implementation Roadmap: Building on ZeroMQ Success

### ‚úÖ Phase 0: Foundation Complete (January 2025)
**Status**: ‚úÖ **COMPLETE** - Production-ready ZeroMQ TCP architecture operational

**Achievements**:
- ‚úÖ ZeroMQ TCP pub-sub architecture with fire-and-forget batch execution
- ‚úÖ Dual result pattern (partial results + batch completion) working
- ‚úÖ Ruby BatchStepExecutionOrchestrator with concurrent worker pools
- ‚úÖ Database integration with HABTM batch tracking and audit trails
- ‚úÖ Configuration-driven YAML system with environment detection
- ‚úÖ Type-safe dry-struct validation throughout Ruby integration
- ‚úÖ Performance: Sub-millisecond TCP communication, 10-1000+ concurrent steps

### üõ†Ô∏è Phase 1: Database-Backed Worker Registration (Week 1-2)
**Goal**: Replace in-memory TaskHandlerRegistry with database-backed worker system
**Status**: üõ†Ô∏è **IN PROGRESS** - Database migrations and models complete

**Tasks**:
- ‚úÖ Database migrations for workers, worker_named_tasks, worker_registrations, command_audit
- ‚úÖ Rust models: Worker, WorkerNamedTask, WorkerRegistration, WorkerCommandAudit
- üõ†Ô∏è Update WorkerManagementHandler to use database persistence
- [ ] Create WorkerSelectionService for intelligent database-backed routing
- [ ] Migrate BatchExecutionSender to use WorkerSelectionService
- [ ] Update Ruby workers to register explicit task support (not namespaces)

**Success Criteria**:
- Workers register with explicit task declarations in database
- Worker-task associations persist across restarts
- "No workers available" errors eliminated through proper task matching
- Namespace mismatch between TaskHandler registration and Worker registration resolved
- Worker selection queries complete in <10ms with proper indexing

### üéØ Phase 2: Command Pattern Integration (Week 3-4)
**Goal**: Evolution from ZeroMQ topics to structured commands while preserving working functionality
**Status**: üìã **NEXT PRIORITY** - Building on ZeroMQ foundation

**Command Infrastructure**:
- [ ] Implement `Command` struct with enhanced metadata and audit context
- [ ] Create `CommandRouter` with database-backed command audit trail
- [ ] Build `CommandBasedTcpExecutor` evolution of current ZeroMQ architecture
- [ ] Add command serialization with backward compatibility to ZeroMQ messages
- [ ] Implement command correlation and audit logging

**Ruby Worker Evolution**:
- [ ] Evolve `BatchStepExecutionOrchestrator` to `CommandBasedWorker`
- [ ] Replace ZeroMQ topic subscription with command listening
- [ ] Implement structured command responses with correlation IDs
- [ ] Add database-backed worker registration (explicit task support)
- [ ] Maintain backward compatibility during transition

**Success Criteria**:
- Commands work alongside existing ZeroMQ for gradual migration
- Worker registration persists to database with explicit task associations
- Command correlation and audit trail working end-to-end
- Performance equals or exceeds current ZeroMQ baseline
- Ruby workers can process both ZeroMQ and Command formats during transition

### üöÄ Phase 3: Proactive Orchestration Implementation (Week 5-6)
**Goal**: Enable Rust-initiated workflow processing using SQL-driven task discovery
**Status**: üîÆ **FUTURE STATE** - Revolutionary architecture enhancement

**Distributed Coordination**:
- [ ] Implement `DistributedOrchestrationCoordinator` with database-backed mutex
- [ ] Create SQL functions for atomic ready task discovery and claiming
- [ ] Add `ReadyTaskDiscoveryHandler` for proactive task identification
- [ ] Implement distributed lock management with expiry and cleanup
- [ ] Build coordination between multiple Rust cores

**Proactive Processing Loop**:
- [ ] Implement `proactive_orchestration_loop` with configurable intervals
- [ ] Add SQL-driven ready task discovery using existing TaskExecutionContext patterns
- [ ] Integrate database-backed worker selection for discovered tasks
- [ ] Create task claiming and releasing mechanisms with failure recovery
- [ ] Add comprehensive coordination metrics and monitoring

**Command Integration**:
- [ ] Implement `QueryReadyTasks`, `ClaimReadyTasks`, `ReleaseMutex` commands
- [ ] Add Rust-to-Rust command communication for core coordination
- [ ] Create internal `InitializeTask` and `ExecuteBatch` command flows
- [ ] Build proactive vs reactive orchestration mode switching

**Success Criteria**:
- Multiple Rust cores coordinate through database mutex without race conditions
- Ready tasks discovered and processed automatically without framework initiation
- Database-backed worker selection routes work to appropriate workers
- Proactive mode coexists with reactive `handle(task_id)` calls
- Distributed coordination completes task claiming in <100ms
- System processes ready tasks within 30 seconds of becoming viable

### üéõÔ∏è Phase 4: Multi-Transport & Pure Command Architecture (Week 7-8)
**Goal**: Support multiple transport types with intelligent selection based on use case
**Status**: üîÆ **ADVANCED FEATURES** - Flexible deployment options

**Transport-Agnostic Strategy**:
- [ ] **Configuration-driven transport selection** - no built-in preferences, purely config-based
- [ ] **Support multiple transports per instance** - TCP and Unix sockets simultaneously
- [ ] **Worker availability awareness** - each core instance knows which workers it can actually reach
- [ ] Create `TransportManager` with multi-transport coordination
- [ ] **Distributed worker registry** - database tracks which transports each worker supports per core instance
- [ ] **Intelligent routing** - batch sending only to workers actually reachable by the sending core instance

**Pure Command Architecture**:
- [ ] Eliminate all operational FFI (initialize_task, handle, analytics)
- [ ] Create command-based clients for all operations
- [ ] Implement configuration-driven transport selection (no transport preferences)
- [ ] Build Rust-backed command infrastructure objects (client, manager, listener)
- [ ] Add transport configuration validation and worker reachability detection

**Production Reliability**:
- [ ] Implement comprehensive health checking across all transports
- [ ] Add circuit breaker patterns for each transport type
- [ ] Create transport-appropriate connection management (pooling for TCP, direct for Unix)
- [ ] Implement graceful degradation and transport failover
- [ ] Add comprehensive monitoring and alerting for transport health

**Success Criteria**:
- Workers can connect via any configured transport (TCP, Unix sockets, etc.) transparently
- All operations use commands (no operational FFI - only infrastructure FFI)  
- Transport selection is purely configuration-driven with no built-in preferences
- Each core instance correctly identifies which workers are actually reachable via each transport
- Database tracks worker transport availability per core instance for intelligent routing
- System gracefully handles transport failures with automatic failover to available transports
- Complete observability across all transport types with unified monitoring

### üìö Phase 5: Production Optimization & Documentation (Week 9-10)
**Goal**: Production-grade performance, monitoring, and comprehensive documentation
**Status**: üéØ **PRODUCTION READINESS** - Final polish and documentation

**Performance Optimization**:
- [ ] Benchmark command pattern vs current ZeroMQ baseline
- [ ] Optimize database queries for worker selection and mutex coordination
- [ ] Implement connection pooling strategies for different deployment patterns
- [ ] Add intelligent batching for high-throughput scenarios
- [ ] Create performance monitoring dashboards and alerting

**Production Monitoring**:
- [ ] Implement comprehensive command audit trail analysis
- [ ] Add worker health monitoring with predictive failure detection
- [ ] Create distributed coordination monitoring (mutex contention, task claiming rates)
- [ ] Build performance analytics for worker selection efficiency
- [ ] Add capacity planning tools based on historical worker performance

**Documentation Excellence**:
- [ ] Document hybrid architecture decision tree (FFI vs Commands vs Transport types)
- [ ] Create deployment guides for different scenarios (single-process, distributed, hybrid)
- [ ] Build troubleshooting guides for distributed coordination issues
- [ ] Document database-backed worker registration best practices
- [ ] Create migration guides for existing ZeroMQ-based deployments

**Gradual Migration Strategy**:
- [ ] Implement feature flags for gradual ZeroMQ ‚Üí Command migration
- [ ] Create A/B testing framework for performance comparison
- [ ] Build rollback capabilities for safe production migration
- [ ] Document migration timeline and risk mitigation strategies
- [ ] **Preserve ZeroMQ as supported option during transition period**

**Success Criteria**:
- Command pattern performance equals or exceeds ZeroMQ baseline
- Database-backed worker selection completes in <10ms at 95th percentile
- Distributed coordination handles 100+ concurrent Rust cores without conflicts
- Complete documentation for all deployment scenarios and migration paths
- Production monitoring provides full observability into distributed operations
- Migration path enables zero-downtime transition from current ZeroMQ architecture

### üîÑ Phase 6: Migration & Coexistence (Ongoing)
**Goal**: Seamless transition from ZeroMQ to Command architecture with zero disruption
**Status**: üìã **CONTINUOUS** - Evolutionary approach preserving stability

**Coexistence Strategy**:
- [ ] Maintain ZeroMQ as stable, supported option during full transition
- [ ] Implement configuration-based architecture selection
- [ ] Create gradual migration tooling for worker-by-worker transition
- [ ] Build comprehensive compatibility testing between architectures
- [ ] Document long-term support timeline for ZeroMQ architecture

**Migration Tools**:
- [ ] Create automated migration scripts for worker configuration
- [ ] Build database migration tooling for existing worker state
- [ ] Implement configuration validation for hybrid deployments
- [ ] Add migration progress monitoring and rollback capabilities
- [ ] Create performance comparison dashboards during transition

**Future Evolution Path**:
- [ ] Plan integration with additional orchestration frameworks (Python, Node.js)
- [ ] Design extension points for future transport types (gRPC, HTTP/2, WebSockets)
- [ ] Create plugin architecture for custom worker selection strategies
- [ ] Plan integration with container orchestration (Kubernetes, Docker Swarm)
- [ ] Design multi-region coordination for global deployments

**Success Criteria**:
- Both ZeroMQ and Command architectures maintained simultaneously
- Organizations can migrate at their own pace without forced upgrades
- Performance and feature parity maintained across both architectures
- Clear migration path with comprehensive tooling and documentation
- Future extensibility designed into command architecture from the beginning

---

## Risk Management & Mitigation

### Technical Risks

1. **Database-Backed Coordination Latency**
   - **Risk**: Database queries for worker selection and mutex coordination add latency
   - **Mitigation**: Optimize queries with proper indexing, implement connection pooling, cache frequently accessed worker data
   - **Target**: Worker selection queries <10ms, mutex coordination <100ms

2. **Distributed Mutex Contention**
   - **Risk**: Multiple Rust cores competing for ready tasks create database lock contention
   - **Mitigation**: Use PostgreSQL FOR UPDATE SKIP LOCKED, implement exponential backoff, batch task claiming
   - **Monitoring**: Track mutex acquisition times and contention rates

3. **Database Single Point of Failure**
   - **Risk**: Database outage affects all distributed coordination
   - **Mitigation**: Implement database failover, maintain local worker cache, graceful degradation to direct connections
   - **Fallback**: Emergency mode with file-based worker registration

4. **Worker Registration State Inconsistency**
   - **Risk**: Worker registrations become stale or inconsistent across multiple Rust cores
   - **Mitigation**: Implement heartbeat-based health checking, automatic cleanup of stale registrations, worker state reconciliation
   - **Recovery**: Periodic full worker state synchronization

5. **Command Pattern Performance Overhead**
   - **Risk**: Structured commands with database audit trail slower than ZeroMQ topics
   - **Mitigation**: Benchmark early and often, implement async audit trail, batch database operations
   - **Target**: <1ms additional overhead per command vs ZeroMQ baseline

6. **Complex Migration Risks**
   - **Risk**: Gradual migration from ZeroMQ introduces bugs and inconsistent behavior
   - **Mitigation**: Comprehensive integration testing, feature flags for safe rollback, parallel running during transition
   - **Strategy**: Worker-by-worker migration with full compatibility validation

### Operational Risks

1. **Database Migration Complexity**
   - **Risk**: Database schema changes affect production systems
   - **Mitigation**: Blue-green database deployment, backward compatible migrations, comprehensive rollback procedures
   - **Testing**: Full database migration testing in staging environments

2. **Distributed System Complexity**
   - **Risk**: Multiple Rust cores introduce distributed system challenges (split-brain, coordination failures)
   - **Mitigation**: Implement proper consensus algorithms, health checking, automatic core registration/deregistration
   - **Monitoring**: Comprehensive distributed system monitoring and alerting

3. **Worker Discovery and Routing Complexity**
   - **Risk**: Database-backed worker selection introduces new failure modes
   - **Mitigation**: Implement fallback to available workers, cache worker state, graceful degradation strategies
   - **Recovery**: Manual worker override capabilities for emergency situations

## Success Metrics & Validation Criteria

### Reliability Metrics
- **Zero task processing failures** due to coordination race conditions
- **< 10ms worker selection queries** at 95th percentile
- **< 100ms distributed mutex coordination** for ready task claiming
- **99.9% uptime** for database-backed worker coordination
- **Graceful handling** of Rust core failures with automatic task reassignment

### Performance Metrics
- **Database-backed worker selection** faster than in-memory registry lookup (target: <5ms)
- **Command overhead** < 1ms additional latency vs current ZeroMQ baseline
- **Distributed coordination** handles 100+ concurrent Rust cores without contention
- **Memory efficiency** improved through persistent database state vs in-memory caches
- **Worker task matching** 100% accurate vs current namespace guessing errors

### Distributed System Metrics
- **Mutex contention rate** < 5% of ready task discovery attempts
- **Task claiming accuracy** 100% (no duplicate task processing across cores)
- **Worker registration persistence** survives all system restarts and failures
- **Cross-core coordination latency** < 50ms for task distribution
- **Database connection efficiency** maintains <100 total connections across all cores

### Operational Excellence Metrics
- **Zero "no workers available" errors** through accurate database-backed task-worker matching
- **< 30s recovery time** from database or individual Rust core failures
- **Complete audit trail** for all commands and worker interactions in database
- **Worker health detection** within 60 seconds of actual failure
- **Migration success rate** 100% for ZeroMQ ‚Üí Command pattern transitions

### Business Impact Metrics
- **Developer productivity** improved through clearer worker registration (explicit vs namespace guessing)
- **System observability** enhanced through database-backed command audit trails
- **Deployment flexibility** enabled through multiple transport options (TCP, Unix, FFI)
- **Horizontal scaling** proven through distributed Rust core coordination
- **Operational confidence** increased through persistent state and comprehensive monitoring

## Conclusion: Evolution to Distributed Orchestration Excellence

This architectural evolution from ZeroMQ success to command-based distributed orchestration with database-backed coordination represents a transformative advancement in workflow orchestration capabilities. Building on the proven ZeroMQ TCP foundation, we're creating a system that can scale horizontally, coordinate intelligently, and operate proactively.

### Architectural Transformation Summary

**üéØ BEFORE (ZeroMQ - ‚úÖ Production Success)**:
- Reactive orchestration: Frameworks call `handle(task_id)` to process work
- In-memory worker registration: Lost on restart, namespace guessing, no persistence
- Topic-based communication: Blind broadcasting with limited routing intelligence
- Single-core processing: One Rust core per deployment, no distributed coordination

**üöÄ AFTER (Command + Database - üéØ Target Architecture)**:
- **Proactive orchestration**: Rust cores discover and process ready tasks automatically
- **Database-backed workers**: Persistent registrations with explicit task support declarations
- **Intelligent routing**: Database queries select optimal workers based on capabilities and load
- **Distributed coordination**: Multiple Rust cores coordinate through database-backed mutex
- **Pure command communication**: Commands for all operations, infrastructure-only FFI, multiple transports

### Revolutionary Capabilities Enabled

1. **üéØ Proactive Task Processing**: Rust cores automatically discover and process ready work using SQL functions like TaskExecutionContext, eliminating the need for frameworks to initiate processing

2. **üóÑÔ∏è Database-Backed Intelligence**: Worker capabilities, task associations, and execution history persisted and queryable, enabling sophisticated routing decisions

3. **üåê Horizontal Scaling**: Multiple distributed Rust cores coordinate through database mutex, preventing race conditions while enabling unlimited horizontal scaling

4. **üîÑ Pure Command Architecture**: Consistent command-based operations for all workflows while preserving infrastructure FFI (startup, object creation) for optimal bootstrapping

5. **üìä Complete Observability**: Database-backed command audit trails provide unprecedented visibility into distributed orchestration operations

### Key Architectural Innovations

1. **üß† Database-Backed Intelligence**: Worker capabilities and task associations stored in database enable sophisticated routing decisions that eliminate "no workers available" errors

2. **‚ö° Proactive Orchestration**: Revolutionary shift from reactive (framework-initiated) to proactive (Rust-initiated) workflow processing using SQL-driven ready task discovery

3. **üéõÔ∏è Pure Command Strategy**: Commands for all operations, infrastructure-only FFI, automatic transport selection (TCP, Unix sockets)

4. **üåç Distributed Coordination**: Database-backed mutex enables multiple Rust cores to coordinate fairly without race conditions or duplicate work processing

5. **üìà Persistent State Management**: All worker registrations, task associations, and command history persist across restarts and failures, providing unprecedented reliability

6. **üîç Complete Audit Trail**: Database-backed command audit enables forensic analysis, performance optimization, and operational excellence

### Strategic Benefits Timeline

- **‚úÖ Immediate (Building on ZeroMQ Success)**: Preserve proven TCP communication while evolving to database-backed worker intelligence
- **üéØ Short-term (6 months)**: Eliminate worker registration issues through explicit database-backed task associations, enabling intelligent routing
- **üöÄ Long-term (12 months)**: Proactive orchestration with distributed Rust cores automatically discovering and processing ready work
- **üåü Future (18+ months)**: Multi-region coordination, advanced transport options, and integration with container orchestration platforms

### Migration Philosophy: Evolution, Not Revolution

This evolution respects and builds upon the excellent ZeroMQ foundation already established. Rather than replacing what works, we're enhancing it with database-backed intelligence and distributed coordination capabilities. The result is a system that maintains proven performance while opening up transformative new capabilities for distributed, intelligent workflow orchestration.

### Implementation Strategy: Gradual Evolution

The implementation strategy preserves the excellent ZeroMQ TCP foundation while gradually introducing database-backed intelligence and command pattern capabilities. This approach ensures:

- **Zero Disruption**: Current ZeroMQ architecture continues working throughout evolution
- **Incremental Value**: Each phase delivers immediate benefits while building toward the final vision
- **Risk Mitigation**: Gradual migration with comprehensive rollback capabilities at each step
- **Performance Validation**: Continuous benchmarking ensures each evolution step maintains or improves performance

This evolution transforms Tasker from a reactive orchestration service into a proactive, intelligent, distributed workflow processing system while maintaining the reliability and performance characteristics that make the current ZeroMQ architecture successful.

### Database-Backed Coordination Innovation

**BEFORE**: In-memory assumptions creating distributed system problems:
- TaskHandlerRegistry: `fulfillment` namespace registration
- WorkerManagementHandler: `default` namespace registration
- Result: "No workers available" errors due to namespace mismatch

**AFTER**: Database-backed explicit associations:
- Workers register with specific task support: `{ namespace: "fulfillment", name: "process_order", version: "1.0.0" }`
- Task execution queries database for compatible workers
- Result: 100% accurate worker-task matching with intelligent load balancing

### The Proactive Orchestration Revolution

The most transformative aspect of this evolution is the shift from reactive to proactive orchestration:

**Current**: `Rails ‚Üí handle(task_id) ‚Üí Rust orchestration ‚Üí finds work to do`
**Future**: `Rust ‚Üí discovers ready tasks via SQL ‚Üí processes automatically ‚Üí frameworks optional`

This enables Rust cores to become autonomous orchestration agents that actively discover and process ready work, transforming the system from a service that responds to requests into an intelligent system that proactively manages workflow execution.

**Recommendation**: Begin with Phase 1 (Database-Backed Worker Registration) to solve immediate namespace mismatch issues, then progress through the command pattern evolution toward the proactive orchestration vision. The database foundation established in Phase 1 enables all subsequent capabilities while providing immediate value through accurate worker-task matching.

---

## üéØ CORRECTED ARCHITECTURAL UNDERSTANDING (January 2025)

### Critical Architecture Insight: Transport vs Orchestration

**BREAKTHROUGH ANALYSIS**: After examining the existing ZmqPubSubExecutor, the correct architecture is:

#### Rust Core Role: Orchestrator (NOT Executor)
- **WorkflowCoordinator** discovers viable steps and creates batches
- **BatchExecutionHandler** SENDS ExecuteBatch commands TO Ruby workers (fire-and-forget)
- **ResultAggregationHandler** RECEIVES results FROM Ruby workers and delegates to existing orchestration
- **StateManager + TaskFinalizer** handle all orchestration logic (preserve existing value)

#### Ruby Worker Role: Executor (NOT Orchestrator)
- **Ruby workers** receive ExecuteBatch commands and execute step handlers
- **Ruby workers** send partial results and batch completion back to Rust
- **BatchStepExecutionOrchestrator** manages concurrent step execution

#### Command Pattern: Transport Layer Only
The command handlers are **thin transport adapters** that:
1. **BatchExecutionHandler**: Routes batches TO workers based on capabilities
2. **ResultAggregationHandler**: Receives results and delegates to existing StateManager/TaskFinalizer logic

#### Value Preservation from ZmqPubSubExecutor
Lines 402-418 and 575+ in `zeromq_pub_sub_executor.rs` show the CORRECT orchestration integration:
```rust
// Handle partial results - delegate to StateManager
state_manager.complete_step_with_results(*step_id, output.clone()).await
state_manager.fail_step_with_error(*step_id, error_message).await

// Handle batch completion - delegate to TaskFinalizer
task_finalizer.handle_no_viable_steps(task_id).await
```

This existing orchestration logic should be **extracted and reused**, not reimplemented.

#### Corrected Flow
```
WorkflowCoordinator
  ‚Üí discovers viable steps
  ‚Üí BatchExecutionHandler (SENDS ExecuteBatch TO Ruby)
  ‚Üí TCP Transport Layer
  ‚Üí Ruby Workers (execute steps concurrently)
  ‚Üí TCP Transport Layer
  ‚Üí ResultAggregationHandler (RECEIVES results, delegates to StateManager/TaskFinalizer)
  ‚Üí StateManager + TaskFinalizer (existing orchestration logic)
```

#### Implementation Fix Required
Current command handlers are **backwards** - they receive when they should send, and reimplement when they should delegate. The fix:

1. **Extract orchestration logic** from ZmqPubSubExecutor into reusable components
2. **Reverse BatchExecutionHandler** to SEND commands instead of receive
3. **Simplify ResultAggregationHandler** to delegate to extracted orchestration logic
4. **Preserve all existing** StateManager and TaskFinalizer integration

---

## Explicit Implementation Plan

### File-Level Dependencies and Creation Strategy

Based on analysis of the current ZeroMQ architecture documented in `docs/analysis-after-zeromq.md`, this section provides explicit file-level implementation details with dependency mapping.

### Phase 1: Command Infrastructure & TCP Foundation

#### Files to Create

**Rust Core Command Infrastructure:**
```
src/execution/command.rs                    # ‚Üê No dependencies, foundational
‚îú‚îÄ‚îÄ Command struct with CommandPayload enum
‚îú‚îÄ‚îÄ CommandMetadata with routing information
‚îú‚îÄ‚îÄ CommandResult and error handling
‚îî‚îÄ‚îÄ Serialization/deserialization traits

src/execution/command_router.rs             # ‚Üê Depends on: command.rs
‚îú‚îÄ‚îÄ CommandRouter with handler registry
‚îú‚îÄ‚îÄ CommandHandler trait definition
‚îú‚îÄ‚îÄ Command lifecycle management
‚îî‚îÄ‚îÄ Handler dispatch and error handling

src/execution/command_handlers/             # ‚Üê Depends on: command_router.rs
‚îú‚îÄ‚îÄ mod.rs                                  # Handler module exports
‚îú‚îÄ‚îÄ worker_management_handler.rs            # Worker registration/heartbeat
‚îú‚îÄ‚îÄ batch_execution_handler.rs              # Step batch distribution
‚îú‚îÄ‚îÄ result_aggregation_handler.rs           # Partial result collection
‚îú‚îÄ‚îÄ task_initialization_handler.rs          # Task creation commands
‚îî‚îÄ‚îÄ health_check_handler.rs                 # System diagnostics

src/execution/tokio_tcp_executor.rs         # ‚Üê Depends on: command_router.rs, worker_pool.rs
‚îú‚îÄ‚îÄ TokioTcpExecutor replacing ZmqPubSubExecutor
‚îú‚îÄ‚îÄ TCP server lifecycle management
‚îú‚îÄ‚îÄ Connection handling with async/await
‚îî‚îÄ‚îÄ Integration with CommandRouter

src/execution/worker_pool.rs                # ‚Üê Depends on: command.rs
‚îú‚îÄ‚îÄ WorkerPool with capability tracking
‚îú‚îÄ‚îÄ WorkerState and WorkerCapabilities
‚îú‚îÄ‚îÄ Load balancing algorithms
‚îú‚îÄ‚îÄ Connection health monitoring
‚îî‚îÄ‚îÄ Namespace-aware work distribution
```

**Database Models for Command Architecture:**
```
src/models/core/worker_registration.rs      # ‚Üê No dependencies
‚îú‚îÄ‚îÄ WorkerRegistration database model
‚îú‚îÄ‚îÄ Capability tracking and validation
‚îú‚îÄ‚îÄ Connection state management
‚îî‚îÄ‚îÄ Namespace support tracking

src/models/core/command_execution.rs        # ‚Üê No dependencies
‚îú‚îÄ‚îÄ CommandExecution audit trail
‚îú‚îÄ‚îÄ Command lifecycle tracking
‚îú‚îÄ‚îÄ Performance metrics collection
‚îî‚îÄ‚îÄ Error/retry tracking

migrations/20250726000001_create_command_tables.sql  # ‚Üê No dependencies
‚îú‚îÄ‚îÄ worker_registrations table
‚îú‚îÄ‚îÄ command_executions table
‚îú‚îÄ‚îÄ worker_capabilities join table
‚îî‚îÄ‚îÄ Indexes for performance
```

#### Files to Update

**Integration Points:**
```
src/ffi/shared/orchestration_system.rs     # ‚Üê Replace ZeroMQ with CommandRouter
‚îú‚îÄ‚îÄ Replace zeromq_executor with command_router: Arc<CommandRouter>
‚îú‚îÄ‚îÄ Remove all ZeroMQ initialization code
‚îú‚îÄ‚îÄ Use TokioTcpExecutor exclusively
‚îî‚îÄ‚îÄ Simplify to single execution path

src/orchestration/workflow_coordinator.rs  # ‚Üê Replace ZeroMQ execution path
‚îú‚îÄ‚îÄ Replace execute_via_zeromq() with execute_via_commands()
‚îú‚îÄ‚îÄ Remove ZmqPubSubExecutor usage entirely
‚îú‚îÄ‚îÄ Use TokioTcpExecutor exclusively
‚îî‚îÄ‚îÄ Simplify to single execution strategy

config/tasker-config.yaml                  # ‚Üê Replace ZeroMQ with TCP config
‚îú‚îÄ‚îÄ Replace zeromq section with tcp_server section
‚îú‚îÄ‚îÄ Environment-specific TCP endpoints (not separate file)
‚îú‚îÄ‚îÄ Worker registration settings in main config
‚îî‚îÄ‚îÄ Remove all ZeroMQ configuration
```

#### Dependency Map - Phase 1
```
command.rs
  ‚îî‚îÄ‚îÄ command_router.rs
      ‚îú‚îÄ‚îÄ command_handlers/*.rs
      ‚îî‚îÄ‚îÄ tokio_tcp_executor.rs
          ‚îî‚îÄ‚îÄ worker_pool.rs

orchestration_system.rs ‚Üê command_router.rs (replaces zeromq)
workflow_coordinator.rs ‚Üê orchestration_system.rs
config files ‚Üê independent (unified tasker-config.yaml only)
database models ‚Üê independent
migrations ‚Üê independent
```

### Phase 2: Ruby Command Client & Worker Registration

#### Files to Create

**Ruby Command Infrastructure:**
```
bindings/ruby/lib/tasker_core/orchestration/tcp_orchestrator.rb
‚îú‚îÄ‚îÄ Replace ZeromqOrchestrator functionality
‚îú‚îÄ‚îÄ TCP connection management
‚îú‚îÄ‚îÄ Command sending/receiving
‚îú‚îÄ‚îÄ Automatic reconnection logic
‚îî‚îÄ‚îÄ Worker capability advertisement

bindings/ruby/lib/tasker_core/orchestration/command_client.rb
‚îú‚îÄ‚îÄ TCP connection abstraction
‚îú‚îÄ‚îÄ Command correlation tracking
‚îú‚îÄ‚îÄ Connection pooling support
‚îú‚îÄ‚îÄ Error handling and circuit breaker
‚îî‚îÄ‚îÄ Async command processing

bindings/ruby/lib/tasker_core/orchestration/worker_capabilities.rb
‚îú‚îÄ‚îÄ Capability definition and validation
‚îú‚îÄ‚îÄ Namespace support declaration
‚îú‚îÄ‚îÄ Performance characteristics
‚îú‚îÄ‚îÄ Ruby runtime information
‚îî‚îÄ‚îÄ Registration payload building

bindings/ruby/lib/tasker_core/types/command_types.rb
‚îú‚îÄ‚îÄ Ruby Command structure matching Rust
‚îú‚îÄ‚îÄ CommandPayload variants as Ruby classes
‚îú‚îÄ‚îÄ JSON serialization compatibility
‚îú‚îÄ‚îÄ Type validation and conversion
‚îî‚îÄ‚îÄ Error handling structures
```

#### Files to Update

**Ruby Integration Points:**
```
bindings/ruby/lib/tasker_core/orchestration/batch_step_execution_orchestrator.rb
‚îú‚îÄ‚îÄ Replace zeromq_orchestrator with tcp_orchestrator
‚îú‚îÄ‚îÄ Remove all ZeroMQ dependencies and imports
‚îú‚îÄ‚îÄ Use TcpOrchestrator exclusively
‚îú‚îÄ‚îÄ Preserve external API (initialize_task, handle methods)
‚îî‚îÄ‚îÄ Remove ZeroMQ configuration options

bindings/ruby/lib/tasker_core/config.rb
‚îú‚îÄ‚îÄ Replace ZeroMQConfig with TcpConfig class
‚îú‚îÄ‚îÄ TCP server endpoint configuration from main tasker-config.yaml
‚îú‚îÄ‚îÄ Worker capability configuration
‚îú‚îÄ‚îÄ Remove ZeroMQ configuration support entirely
‚îî‚îÄ‚îÄ Environment-aware TCP settings from unified config

bindings/ruby/lib/tasker_core/internal/orchestration_manager.rb
‚îú‚îÄ‚îÄ Replace ZeroMQ handle with TCP command handle
‚îú‚îÄ‚îÄ Remove ZeroMQ orchestrator lifecycle management
‚îú‚îÄ‚îÄ Use CommandRouter handle exclusively
‚îú‚îÄ‚îÄ Simplify to single orchestrator type
‚îî‚îÄ‚îÄ Remove dual orchestrator complexity
```

#### Dependency Map - Phase 2
```
command_types.rb
  ‚îî‚îÄ‚îÄ tcp_orchestrator.rb
      ‚îú‚îÄ‚îÄ command_client.rb
      ‚îî‚îÄ‚îÄ worker_capabilities.rb

batch_step_execution_orchestrator.rb ‚Üê tcp_orchestrator.rb (replaces zeromq)
config.rb ‚Üê command_types.rb (removes ZeroMQConfig)
orchestration_manager.rb ‚Üê config.rb (simplified single orchestrator)
```

### Phase 3: Command Handlers & Worker Pool Management

#### Files to Update

**Handler Implementation:**
```
src/execution/command_handlers/worker_management_handler.rs
‚îú‚îÄ‚îÄ Process RegisterWorker commands
‚îú‚îÄ‚îÄ Maintain worker pool state
‚îú‚îÄ‚îÄ Handle heartbeat commands
‚îú‚îÄ‚îÄ Worker capability validation
‚îî‚îÄ‚îÄ Connection lifecycle management

src/execution/command_handlers/batch_execution_handler.rs
‚îú‚îÄ‚îÄ Process ExecuteBatch commands
‚îú‚îÄ‚îÄ Intelligent worker selection
‚îú‚îÄ‚îÄ Namespace compatibility checking
‚îú‚îÄ‚îÄ Load balancing implementation
‚îî‚îÄ‚îÄ Batch distribution coordination

src/execution/command_handlers/result_aggregation_handler.rs
‚îú‚îÄ‚îÄ Process ReportPartialResult commands
‚îú‚îÄ‚îÄ Process ReportBatchCompletion commands
‚îú‚îÄ‚îÄ Dual result pattern implementation
‚îú‚îÄ‚îÄ Result reconciliation logic
‚îî‚îÄ‚îÄ State machine integration

src/execution/worker_pool.rs
‚îú‚îÄ‚îÄ Worker capability matching
‚îú‚îÄ‚îÄ Load balancing algorithms
‚îú‚îÄ‚îÄ Connection health monitoring
‚îú‚îÄ‚îÄ Namespace-aware routing
‚îî‚îÄ‚îÄ Graceful worker removal
```

#### Integration Updates
```
src/execution/tokio_tcp_executor.rs
‚îú‚îÄ‚îÄ Full TCP server implementation
‚îú‚îÄ‚îÄ Connection acceptance and management
‚îú‚îÄ‚îÄ Command routing to handlers
‚îú‚îÄ‚îÄ Worker lifecycle coordination
‚îî‚îÄ‚îÄ Performance monitoring

src/orchestration/workflow_coordinator.rs
‚îú‚îÄ‚îÄ Complete command execution path
‚îú‚îÄ‚îÄ Feature flag implementation
‚îú‚îÄ‚îÄ Performance comparison logging
‚îú‚îÄ‚îÄ Backward compatibility preservation
‚îî‚îÄ‚îÄ Error handling unification
```

### Phase 4: Advanced Reliability & Command Infrastructure Completion

#### Files to Create

**Infrastructure Command Support:**
```
src/execution/command_handlers/system_management_handler.rs
‚îú‚îÄ‚îÄ Process SystemManagement commands (startup, shutdown)
‚îú‚îÄ‚îÄ Orchestration system lifecycle management
‚îú‚îÄ‚îÄ Configuration validation and setup
‚îú‚îÄ‚îÄ Health monitoring integration
‚îî‚îÄ‚îÄ Resource cleanup and shutdown

bindings/ruby/lib/tasker_core/infrastructure/system_manager.rb
‚îú‚îÄ‚îÄ Ruby interface for system lifecycle
‚îú‚îÄ‚îÄ Embedded server management
‚îú‚îÄ‚îÄ Configuration loading and validation
‚îú‚îÄ‚îÄ Health check coordination
‚îî‚îÄ‚îÄ Graceful shutdown procedures
```

#### Files to Update

**Reliability Enhancements:**
```
src/execution/command_router.rs
‚îú‚îÄ‚îÄ Command retry policies
‚îú‚îÄ‚îÄ Timeout handling
‚îú‚îÄ‚îÄ Circuit breaker integration
‚îú‚îÄ‚îÄ Command history and replay
‚îî‚îÄ‚îÄ Performance metrics collection

src/execution/worker_pool.rs
‚îú‚îÄ‚îÄ Connection pooling for multiple processes
‚îú‚îÄ‚îÄ Health check command processing
‚îú‚îÄ‚îÄ Automatic worker removal on failure
‚îú‚îÄ‚îÄ Load balancing optimization
‚îî‚îÄ‚îÄ Graceful degradation handling

bindings/ruby/lib/tasker_core/orchestration/tcp_orchestrator.rb
‚îú‚îÄ‚îÄ Exponential backoff reconnection
‚îú‚îÄ‚îÄ Connection pooling support
‚îú‚îÄ‚îÄ Health check implementation
‚îú‚îÄ‚îÄ Performance monitoring
‚îî‚îÄ‚îÄ Graceful shutdown handling
```

### Phase 5: Migration, Cleanup & Documentation

#### Files to Remove (After Migration Complete)

**ZeroMQ-Specific Components:**
```
src/execution/zeromq_pub_sub_executor.rs              # ‚Üê Replaced by TokioTcpExecutor
bindings/ruby/lib/tasker_core/orchestration/zeromq_orchestrator.rb  # ‚Üê Replaced by TcpOrchestrator

# ZeroMQ Dependencies
Cargo.toml                                            # ‚Üê Remove zmq dependency
bindings/ruby/Gemfile                                 # ‚Üê Remove ffi-rzmq dependency

# Configuration Cleanup
config/zeromq.yaml sections                          # ‚Üê Remove ZeroMQ config
bindings/ruby/config/zeromq.yaml                     # ‚Üê Replace with TCP config
```

#### Files to Update for Cleanup

**Configuration Simplification:**
```
config/tasker-config.yaml                  # ‚Üê Unified config file only
‚îú‚îÄ‚îÄ Remove zeromq sections completely
‚îú‚îÄ‚îÄ Consolidate TCP configuration in main file
‚îú‚îÄ‚îÄ Environment-specific TCP settings (test/development/production)
‚îú‚îÄ‚îÄ Worker registration settings
‚îî‚îÄ‚îÄ Remove separate ZeroMQ config files

bindings/ruby/lib/tasker_core/config.rb
‚îú‚îÄ‚îÄ Remove ZeroMQConfig class entirely
‚îú‚îÄ‚îÄ Consolidate into single TcpConfig class
‚îú‚îÄ‚îÄ Load TCP config from main tasker-config.yaml only
‚îú‚îÄ‚îÄ Remove dual configuration support complexity
‚îî‚îÄ‚îÄ Simplify environment detection logic
```

### Critical File Dependencies

#### Preservation Requirements (Never Remove)

**FFI Boundary Architecture:**
- `src/ffi/shared/orchestration_system.rs` - Core orchestration system (simplified to Command-only)
- `src/ffi/shared/handles.rs` - Handle-based architecture foundation (unchanged)
- `bindings/ruby/lib/tasker_core/internal/orchestration_manager.rb` - Ruby handle lifecycle (simplified)

**Core Orchestration (Communication-Independent):**
- `src/orchestration/workflow_coordinator.rs` - Core workflow logic (communication layer abstracted)
- `src/orchestration/state_manager.rs` - State management (communication layer independent)
- `src/orchestration/task_initializer.rs` - Task creation (communication layer independent)

**Database Models (Fully Reusable):**
- `src/models/core/step_execution_batch*.rs` - Batch concept identical for Command system
- All migration files - Database schema communication-layer independent
- Worker registration and command execution models - new additions

### Migration Strategy - Complete Replacement Approach

#### No Backward Compatibility - Clean Implementation Strategy

Since this is a new branch where ZeroMQ was recently added, we take a **complete replacement** approach:

1. **Phase 1**: Build Command infrastructure (ZeroMQ remains for pattern reference)
2. **Phase 2**: Replace ZeroMQ usage in Ruby layer completely
3. **Phase 3**: Replace ZeroMQ usage in Rust orchestration completely
4. **Phase 4**: Production testing and performance validation
5. **Phase 5**: **TOTAL ZEROMQ ELIMINATION** - delete all files, dependencies, config

#### Rollback Strategy
- **Git-based rollback**: Each phase is a separate commit/branch
- **Feature completeness**: Each phase results in fully working system
- **Performance validation**: Each phase includes benchmarking
- **Integration testing**: Each phase must pass full test suite

#### Implementation Pattern
```rust
// Phase 1-2: Build new system
impl OrchestrationSystem {
    fn new() -> Self {
        // Build Command infrastructure
        let command_router = CommandRouter::new();
        // ZeroMQ still exists for reference
    }
}

// Phase 3: Replace completely
impl OrchestrationSystem {
    fn execute_workflow(&self) {
        // Only Command system - ZeroMQ usage removed
        self.command_router.execute()
    }
}

// Phase 5: Total cleanup
// All ZeroMQ files deleted, no references remain
```

This approach results in **cleaner, simpler architecture** without dual-system complexity.

## Success Metrics and Validation

### Architectural Success Criteria

#### Database-Backed Intelligence (Phase 1)
- ‚úÖ Database schema supports explicit worker-task associations
- [ ] Zero "No workers available" errors through intelligent task-worker matching
- [ ] Worker registrations persist across all system restarts and failures
- [ ] Worker selection queries complete in <10ms with proper database indexing
- [ ] Complete audit trail of worker capabilities and performance available

#### Proactive Orchestration (Phase 2)
- [ ] Multiple Rust cores coordinate through database coordination without race conditions
- [ ] Ready tasks discovered and processed automatically without framework initiation
- [ ] Proactive mode coexists seamlessly with reactive `handle(task_id)` calls
- [ ] System processes ready tasks within 30 seconds of becoming viable
- [ ] Distributed coordination completes task claiming in <100ms

#### Ruby Simplification (Phase 3-4)
- [ ] Ruby codebase reduced by 70%+ through delegation to Rust infrastructure
- [ ] Essential test coverage maintained while dramatically simplifying implementation
- [ ] Command infrastructure handles all worker coordination and distribution
- [ ] Performance equals or exceeds current ZeroMQ baseline

### Performance Validation

#### System Performance
- [ ] Database-backed worker selection faster than in-memory registry lookup (target: <5ms)
- [ ] Command infrastructure overhead < 1ms additional latency vs current ZeroMQ baseline
- [ ] Distributed coordination handles 100+ concurrent Rust cores without contention
- [ ] Memory efficiency improved through persistent database state vs in-memory caches

#### Distributed Coordination
- [ ] Coordination contention rate < 5% of ready task discovery attempts
- [ ] Task claiming accuracy 100% (no duplicate task processing across cores)
- [ ] Cross-core coordination latency < 50ms for task distribution
- [ ] Database connection efficiency maintains <100 total connections across all cores

### Operational Excellence

#### Reliability Metrics
- [ ] Worker health detection within 60 seconds of actual failure
- [ ] < 30s recovery time from database or individual Rust core failures
- [ ] Complete audit trail for all commands and worker interactions in database
- [ ] Graceful handling of Rust core failures with automatic task reassignment

#### Business Impact
- [ ] Developer productivity improved through clearer worker registration (explicit vs namespace guessing)
- [ ] System observability enhanced through database-backed command audit trails
- [ ] Deployment flexibility enabled through multiple transport options (TCP, Unix, FFI)
- [ ] Horizontal scaling proven through distributed Rust core coordination
- [ ] Operational confidence increased through persistent state and comprehensive monitoring

## Conclusion: The Evolution to Intelligent Distributed Orchestration

### üéØ Architectural Transformation Summary

This document outlines the evolution from successful ZeroMQ TCP communication to a revolutionary **proactive distributed orchestration system**. Building on proven foundations, we're solving critical architectural problems while unlocking transformative new capabilities.

#### üîß Current State: Solid Foundation with Critical Gap
- **‚úÖ ZeroMQ TCP Success**: Production-ready fire-and-forget batch execution with dual result pattern
- **‚úÖ Performance Excellence**: Sub-millisecond communication supporting 1000+ concurrent steps  
- **‚úÖ Database Integration**: Complete HABTM batch tracking with audit trails
- **üîß Critical Issue**: Namespace mismatch preventing intelligent worker selection ("No workers available")

#### üöÄ Evolution Goals: From Reactive to Proactive Intelligence

**DATABASE-BACKED INTELLIGENCE** (Phase 1):
- Replace namespace guessing with explicit worker-task associations in database
- Enable intelligent worker selection based on capabilities, health, and load
- Provide persistent worker state that survives restarts and failures

**PROACTIVE ORCHESTRATION** (Phase 2):
- Transform from reactive (frameworks call `handle(task_id)`) to proactive (Rust discovers ready work)
- Enable multiple Rust cores to coordinate through transient database coordination
- Leverage existing SQL functions (`get_task_execution_context`) for ready task discovery

**RUBY SIMPLIFICATION** (Phase 3-4):
- Reduce Ruby codebase by 70%+ through delegation to Rust infrastructure
- Transform complex Ruby orchestration into thin client wrappers
- Preserve essential test coverage while dramatically simplifying implementation

### üéÜ Revolutionary Capabilities Enabled

1. **üß† Intelligent Work Distribution**: Database queries replace namespace guessing, eliminating "No workers available" errors

2. **‚ö° Autonomous Task Processing**: Rust cores autonomously discover and process ready tasks using SQL-driven coordination

3. **üåê Horizontal Scaling**: Multiple distributed Rust cores coordinate through database coordination without race conditions

4. **üîÑ Pure Command Architecture**: Migrate all operations to commands while preserving infrastructure FFI for system lifecycle

5. **üìä Complete Observability**: Database-backed audit trails provide unprecedented visibility into distributed operations

### üèóÔ∏è Implementation Strategy: Evolution, Not Revolution

**PRESERVE SUCCESS**:
- ZeroMQ TCP communication foundation (proven, performant)
- FFI task creation and analytics interfaces (working excellently)
- Existing state machine orchestration logic (production-ready)
- Essential test frameworks and patterns

**FIX PROBLEMS**:
- Replace in-memory worker registration with database-backed explicit associations
- Eliminate namespace mismatch through intelligent database-driven worker selection
- Add distributed coordination for multiple Rust cores

**UNLOCK POTENTIAL**:
- Enable proactive orchestration alongside reactive execution
- Simplify Ruby codebase through delegation to Rust infrastructure
- Create foundation for unlimited horizontal scaling

### üìà Expected Impact

#### Short-Term Benefits (Phase 1 - 2 months)
- **Zero "No Workers Available" Errors**: Explicit database associations eliminate current routing failures
- **Persistent Worker State**: Worker capabilities survive restarts and system failures
- **Intelligent Load Balancing**: Priority-based worker selection with health tracking

#### Medium-Term Transformation (Phase 2 - 4 months)  
- **Proactive Orchestration**: Rust cores autonomously discover and process ready work
- **Distributed Coordination**: Multiple Rust cores coordinate without conflicts
- **Enhanced Observability**: Complete audit trails for all distributed operations

#### Long-Term Revolution (Phase 3-4 - 6 months)
- **Dramatic Simplification**: 70%+ reduction in Ruby orchestration complexity
- **Multi-Language Foundation**: Same Rust infrastructure supports Python, Node.js, etc.
- **Unlimited Scaling**: Horizontal distribution across unlimited Rust cores and workers

### üéØ Next Steps: Begin with Database-Backed Worker Intelligence

**IMMEDIATE PRIORITY**: Complete Phase 1 database-backed worker registration to solve the critical namespace mismatch problem preventing intelligent work distribution.

**FOUNDATION**: This database foundation enables all subsequent capabilities - proactive orchestration, distributed coordination, and Ruby simplification.

**RECOMMENDATION**: Begin Phase 1 implementation immediately. The namespace mismatch issue is blocking full utilization of the excellent ZeroMQ foundation already built.

---

*This evolution transforms Tasker from a reactive orchestration service into an intelligent, autonomous, distributed workflow processing system while preserving all the reliability and performance characteristics that make the current ZeroMQ architecture successful.*
